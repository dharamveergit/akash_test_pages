[{"title":"Akash Node CLI Build","body":"\nWe would want our own Akash node when we have one of the following needs:\n\n* _**Akash Validator**_ - a full Akash Node is a prerequisite step to run a validator&#x20;\n* _**Akash Provider**_ - a dedicated Akash Node is recommended for providers&#x20;\n* _**Akash Production dApps**_ - a dedicated Akash Node is best practice to eliminate reliance on public nodes when you take your distributed apps past the testing phase.\n\nIn this guide we will review the Akash Node setup which includes the following steps:\n\n* [STEP1 - Install Akash Software](#step1---install-akash-software)\n* [STEP2 - Add Akash Install Location in the User’s Path](#step2---add-akash-install-location-in-the-users-path)\n* [STEP3 - Choose a Node Moniker](#step3---choose-a-node-moniker)\n* [STEP4 - Initialize New Node](#step4---initialize-new-node)\n* [STEP5 - Set Minimum Gas Price](#step5---set-minimum-gas-price)\n* [STEP6 - Copy the Genesis File](#step6---copy-the-genesis-file)\n* [STEP7 - Add Seed Nodes](#step7---add-seed-and-peer-nodes)\n* [STEP8 - Fast Sync](#step8---fast-sync)\n* [STEP9 - Blockchain Snapshot Use](#step9---blockchain-snapshot-use)\n* [STEP10 - Start the Akash Node](#step10---start-the-akash-node)\n* [Additional Information](#additional-information)\n* [RPC Service](#rpc-service)\n* [API Service](#api-service)\n\n\n\n## STEP1 - Install Akash Software\n\n\n\nIn this step we will cover Installing the Akash software on a Linux server.  We will use an Ubuntu server for our examples.  The commands may need to be changed slightly depending on your Linux distribution.\n\n### Download and Install Akash\n\n#### Install Latest Stable Akash Version\n\n```\ncd ~\n\napt install jq -y\n\napt install unzip -y\n\ncurl -sSfL https://raw.githubusercontent.com/akash-network/node/main/install.sh | sh\n```\n\n## STEP2 - Add Akash Install Location in the User’s Path\n\nAdd the Akash install location to the user’s path for ease of use.\n\n**NOTE -** below we provide the steps to add the Akash install directory to a user’s path on a Linux Ubuntu server.  Please take a look at a guide for your operating system and how to add a directory to a user’s path.\n\n### Open User’s Path Settings in an Editor\n\n```\nvi /etc/environment\n```\n\n### View Current Path Settings\n\n* It is always best practice to view the path within a text editor or cat it out to console prior to the update to avoid errors.\n* Example file_**:**_\n\n```\nPATH=\"/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin\"\n```\n\n### Add the Akash Executable Path\n\n* Add the following directory, which is the Akash install location, to PATH\n*   _**Note**_ - this assumes Akash was installed while logged in as the root user.\n\n    ```\n    /root/bin\n    ```\n\n### Post Update Path Settings\n\n```\nPATH=\"/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/root/bin\"\n```\n\n### Activate New Path Settings\n\n*   Make the new path active in the current session\n\n    ```\n    . /etc/environment\n    ```\n\n### Verify New Path Settings and Akash Install\n\n*   Display the version of Akash software installed.  This confirms the software installed and that the new user path addition worked.\n\n    ```\n    akash version\n    ```\n* Expected result (version displayed may be different)\n\n```\nroot@ip-10-0-10-146:~# akash version\n\nv0.26.1\n```\n\n## STEP3 - Choose a Node Moniker\n\nWe choose a \"moniker\" which is a readable name for your Akash Node.\n\n* Replace the moniker value with a name of your choice\n* **NOTE** - monikers can contain only ASCII characters\n\n```\nAKASH_MONIKER=<moniker>\n```\n\n### Moniker Updates\n\n*   The moniker can be changed later, if needed, within the following file:\n\n    ```\n    ~/.akash/config/config.toml\n    ```\n\n## STEP4 - Initialize New Node\n\nIn this step we will initialize our new Akash Node.  In the background several configuration files will be created which can be edited later as needed.\n\n_**Before starting the node, we specify the Akash network and chain ID**_\n\n```\nAKASH_NET=\"https://raw.githubusercontent.com/akash-network/net/main/mainnet\"\n\nexport AKASH_CHAIN_ID=\"$(curl -s \"$AKASH_NET/chain-id.txt\")\"\n```\n\n_**Start the node**_\n\n```\nakash init --chain-id \"$AKASH_CHAIN_ID\" \"$AKASH_MONIKER\"\n```\n\n_**Example/Expected Result**_\n\n```\n{\"app_message\":{\"audit\":{\"attributes\":[]},\"auth\":{\"accounts\":[],\"params\":{\"max_memo_characters\":\"256\",\"sig_verify_cost_ed25519\":\"590\",\"sig_verify_cost\n\n<output truncated>\n\n},\"upgrade\":{},\"vesting\":{}},\"chain_id\":\"akashnet-2\",\"gentxs_dir\":\"\",\"moniker\":\"chainzero\",\"node_id\":\"2f4491952df08e69fd988c6f5d6ed21e25318fbc\"}\n```\n\n## STEP5 - Set Minimum Gas Price\n\nYour node keeps unconfirmed transactions in its mempool. In order to protect the node from spam, it is best to set a minimum gas price that the transaction must meet in order to be accepted into the mempool.\n\n_**This setting can be found in the following file and we will change the default value which is blank.**_\n\n```\nvi ~/.akash/config/app.toml\n```\n\n_**The initial recommended min-gas-prices is 0.025uakt but you might want to change it later.**_\n\n```\n# This is a TOML config file.\n# For more information, see https://github.com/toml-lang/toml\n\n##### main base config options #####\n\n# The minimum gas prices a validator is willing to accept for processing a\n# transaction. A transaction's fees must meet the minimum of any denomination\n# specified in this config (e.g. 10uatom).\n\nminimum-gas-prices = \"0.025uakt\"\n```\n\n## STEP6 - Copy the Genesis File\n\nAkash nodes need the Genesis file for the blockchain.  In this step we will gather the genesis.json file and make sure it is valid.\n\n### Copy the Genesis File\n\n```\ncurl -s \"$AKASH_NET/genesis.json\" > $HOME/.akash/config/genesis.json\n```\n## STEP7 - Add Seed and Peer Nodes\n\n\n\nA seed node is used as an initial peer on the network. The seed node will provide a list of peers which can be used going forward. In this step we will configure a seed node to connect with.\n\n## Seed Nodes\n\n### List Current Seed Nodes\n\n```\ncurl -s \"$AKASH_NET/seed-nodes.txt\" | paste -d, -s\n```\n\n### Expected Output of Seed Node List\n\n```\nroot@ip-10-0-10-101:~# curl -s \"$AKASH_NET/seed-nodes.txt\" | paste -d, -s\n\n27eb432ccd5e895c5c659659120d68b393dd8c60@35.247.65.183:26656,8e2f56098f182ffe2f6fb09280bafe13c63eb42f@46.101.176.149:26656,fff99a2e8f3c9473e4e5ee9a99611a2e599529fd@46.166.138.218:26656\n```\n\n### **Include the Seed Nodes in Config Files**\n\n#### Seed Nodes\n\n*   Open the config.toml file in an editor\n\n    ```\n    vi $HOME/.akash/config/config.toml\n    ```\n* Within the editor find the seeds field as shown at the bottom of this output\n\n```\n#######################################################\n###           P2P Configuration Options             ###\n#######################################################\n[p2p]\n\n# Address to listen for incoming connections\nladdr = \"tcp://0.0.0.0:26656\"\n\n# Address to advertise to peers for them to dial\n# If empty, will use the same port as the laddr,\n# and will introspect on the listener or use UPnP\n# to figure out the address.\nexternal_address = \"\"\n\n# Comma separated list of seed nodes to connect to\nseeds = \"\"\n```\n\n* Copy and paste the seed nodes returned via the “List Current Seed Nodes” part of this section\n*   Following update the seeds field should appear like this:\n\n    ```\n    # Comma separated list of seed nodes to connect to\n    seeds = \"27eb432ccd5e895c5c659659120d68b393dd8c60@35.247.65.183:26656,8e2f56098f182ffe2f6fb09280bafe13c63eb42f@46.101.176.149:26656,fff99a2e8f3c9473e4e5ee9a99611a2e599529fd@46.166.138.218:26656\"\n    ```\n\n## Peer Nodes\n\n### List Current Peer Nodes\n\n```\ncurl -s \"$AKASH_NET/peer-nodes.txt\" | paste -d, -s\n```\n\n### Expected Output of Peer Node List\n\n```\nroot@ip-10-0-10-146:~# curl -s \"$AKASH_NET/peer-nodes.txt\" | paste -d, -s\n\n27eb432ccd5e895c5c659659120d68b393dd8c60@35.247.65.183:26656,9180b99a5be3443677e0f57fc5f40e8f071bdcd8@161.35.239.0:51656,47c9acc0e7d9b244a6404458e76d50b6284bfbbb@142.93.77.25:26656,ab7b55588ea3f4f7a477e852aa262959e54117cd@3.235.249.94:26656,4acf579e2744268f834c713e894850995bbf0ffa@50.18.31.225:26656,3691ac1f56389ffec8579c13a6eb8eca41cf8ae3@54.219.88.246:26656,86afe23f116ba4754a19819a55d153008eb74b48@15.164.87.75:26656,6fbc3808f7d6c961e84944ae2d8c800a8bbffbb4@138.201.159.100:26656,a2a3ffe7ac122a218e1f59c32a670f04b8fd3033@165.22.69.102:26656\n```\n\n### **Include the Peer Nodes in Config Files**\n\n#### Persistent Peers\n\n*   Open the config.toml file in an editor\n\n    ```\n    vi $HOME/.akash/config/config.toml\n    ```\n* Within the editor find the persistent\\_peers field as shown at the bottom of this output\n\n```\n#######################################################\n###           P2P Configuration Options             ###\n#######################################################\n[p2p]\n\n# Address to listen for incoming connections\nladdr = \"tcp://0.0.0.0:26656\"\n\n# Address to advertise to peers for them to dial\n# If empty, will use the same port as the laddr,\n# and will introspect on the listener or use UPnP\n# to figure out the address. ip and port are required\n# example: 159.89.10.97:26656\nexternal_address = \"\"\n\n# Comma separated list of seed nodes to connect to\nseeds = \"429d14fe2ab411e946623c20b060efdf230a5a8a@p2p.edgenet-1.ewr1.aksh.pw:26656,174e186ab7ef0aa8add457fecc5cca41b52cc031@p2p.edgenet-1.ewr1.aksh.pw:26652,a0dcc96946847f8bee74ffabd7d6d4809d030829@p2p.edgenet-1.ewr1.aksh.pw:26653,49c15444a04187b46db5e35b428b93bda42885e8@p2p.edgenet-1.ewr1.aksh.pw:26654,d9b7aba0738fd94072f11f49d5c2c0e119ef4268@170.187.200.114:26656\"\n\n# Comma separated list of nodes to keep persistent connections to\npersistent_peers = \"\"\n```\n\n* Copy and paste the seed nodes returned via the “List Current Peer Nodes” part of this section\n* Following update the persistent\\_peers field should appear like this:\n\n```\n# Comma separated list of nodes to keep persistent connections to\n\npersistent_peers = \"27eb432ccd5e895c5c659659120d68b393dd8c60@35.247.65.183:26656,9180b99a5be3443677e0f57fc5f40e8f071bdcd8@161.35.239.0:51656,47c9acc0e7d9b244a6404458e76d50b6284bfbbb@142.93.77.25:26656,ab7b55588ea3f4f7a477e852aa262959e54117cd@3.235.249.94:26656,4acf579e2744268f834c713e894850995bbf0ffa@50.18.31.225:26656,3691ac1f56389ffec8579c13a6eb8eca41cf8ae3@54.219.88.246:26656,86afe23f116ba4754a19819a55d153008eb74b48@15.164.87.75:26656,6fbc3808f7d6c961e84944ae2d8c800a8bbffbb4@138.201.159.100:26656,a2a3ffe7ac122a218e1f59c32a670f04b8fd3033@165.22.69.102:26656\"\n```\n\n## RPC Listening Address\n\n\n\n*   Open the config.toml file in an editor\n\n    ```\n    vi $HOME/.akash/config/config.toml\n    ```\n* Within the editor find the RPC listening address field as shown in this output\n* Update the listening address field to `\"tcp://0.0.0.0:26657\"` as shown\n* This setting will ensure listening occurs on all interfaces\n\n```\n[rpc]\n\n# TCP or UNIX socket address for the RPC server to listen on\nladdr = \"tcp://0.0.0.0:26657\"\n```\n\n\n## STEP8 - Fast Sync\n\nFast Sync means nodes can catch up quickly by downloading blocks in bulk.\n\n*   Fast Sync settings can be found in the following file\n\n\n\n```\ncat $HOME/.akash/config/config.toml\n```\n\n### Verify Fast Sync Settings\n\n* Most likely no changes will be necessary to config.toml and the default settings will be fine. But we will make sure.&#x20;\n*   Verify the fast\\_sync field is set to true\n\n    ```\n    # If this node is many blocks behind the tip of the chain, FastSync\n    # allows them to catchup quickly by downloading blocks in parallel\n    # and verifying their commits\n    fast_sync = true\n    ```\n* Verify the Fast Sync version is set to v0.&#x20;\n*   While version 0 is said to be the “legacy” version, in our experience this version works better.\n\n    ```\n    #######################################################\n    ###       Fast Sync Configuration Connections       ###\n    #######################################################\n    [fastsync]\n\n    # Fast Sync version to use:\n    #   1) \"v0\" (default) - the legacy fast sync implementation\n    #   2) \"v1\" - refactor of v0 version for better testability\n    #   2) \"v2\" - complete redesign of v0, optimized for testability & readability\n    version = \"v0\"\n    ```\n\n\n    ## STEP9 - Blockchain Snapshot Use\n\nWe could let our node catch up to the current block but this would take a very long time. Instead we will download a snapshot of the blockchain before starting our node.\n\n**NOTE** - at the time of this writing the snapshot is 200GB and could take some time to pull down.\n\n### Remove Existing Data\n\n```\nrm -rf ~/.akash/data; \\\nmkdir -p ~/.akash/data; \\\ncd ~/.akash\n```\n\n### Download Snapshot&#x20;\n\n> NOTE - in the `Example Steps` provided below a specific snapshot version is used (`12992214`).  In your use the current/latest snapshot version should be used.  The latest Akash snapshot version - made available via Polkachu - can be found [here](https://polkachu.com/tendermint\\_snapshots/akash).  Replace all references to example snapshot version `12992214` with the current/latest version found on this site.\n\n#### Example Steps\n\n```\nwget -O akash_12992214.tar.lz4 https://snapshots.polkachu.com/snapshots/akash/akash_12992214.tar.lz4 --inet4-only\n\napt-get install lz4\n\nlz4 -d akash_12992214.tar.lz4\n\ntar -xvf akash_12992214.tar\n```\n\n## STEP10 - Start the Akash Node\n\n\n\n### Start the Node\n\nIn this section we will create a script and a related service to start the node.  The service will additionally ensure that the node is restarted following reboots.\n\n#### Create Script to Start Node\n\n* Create a script to start the Akash Node\n\n```\ncd ~\n\ncat <<EOF | tee /usr/local/bin/start-node.sh\n#!/usr/bin/env bash\n\n/root/bin/akash start\nEOF\n```\n\n* Make the script an executable\n\n```\nchmod 744 /usr/local/bin/start-node.sh\n```\n\n#### Create Related Service\n\n```\ncat > /etc/systemd/system/akash-node.service << 'EOF'\n[Unit]\nDescription=Akash Node\nAfter=network.target\n\n[Service]\nUser=root\nGroup=root\nExecStart=/usr/local/bin/start-node.sh\nKillSignal=SIGINT\nRestart=on-failure\nRestartSec=15\nStartLimitInterval=200\nStartLimitBurst=10\n#LimitNOFILE=65535\n\n[Install]\nWantedBy=multi-user.target\nEOF\n```\n\n#### Start the Service\n\n```\nsystemctl daemon-reload\nsystemctl start akash-node\nsystemctl enable akash-node\n```\n\n### Check the Status of the Node\n\n* Initially the node will show a status of `\"catching_up\":true` within the output of the status command\n* Eventually the node will show the height of the latest block and should indicate `\"catching_up\":false`\n*   The latest block number can be found on the [Mintscan](https://www.mintscan.io/akash) website for comparison.\n\n    ```\n    akash status\n    ```\n\n#### Example/Sample Output\n\n* Status output while Node is catching up to current block\n\n```\nakash status\n\n{\"NodeInfo\":{\"protocol_version\":{\"p2p\":\"8\",\"block\":\"11\",\"app\":\"0\"},\"id\":\"f2b42b91d103996efa12ab860c05dac66b8e9f7c\",\"listen_addr\":\"tcp://0.0.0.0:26656\",\"network\":\"akashnet-2\",\"version\":\"0.34.19\",\"channels\":\"40202122233038606100\",\"moniker\":\"node\",\"other\":{\"tx_index\":\"on\",\"rpc_address\":\"tcp://0.0.0.0:26657\"}},\"SyncInfo\":{\"latest_block_hash\":\"D18299F966825C5886B7F88D677413F6564F323ADBF55C5E80AF76CA8E151E06\",\"latest_app_hash\":\"4E6A696D911E7A238F4E4BD437712B197858E3A7ED319B12F38961B2C2960C47\",\"latest_block_height\":\"6493433\",\"latest_block_time\":\"2022-06-27T04:03:30.370092856Z\",\"earliest_block_hash\":\"E25CE5DD10565D6D63CDA65C8653A15F962A4D2960D5EC45D1DC0A4DE06F8EE3\",\"earliest_app_hash\":\"19526102DDBCE254BA71CC8E44185721D611635F638624C6F950EF31D3074E2B\",\"earliest_block_height\":\"5851001\",\"earliest_block_time\":\"2022-05-12T17:51:58.430492536Z\",\"catching_up\":true},\"ValidatorInfo\":{\"Address\":\"CF6AC143794B35885AA4B29CA012DABFAEB88EAB\",\"PubKey\":{\"type\":\"tendermint/PubKeyEd25519\",\"value\":\"f0mUYVv5z2HCi4UmK72C/pxwbIu5tdDHEVc94yCLZQc=\"},\"VotingPower\":\"0\"}}\n```\n\n## Additional Information\n\n### Config Files\n\nAkash Node configurations are found within these files:\n\n#### Cosmos Specific Configuration\n\n```\n~/.akash/config/app.toml\n```\n\n#### Tendermint Specific Configuration\n\n```\n~/.akash/config/config.toml\n```\n\n### Akash Networks\n\nWithin this guide the Akash mainnet is used and as specified in the AKASH\\_NET value.  To launch a node on the testnet or edgenet and for additional network information, use this [guide](https://github.com/akash-network/net).\n\n### State Pruning\n\nThere are several strategies for pruning state, please be aware that this is only for state and not for block storage:\n\n1. **default:** the last 100 states are kept in addition to every 500th state; pruning at 10 block intervals\n2. **nothing:** all historic states will be saved, nothing will be deleted (i.e. archiving node)\n3. **everything:** all saved states will be deleted, storing only the current state; pruning at 10 block intervals\n4. **custom:** allow pruning options to be manually specified through pruning-keep-recent, pruning-keep-every, and pruning-interval\n\nYou can configure the node's pruning strategy at start time with the --pruning or by configuring the app.toml file.\n\n_**Validator Node Pruning Note**_** -** please do not use --pruning everything on validator nodes as it is known to cause issues. Instead use --pruning default.\n\n## RPC Service\n\nThe RPC Service allows for both sending transactions to the network and for querying state from the network. It is used by the `akash` command-line tool when using an `akash tx` command or `akash query` command.\n\nThe RPC Service is configured in the `[rpc]` section of `~/.akash/config/config.toml`.\n\nBy default, the service listens on port `26657`, but this can also be changed in the `[rpc]` section of `config.toml`.\n\n\n## API Service\n\nThe API Service of a full node enables a read-only query API that is useful for many tools such as dashboards, wallets, and scripting in general.\n\nThe API Service is configured in `~/.akash/config/app.toml` and can be enabled in the `[api]` section:\n\n```\n[api]\nenable = \"true\"\n```\n\nBy default, the service listens on port `1317`, but this can also be changed in the `[api]` section of `app.toml`.","description":null,"slug":"docs/akash-nodes/akash-node-cli-build"},{"title":"Akash Node Deployment Via Omnibus","body":"\nIn this guide we will cover the deployment of an Akash Node using Cosmos Omnibus. Omnibus will deploy the node onto Akash’s distributed network. Omnibus will greatly simplify the deployment process.\n\nFor fine tuning of the Akash Node check the _Additional Information_ section.\n\n## Cloudmos Deployment of a Node\n\n### Cloudmos Deploy Overview\n\nIf you have not used Cloudmos Deploy previously, please use [this guide](/docs/docs/deployments/akash-cli/installation/) to get started. The guide includes steps to install the app and to set up a wallet for Akash deployments. Once the Cloudmos Deploy tool is installed, return to this guide to walk through the Akash Node deployment.\n\n### Cloudmos Deploy Walkthrough\n\n* Our Akash Node install begins by creating a new deployment\n\n![](../../../assets/deploymentsHomeScreen.png)\n\n* A number of checks are completed to make sure we are ready to deploy a new app onto Akash\n* Revisit the Cloudmos Deploy guide for tips if any checks fail\n\n![](<../../../assets/akashlyticsBaseVerify (1).png>)\n\n* To install the Akash Node we will use a custom SDL file\n* Select the “Empty” option so that we can copy/paste the Akash Node SDL in the next step\n\n![](<../../../assets/manifestSelectInitial (1).png>)\n\n* Copy and paste the SDL from [this site](https://github.com/akash-network/cosmos-omnibus/blob/master/akash/deploy.yml) into the Cloudmos SDL editor window\n* **NOTE -** the SDL within GitHub currently has a storage > size value of 120Gi. Omnibus uses a compressed snapshot of the blockchain and when expanded 120GB of storage for the deployment will not be enough. At the time of this writing adjusting the storage size to 350GB will suffice and allow some growth. Please adjust the storage appropriately and as shown in the screenshot below.\n\n![](../../../assets/sdlWithStorageAdjustment.png)\n\n* Accept the initial deposit of 5 AKT into the deployment’s escrow account\n* The escrow can be refilled easily within Cloudmos Deploy at any time\n\n![](<../../../assets/acceptDeposit (1) (1) (1) (2).png>)\n\n* Approve the transaction fee to allow the deployment to continue\n\n![](../../../assets/transactionFeeDeployAccept.png)\n\n* Select a provider from the bid list\n\n![](<../../../assets/bidSelect (1).png>)\n\n* Accept the transaction fee to create a lease with the provider\n\n![](<../../../assets/bidTransactionFee (1).png>)\n\n### Cloudmos Deploy Complete\n\n* Once the deployment is complete the lease details are shown\n\n![](../../../assets/deploymentComplete.png)\n\n* After some time the Available/Ready Replicas fields will update to show the current node count. It may be necessary to refresh the screen for this count to update.\n\n![](<../../../assets/deploymentCounts (1).png>)\n\n## Confirmation of Node Deployment\n\nWith the install of the Akash node complete, it must sync with the blockchain. Omnibus will use a snapshot of prior blocks to speed the sync but even so this will take several hours. In the meantime let’s look into monitoring the sync and the running node’s health.\n\n### Snapshot Download Progress\n\n* While the blockchain snapshot is downloading, the following logs should be visible within Cloudmos\n* We can know that the snapshot download is not yet complete if we see this message in the logs\n\n![](../../../assets/snapshotDownloading.png)\n\n### Snapshot Download Completed\n\n* After the snapshot download completes the logs will begin showing blockchain sync activity\n* Example output shown should look something like this but with the current blocks on the chain\n\n![](../../../assets/snapshotDownloadComplete.png)\n\n### Akash Node Verifications\n\n* These confirmations can be used after the snapshot has been completed\n\n#### Capture Deployment Address\n\n* Begin by capturing the deployment’s public URI from Cloudmos\n\n![](../../../assets/nodeUIR.png)\n\n#### Confirm Blockchain Sync\n\n* Open a web browser and enter the Node’s URI as an address\n* If the Node deployed successfully we should view a list of RPC endpoints\n* Click the link to visit the Node’s status page\n\n![](<../../../assets/rpcStatusLink (1) (1) (1) (2) (2).png>)\n\n* Look for the “latest\\_block\\_height” field\n\n![](../../../assets/rpcStatusVerification.png)\n\n* Open [Mintscan](https://www.mintscan.io/akash), a popular blockchain explorer, to compare the captured “latest\\_block\\_height” value to the latest block displayed in the explorer\n* The block height from the Akash Node and Mintscan will not be exactly match but should be close to each other\n\n![](../../../assets/mintscanBlockHeight.png)\n\n#### Confirm Peer Nodes\n\n* Navigate back to the home page for your Node\n* Click the link for “net\\_info”\n\n![](<../../../assets/rpcNetInfoLink (1).png>)\n\n* Find the section with the name of “peers”\n* Here we can determine what other Akash nodes our node is connected to and the status of these connections\n\n![](<../../../assets/rpcNetInfoData (1).png>)\n\n## Additional Information\n\n### Cosmos Omnibus Repository\n\n* The repository for the [Akash Cosmos Omnibus ](https://github.com/akash-network/cosmos-omnibus)project.\n\n### Akash SDL\n\n* The Akash manifest/SDL used in this [guide](https://github.com/akash-network/cosmos-omnibus/blob/master/akash/deploy.yml).\n\n### Chain JSON Config File\n\n* The [config file](https://raw.githubusercontent.com/akash-network/net/main/mainnet/meta.json) used for the genesis URL, seed nodes, etc.\n\n### Cosmos Chain Registry\n\n* This [repo](https://github.com/cosmos/chain-registry) contains a chain.json for a number of cosmos-sdk based chains. A chain.json contains data that makes it easy to start running or interacting with a node.","description":null,"slug":"docs/akash-nodes/akash-node-deployment-via-omnibus"},{"title":"Akash Node Via Helm Charts","body":"\n\nBuild an Akash Node quickly and simply with a deployment onto your Kubernetes cluster via a ready to use Helm Chart.\n\nThe Akash Node build detailed takes advantage of \"state sync\" which allows rapid activation.  State Sync allows a new node to join a network by fetching a snapshot of the state at a recent height instead of fetching and replaying all historical blocks. This can reduce the time needed to sync with the network from hours to minutes.\n\nSteps in this guide include:\n\n* **STEP 1** - [Prepare Kubernetes Cluster](#prepare-kubernetes-cluster)\n* **STEP 2** - [Akash Node Installation](#akash-node-installation)\n* **STEP 3** - [Node Verifications](#node-verifications)\n\n## Prepare Kubernetes Cluster\n\n### Overview\n\nIn this section we will create necessary Kubernetes labels, install Helm, and ensure the necessary Helm repositories are available.\n\n### Create Necessary Kubernetes Labels\n\n* Create the `akash-services` label if not done so prior\n\n```\nkubectl create ns akash-services\nkubectl label ns akash-services akash.network/name=akash-services akash.network=true\n```\n\n### Install Helm\n\n* Install Helm for Kubernetes package management if not done so prior\n* Execute on these steps on a Kubernetes control plane node\n\n```\nwget https://get.helm.sh/helm-v3.11.0-linux-amd64.tar.gz\n\ntar -zxvf helm-v3.11.0-linux-amd64.tar.gz\n\ninstall linux-amd64/helm /usr/local/bin/helm\n\n###Remove any potential prior repo instances\nhelm repo remove akash\n\nhelm repo add akash https://akash-network.github.io/helm-charts\n```\n\n### Update the Akash Helm Repo\n\n```\nhelm repo update akash\n```\n\n## Akash Node Installation\n\n### Uninstall Any Prior Node Instances\n\n**NOTE** - if no previous Akash node instances are found the following message will be received and is expected\n\n* \\``` Error: uninstall: Release not loaded: akash-node: release: not found` ``\n\n```\nhelm -n akash-services uninstall akash-node\n```\n\n### **Install Akash Node as a Kubernetes Pod**\n\n```\nhelm install akash-node akash/akash-node -n akash-services\n```\n\n#### **Expected/Sample Output**\n\n```\nhelm install akash-node akash/akash-node -n akash-services\n\nNAME: akash-node\nLAST DEPLOYED: Thu Jun 23 13:26:03 2022\nNAMESPACE: akash-services\nSTATUS: deployed\nREVISION: 1\nTEST SUITE: None\n```\n\n## Node Verifications\n\n### View Helm Chart Values\n\n* Utilize this step to ensure the installed Helm Chart user supplied variables are correct\n* The values were defined when we executed the `helm install` command with specified parameters\n\n```\nhelm -n akash-services get values akash-node\n```\n\n#### **Expected/Sample Output**\n\n* If custom values were included with the `helm install` command the following output is expected (example)\n\n```\nhelm -n akash-services get values akash-node\n\nUSER-SUPPLIED VALUES:\nstate_sync:\n  enabled: true\n```\n\n* If no custom values were included with the `helm install` command the following output is expected\n\n```\nUSER-SUPPLIED VALUES:\nnull\n```\n\n### Verify Akash Node Pod Status\n\n* Confirm that the Akash Node pod is in a pristine state\n\n```\nkubectl get pods -n akash-services\n```\n\n#### Expected/Sample Output\n\n```\nkubectl get pods -n akash-services\n\nNAME                            READY   STATUS    RESTARTS   AGE\nakash-node-1-78954d745c-xgkhx   1/1     Running   0          50s\n```\n\n### Verify Akash Node Sync via Logs\n\n* Ensure that the Akash Node pod has errors of concerns in logs and that the blockchain sync is progressing\n* Replace the `name-of-pod` variable with `akash-node` pod name displayed in the previous step\n\n```\nkubectl logs <name-of-pod> -n akash-services | grep -iv peer | tail\n```\n\n#### Expected/Sample Output\n\n```\nkubectl logs akash-node-1-78954d745c-xgkhx -n akash-services | grep -iv peer | tail\n\n2:07PM INF Applied snapshot chunk to ABCI app chunk=16 format=1 height=6442000 module=statesync total=26\n2:07PM INF Applied snapshot chunk to ABCI app chunk=17 format=1 height=6442000 module=statesync total=26\n2:07PM INF Applied snapshot chunk to ABCI app chunk=18 format=1 height=6442000 module=statesync total=26\n2:07PM INF Applied snapshot chunk to ABCI app chunk=19 format=1 height=6442000 module=statesync total=26\n2:07PM INF Discovered new snapshot format=1 hash=\":��6u�^�/G号Ĝ��d]��W�]/\\x11�S�N*�\" height=6443000 module=statesync\n2:07PM INF Discovered new snapshot format=1 hash=\"�\\x1bkn\\x18��_��\\x06���8��,��\\f�Jp�\\x01Ft�\\t�~\" height=6442500 module=statesync\n2:07PM INF Applied snapshot chunk to ABCI app chunk=20 format=1 height=6442000 module=statesync total=26\n2:07PM INF Applied snapshot chunk to ABCI app chunk=21 format=1 height=6442000 module=statesync total=26\n2:07PM INF Applied snapshot chunk to ABCI app chunk=22 format=1 height=6442000 module=statesync total=26\n2:07PM INF Applied snapshot chunk to ABCI app chunk=23 format=1 height=6442000 module=statesync total=26\n```\n\n### Akash Node Status\n\n#### Access Pod Shell\n\n* Access the Kubernetes shell of the Akash Node deployment to view sync status\n\n```\nkubectl exec --stdin --tty -n akash-services <pod-name> -- /bin/bash\n```\n\n#### Verify Node Status\n\n```\nakash status\n```\n\n##### Expected/Sample Output\n\n* Note the following expected values:\n  * `catching_up` - should be `false` once your Node is in sync with the blockchain\n  * `latest_block_height` - compare this block height with the latest height on a block explorer as another method of validating if the node is in sync or if not in sync how close to completing that process the node may be\n\n```\nkubectl exec --stdin --tty -n akash-services akash-node-1-78954d745c-g46pf -- /bin/bash\n\nroot@akash-node-1-78954d745c-g46pf:/# akash status\n{\"NodeInfo\":{\"protocol_version\":{\"p2p\":\"8\",\"block\":\"11\",\"app\":\"0\"},\"id\":\"330603b82b2e0dbeadf84b13d00d81ff19017854\",\"listen_addr\":\"tcp://0.0.0.0:26656\",\"network\":\"akashnet-2\",\"version\":\"0.34.19\",\"channels\":\"40202122233038606100\",\"moniker\":\"mynode-1\",\"other\":{\"tx_index\":\"on\",\"rpc_address\":\"tcp://0.0.0.0:26657\"}},\"SyncInfo\":{\"latest_block_hash\":\"826995C3E57B5D5B56F2EB5B47C3F9315F87795F078063DE9E6C736064C3A6C3\",\"latest_app_hash\":\"1B3DCEFCFA1752777FBEC5B1E26DCB29484D22518C86492D50E8FCE02560D1B5\",\"latest_block_height\":\"6260678\",\"latest_block_time\":\"2022-06-10T16:51:26.696963875Z\",\"earliest_block_hash\":\"1957CBF8018B0819880ADB44402AE837E170FAD47FF5F745F9872D622F037816\",\"earliest_app_hash\":\"2A0D0C3541D399D24C26A4098A5741C628B28AC15EFEA6947DF6D3D71FD24B1F\",\"earliest_block_height\":\"6260001\",\"earliest_block_time\":\"2022-06-10T15:42:43.877575807Z\",\"catching_up\":false},\"ValidatorInfo\":{\"Address\":\"3410C9951968DA68145D8A4F06B3C7BA962D6926\",\"PubKey\":{\"type\":\"tendermint/PubKeyEd25519\",\"value\":\"tHKKYDHdV3VoWtRHzOu5vUP94vGc98QD8bxytH1Jlwo=\"},\"VotingPower\":\"0\"}}\n```\n\n### Additional Node Verification\n\n#### STEP 1 - From the K8s Control-Plane Node\n\n```\nexport AKASH_NODE=\"http://$(kubectl -n akash-services get ep akash-node-1 -o jsonpath='{.subsets[0].addresses[0].ip}'):26657\"\n\ncurl -s \"$AKASH_NODE/status\" | jq -r .\n```\n\n#### STEP 2 - From a Remote Address (Outside the K8s Network)\n\n* Use `kubectl port-forward` for forwarding the akash node (RPC) port 26657/tcp to your local station.\n\n##### Forward 26657 to 127.0.0.1:26657\n\n```\nkubectl -n akash-services port-forward service/akash-node-1 26657:26657\n```\n\n_Expected/Example Output_\n\n```\n$ kubectl -n akash-services port-forward service/akash-node-1 26657:26657\nForwarding from 127.0.0.1:26657 -> 26657\nForwarding from [::1]:26657 -> 26657\n```\n\n##### Put the kubectl port-forward Process into Background\n\n* Press Ctrl+Z and type bg + Enter as follows:\n\n```\n^Z\n[1]+  Stopped                 kubectl -n akash-services port-forward service/akash-node-1 26657:26657\n$ bg\n[1]+ kubectl -n akash-services port-forward service/akash-node-1 26657:26657 &\n```\n\n##### Access the RPC as if it were Running Locally\n\n```\ncurl -s http://127.0.0.1:26657/status | jq -r .\n```\n\n##### Stop kubectl port-forward When Done Testing\n\n```\n$ jobs\n[1]+  Running                 kubectl -n akash-services port-forward service/akash-node-1 26657:26657 &\n$ kill %1\n$ jobs\n[1]+  Terminated              kubectl -n akash-services port-forward service/akash-node-1 26657:26657\n```","description":null,"slug":"docs/akash-nodes/akash-node-via-helm-chart"},{"title":"API Endpoints","body":"\n\n\n* The following Swagger file provides information and initial testing of available Akash API endpoints\n\n[Akash API Endpoints](https://github.com/akash-network/akash-api/blob/main/docs/swagger-ui/swagger.yaml)","description":"Listing of APIs used for deployment and transaction details","slug":"docs/akash-nodes/api-endpoints"},{"title":"Public RPC Nodes","body":"\n## Available Nodes\n\n* A list of publicly available RPC nodes recommended for use:\n\n#### Trusted Community RPC Nodes\n\n```\nhttps://rpc.akashnet.net:443\nhttps://rpc.akash.forbole.com:443\nhttps://rpc-akash.ecostake.com:443\nhttps://akash-rpc.polkachu.com:443\n```\n\n## RPC Node Repo\n\n* The list of public RPC nodes recommended at the time of this writing are listed in the Available Nodes section\n* While we seek to ensure that this documentation is always up to date, visit the [`Akash RPC list` ](https://raw.githubusercontent.com/akash-network/net/main/mainnet/rpc-nodes.txt)in our GitHub repo for possible recent additions","description":null,"slug":"docs/akash-nodes/public-rpc-nodes"},{"title":"Akash Node","body":"\n\nThe Akash Node is a vital component within the Akash Network, a decentralized cloud computing platform that establishes a marketplace for computing resources. This node facilitates interactions with the network, transaction validation, and active participation in the consensus process.\n\n### Key Responsibilities\n\n1. **Blockchain Synchronization**: Ensures continuous synchronization with the network, maintaining an up-to-date copy of the blockchain to ensure data consistency and availability.\n\n2. **Transaction Submission**: Enables users to submit various transactions, including deployments, bids, and leases, to the Akash Network.\n\n3. **Querying Network State**: Provides an interface for querying the network's state, allowing users to access information related to deployments, orders, and account balances.\n\n### How it Works\n\nThe [Akash Node](https://github.com/akash-network/node) plays a critical role in facilitating interactions within the Akash Network. Here's a detailed look at its functioning and its integral role in the ecosystem:\n\n#### Blockchain Synchronization\n\nThe Akash Node continuously synchronizes with the network to maintain an up-to-date blockchain copy. Leveraging the Tendermint consensus [algorithm](https://tendermint.com/core/), a Byzantine Fault Tolerant (BFT) engine, the node ensures efficient agreement on the blockchain state.\n\n#### Transaction Validation and Propagation\n\nResponsible for validating and propagating transactions, the node checks the validity of received transactions according to the network's rules. Valid transactions are added to the mempool and propagated to [peers](https://tendermint-core-documentation).\n\n#### Block Creation\n\nValidator nodes within the Akash Network engage in block creation, proposing new blocks that contain transactions from the mempool. Validators then participate in the consensus process to reach agreement on block validity. Once consensus is reached, the new block is appended to the blockchain, and the node updates its local copy [accordingly](https://tendermint-core-documentation).\n\n#### Gossip Protocol\n\nThe Akash Node utilizes a gossip protocol to efficiently communicate with [peers](https://github.com/tendermint/spec/tree/master/spec). This protocol enables nodes to share information about the network's state, ensuring consistent views of the blockchain [state](https://github.com/tendermint/spec/tree/master/spec).\n\n#### Querying Network State\n\nMaintaining an indexed view of the blockchain state, the Akash Node enables users to query information such as account balances, deployment statuses, and order books. Leveraging the [Application Blockchain Interface (ABCI)](https://docs.tendermint.com/master/spec/abci/), the node processes and responds to user queries, offering insights into the network's state.\n\n#### Governance and Staking\n\nValidator nodes, particularly, participate in governance and staking processes. Validators stake Akash Tokens (AKT) as collateral, securing their network position. These nodes engage in governance by voting on proposals that impact the network's parameters, upgrades, and critical decisions.\n\nIn conclusion, the Akash Node serves as a pivotal component in the Akash Network, responsible for blockchain synchronization, transaction validation and propagation, block creation, and facilitating user queries. Additionally, it actively contributes to governance and staking processes, ensuring the network's proper functioning and decentralization.\n\n- [Akash Node GitHub](https://github.com/akash-network/node)\n- [Akash Network Official Website](https://akash.network/)\n- [Tendermint Consensus](https://tendermint.com/core/)\n- [Tendermint Core Documentation](https://docs.tendermint.com/)\n- [Gossip Protocol in Tendermint](https://github.com/tendermint/spec/tree/master/spec)\n- [Application Blockchain Interface](https://docs.tendermint.com/master/spec/abci/)","description":null,"slug":"docs/architecture/akash-node"},{"title":"Akash Provider","body":"\nAkash Providers play a pivotal role in the Akash Network by utilizing the [Akash Provider software](https://github.com/akash-network/provider), enabling them to effectively manage resources, submit bids, and interact with users. This section discusses the key components and responsibilities of Akash Providers.\n\n### Key Components\n\n1. **[Provider Daemon (`akashd`)][akashd]**: The Provider Daemon is a crucial software component responsible for managing the provider's resources. It communicates with the Akash blockchain, handles resource allocation for deployments, and processes deployment orders. This includes receiving and processing deployment orders, submitting bids, and orchestrating the deployment of user applications.\n\n2. [**Container Orchestration**][kubernetes]: Akash Providers leverage container orchestration systems like Kubernetes or Docker Swarm. These systems efficiently manage the deployment and scaling of user applications, ensuring optimal resource allocation and the secure execution of applications within the provider's infrastructure.\n\n### Responsibilities\n\n#### Resource Management\n\nAkash Providers bear the responsibility of managing their computing resources, encompassing CPU, memory, storage, and bandwidth. Efficient and secure allocation of resources to user applications is crucial, requiring vigilant monitoring of infrastructure health and performance.\n\n#### Bidding on Orders\n\nUpon the generation of deployment orders broadcast to the network, Akash Providers analyze and place competitive bids. Balancing bids is essential for maximizing utilization and revenue while remaining competitive in the marketplace.\n\n#### Lease Management\n\nAfter a user selects a winning bid, Akash Providers must manage the resulting lease. This involves allocating resources according to lease terms, ensuring secure and efficient deployment and operation of user applications.\n\n#### Deployment Management\n\nProviders are tasked with deploying and managing user applications within their infrastructure. This includes overseeing the application's lifecycle, such as starting, stopping, and scaling, while maintaining security and isolation from other deployments.\n\n#### Monitoring and Reporting\n\nAkash Providers are obligated to monitor their infrastructure and user applications, guaranteeing optimal and secure operation. Reporting metrics and events to users and the Akash Network, such as resource utilization, deployment status, and billing information, is an integral part of their role.\n\nIn summary, Akash Providers are indispensable contributors to the Akash Network, providing computing resources and managing user application deployments. Their interaction with the Akash blockchain through the [Provider Daemon (`akashd`)][akashd] and utilization of container orchestration systems ensures the secure and efficient deployment of applications. Managing resources, bidding on orders, handling leases, and proactive infrastructure monitoring are vital tasks for maintaining competitiveness in the marketplace.\n\n[kubernetes]: /docs/docs/architecture/containers-and-kubernetes/\n[akash-providers]: https://github.com/akash-network/provider\n[akashd]: https://github.com/akash-network/provider","description":null,"slug":"docs/architecture/akash-provider"},{"title":"Containers & Kubernetes","body":"\n\nThe **Akash Container Platform** serves as a deployment platform for hosting and managing [containers](#containers), providing users the capability to run _**any**_ Cloud-Native application. The Akash Network incorporates a set of cloud management services, including [Kubernetes](https://kubernetes.io), to orchestrate and manage containers seamlessly.\n\n## Containers\n\nA **container** represents a standardized software unit that encapsulates code and all its dependencies. This packaging allows applications to run consistently and reliably across various computing environments. A **container image** is a lightweight, standalone, executable package containing everything necessary to run an application: code, runtime, system tools, system libraries, and settings. At runtime, **container images** transform into **containers**. Whether for Linux or Windows-based applications, containerized software ensures uniform operation, irrespective of the underlying infrastructure. Containers provide isolation, ensuring consistent functionality regardless of differences between development and staging environments.\n\n## Kubernetes\n\nThe Akash Network functions as a peer-to-peer network comprising clusters of computation nodes, each running Kubernetes. According to the official documentation:\n\n> Kubernetes is a portable, extensible, open-source platform for managing containerized workloads and services, facilitating both declarative configuration and automation. It boasts a large, rapidly growing ecosystem, with widely available services, support, and tools.\n\n> Kubernetes executes workloads by placing containers into Pods to run on Nodes. Nodes can be virtual or physical machines, depending on the cluster. Each node, managed by the control plane, houses the services necessary to run Pods.\n\nThe scalability, resilience, and security inherent in Kubernetes make it an ideal solution for providers to run their tenants' workloads. By leveraging this technology, Akash Network establishes itself as a decentralized Serverless Compute marketplace.","description":null,"slug":"docs/architecture/containers-and-kubernetes"},{"title":"Overview","body":"\nThe architecture of Akash Network is composed of several key components.\n\n## Key Components\n<br/>\n\n1. **Blockchain Layer**: Provides a secure, scalable consensus mechanism using Tendermint Core and Cosmos SDK.\n2. **Application Layer**: Handles deployment, resource allocation, and the lifecycle of deployments within the Akash ecosystem.\n3. **Provider Layer**: Manages providers' resources, bids, and user application deployments using Provider Daemon and Container Orchestration.\n4. **User Layer**: Enables users to interact with the network, manage resources, and monitor application status using CLI, Console, and Dashboard.\n\n## Blockchain Layer\n\nThe Blockchain Layer is built on top of the Cosmos SDK and Tendermint Core, providing a scalable and secure consensus mechanism. This layer is responsible for maintaining the distributed ledger, validator management, governance, and token transactions (Akash Token, AKT). It ensures the security and decentralization of the Akash Network.\n\n### Key Components\n\n- [**Tendermint Core**](https://tendermint.com/sdk/): A Byzantine Fault Tolerant (BFT) consensus engine that provides the foundation for the Akash blockchain.\n- [**Cosmos SDK**](https://v1.cosmos.network/sdk): A modular framework that allows for the creation of custom blockchains, offering tools and modules to build the Akash Network.\n\n## Application Layer\n\nThe Application Layer is responsible for handling the deployment of applications, resource allocation, and the lifecycle of deployments within the Akash ecosystem. It includes the following components:\n\n- **Deployment**: Users submit deployment configurations specifying their requirements, such as computing resources, storage, and geographic location.\n- **Order**: Orders are generated based on deployment configurations and broadcast to the network.\n- **Bid**: Providers place bids for the orders, offering their resources at a competitive price.\n- **Lease**: Once a user selects the winning bid, a lease is created between the user and the provider.\n\n## Provider Layer\n\nThe Provider Layer consists of data centers, cloud providers, and individual server operators who offer their resources to the Akash Network. They run the Akash Provider software to manage their resources, submit bids, and interact with users.\n\n### Key Components\n\n- [**Provider Daemon**](https://github.com/akash-network/provider): A software component that manages the provider's resources, communicates with the Akash blockchain, and handles resource allocation for deployments.\n- **Container Orchestration**: Providers utilize container orchestration systems, such as Kubernetes or Docker Swarm, to manage the deployment and scaling of user applications.\n\n## User Layer\n\nThe User Layer comprises users who require computing resources, such as developers or businesses, who utilize the Akash Network to deploy and manage their applications. Users interact with the network through various tools and interfaces.\n\n### Key Components\n\n- [**Akash Client (cli)**](https://github.com/akash-network/provider): A command-line interface (CLI) tool that allows users to interact with the Akash Network, create deployments, manage resources, and monitor the status of their applications.\n- [**Akash Console**](https://console.akash.network/new-deployment): Akash Console is a web app for deploying applications on Akash Network, offering a dashboard for deployment monitoring and administration.\n- [**Akash Cloudmos Dashboard**](https://cloudmos.io/akash/dashboard): A graphical user interface (GUI) that provides users with an intuitive way to interact with the network, manage deployments, and monitor their resources.\n\nIn summary, the Akash Network architecture consists of the Blockchain Layer, Application Layer, Provider Layer, and User Layer. This structure enables the platform to provide a decentralized marketplace for computing resources, connecting users who need resources with providers.\n","description":null,"slug":"docs/architecture/overview"},{"title":"Akash CLI Booster","body":"## Overview\n\n- [Close a Deployment](#close-a-deployment)\n      - [Expected/Sample Output](#expectedsample-output)\n- [List All Active Deployments](#list-all-active-deployments)\n      - [Expected/Sample Output](#expectedsample-output-1)\n- [Access a Deployment’s Shell](#access-a-deployments-shell)\n      - [Example/Expected Output](#exampleexpected-output-2)\n- [Obtain a Deployment’s Logs](#obtain-a-deployments-logs)\n      - [Example/Expected Output](#exampleexpected-output-3)\n\n## Getting Started with the CLI Booster\n\nOur use of the Akash CLI Booster begins by downloading the application.\n\nEnsure the following steps have been completed prior:\n\n* Install Akash CLI which is covered[ here](/docs/docs/deployments/akash-cli/overview/).\n* Create a new wallet or import a pre-existing wallet.  New wallet creation steps are covered [here](/docs/docs/getting-started/token-and-wallets/#keplr-wallet).\n\n## Clone Repo\n<br/>\n\n```\ngit clone https://github.com/arno01/akash-tools.git\n```\n\n## Initialize\n\nStart the tool with these quick steps.\n\n```\ncd akash-tools/cli-booster\n\n. akash.source\n```\n\n* Select account that should be used for deployments and/or other activities from list\n* Example prompt shown below.  In this case only one account is available in the keyring and we could press accept the default list number of “0” by pressing enter with no change.\n\n```\nAvailable keys:\n\t\"name\"           \"address\"\n0>\t\"deploymentone\"  \"akash1f53fp8kk470f7k26yr5gztd9npzpczqv4ufud7\"\nChoose your key from the list [0]:\n```\n\n#### Example/expected prompt following successful CLI Booster initialization\n<br/>\n```\nroot@ip-10-0-10-163:~/akash-tools/cli-booster[https://rpc.akash.smartnodes.one:443][deploymentone][]$\n```\n\n### **Create Certificate**\n\nIf this is your first time using the selected key, create a client certificate.\n\n```\nakash_mkcert\n```\n\n## Creating a Deployment with the CLI Booster\n\nIn the series of  steps in this section we will create the deployment, request bids from available providers, and select the most attractive bid.\n\n### Create Deployment\n\n```\nakash_deploy <path-to-Akash-SDL-file>\n```\n\n#### Example/Expected Output\n\n```\nroot@ip-10-0-10-163:~/akashApps/helloWorld[https://rpc.akash.smartnodes.one:443][deploymentone][]$ akash_deploy /root/akashApps/helloWorld/deploy.yml\n\nINFO: Broadcasting 'akash deployment create -y deploy.yml' transaction...\nEnter keyring passphrase:\ngas estimate: 110705\nINFO: Waiting for the TX EB8CB0EC7A09339D8004DD6FF1CE9054189315656687B5B3D5AF70565896A22F to get processed by the Akash network\nINFO: Success\n```\n\n## Select and Accept Bid\n<br/>\n```\nakash_accept\n```\n\n#### Expected/Example Output (Pre-Bid Accept)\n\n* Select a provider of your preference by entering the number of the associated row\n\n```\nroot@ip-10-0-10-163:~/akash-tools/cli-booster[http://akash-sentry01.skynetvalidators.com:26657][deploymentone][4815510-1-1]$ akash_accept\n\nAKASH_PROVIDER was not set so let's try to pick from what's available for your deployment.\n\trate\tmonthly\tusd\tdseq/oseq/gseq\tprovider\t\t\t\t\thost\n0>\t5.00\t2.03\t$2.44\t4815510/1/1\takash10fl5f6ukr8kc03mtmf8vckm6kqqwqpc04eruqa\tprovider.akash.world:8443\n1>\t3.00\t1.21\t$1.45\t4815510/1/1\takash14c4ng96vdle6tae8r4hc2w4ujwrsh3x9tuudk0\tprovider.provider-0.prod.ams1.akash.pub:8443\n2>\t4.00\t1.62\t$1.94\t4815510/1/1\takash19yhu3jgw8h0320av98h8n5qczje3pj3u9u2amp\tprovider.bdl.computer:8443\n3>\t6.00\t2.43\t$2.92\t4815510/1/1\takash1g8m36ge6yekgkfktl08x8vrp0nq9v0l73jzy32\tprovider.xch.computer:8443\n4>\t3.00\t1.21\t$1.45\t4815510/1/1\takash1m7tex89ddnwp3cm63ehfzfe2kj2uxmsugtx2qc\tprovider.provider-0.prod.sjc1.akash.pub:8443\n5>\t3.00\t1.21\t$1.45\t4815510/1/1\takash1r7y2msa9drwjss5umza854he5vwr2czunye9de\tus-east01-akash.qloudit.com:8443\n6>\t2.00\t0.81\t$0.97\t4815510/1/1\takash1u5cdg7k3gl43mukca4aeultuz8x2j68mgwn28e\td3akash.cloud:8443\n7>\t3.00\t1.21\t$1.45\t4815510/1/1\takash1vky0uh4wayh9npd74uqesglpaxwymynnspf6a4\tprovider.provider-2.prod.ewr1.akash.pub:8443\n8>\t3.00\t1.21\t$1.45\t4815510/1/1\takash1x32axkrtkv2et7etdwh77hj9a6vnc8un9th4e9\tsupernaut.ddns.net:8443\n```\n\n#### Expected/Example Out (Post Bid Selection)\n\n```\nINFO: Accepting the bid offered by akash1x32axkrtkv2et7etdwh77hj9a6vnc8un9th4e9 provider for 4748311 deployment\nINFO: Broadcasting 'akash market lease create -y' transaction...\nEnter keyring passphrase:\ngas estimate: 442786\nINFO: Waiting for the TX 5E33B27E631B0FD94BD9FB23D705B8D40F4BDC741829B0F669EC44C31ACA8A9C to get processed by the Akash network\nINFO: Success\n5E33B27E631B0FD94BD9FB23D705B8D40F4BDC741829B0F669EC44C31ACA8A9C\n```\n\n## Send Manifest to Provider\n<br/>\n```\nakash_send_manifest <path-to-Akash-SDL-file>\n```\n\n#### Expected/Example Output\n\n\n\n```\nroot@ip-10-0-10-163:~/akash-tools/cli-booster[https://rpc.akash.forbole.com:443][deploymentone][4748311-1-1]$ akash_send_manifest /root/akashApps/helloWorld/deploy.yml\n\nEnter keyring passphrase:\n[{\"provider\":\"akash1x32axkrtkv2et7etdwh77hj9a6vnc8un9th4e9\",\"status\":\"PASS\"}]\n```\n\n## Status of Deployment\n<br/>\n```\nakash_status\n```\n\n#### Example/Expected Output\n\n```\nroot@ip-10-0-10-163:~/akash-tools/cli-booster[http://akash-sentry01.skynetvalidators.com:26657][deploymentone][4751918-1-1]$ akash_status\n\nEnter keyring passphrase:\n{\n  \"services\": {\n    \"web\": {\n      \"name\": \"web\",\n      \"available\": 1,\n      \"total\": 1,\n      \"uris\": [\n        \"scottsdgsrepprrtt.com\",\n        \"toegk7990pcnb7jefh423r2gdk.ingress.provider-2.prod.ewr1.akash.pub\"\n      ],\n      \"observed_generation\": 1,\n      \"replicas\": 1,\n      \"updated_replicas\": 1,\n      \"ready_replicas\": 1,\n      \"available_replicas\": 1\n    }\n  },\n  \"forwarded_ports\": {}\n}\n```\n\n## Close a Deployment\n<br/>\n```\nakash_close\n```\n\n#### Expected/Sample Output\n\n```\nroot@ip-10-0-10-163:~/akash-tools/cli-booster[http://akash-sentry01.skynetvalidators.com:26657][deploymentone][4751918-1-1]$ akash_close\n\nINFO: Broadcasting 'akash deployment close -y' transaction...\nEnter keyring passphrase:\ngas estimate: 237627\nINFO: Waiting for the TX 7AF03163EC3347712DCD2BBC648D82F70BCE0F681683AFAC537EA5520F8F2785 to get processed by the Akash network\nINFO: Success\n4751918 deployment has been successfully closed.\nINFO: Total spent for 4751918: 129 uakt or $0.0001767\n```\n\n## List All Active Deployments\n<br/>\n```\nakash_deployments\n```\n\n#### Expected/Sample Output\n\n* Select the associated number of a deployment to reveal greater details.  At this time those details include listing the deployment’s current provider.\n* Select an instance to perform actions such as closing a deployment or accessing a deployment’s shell.  The specific deployment must be selected prior to such actions and the selected deployment will be indicated in the CLI’s prompt.\n\n```\nroot@ip-10-0-10-163:~/akash-tools/cli-booster[http://akash-sentry01.skynetvalidators.com:26657][deploymentone][4815131-1-1]$ akash_deployments\n\nDeployments you have accepted the bids for (i.e. have ran lease create).\nFound the following active deployments:\n\t\"dseq\"     \"gseq\"  \"oseq\"\n0>\t\"4815090\"  1       1\n1>\t\"4815131\"  1       1\nChoose your deployment from the list [1]:\nSelected 1: 4815131-1-1\nAKASH_DSEQ=4815131\nLooking for a matching provider for this order...\nAKASH_PROVIDER=akash14c4ng96vdle6tae8r4hc2w4ujwrsh3x9tuudk0\n```\n\n## Access a Deployment’s Shell\n<br/>\n```\nakash_shell sh\n```\n\n#### Example/Expected Output\n\n* In this example the contents of the current directory are listed - via “ls” - to prove we are inside the deployment’s container\n\n```\nroot@ip-10-0-10-163:~/akash-tools/cli-booster[http://akash-sentry01.skynetvalidators.com:26657][deploymentone][4815131-1-1-web]$ akash_shell sh\n\nEnter keyring passphrase:\nEnter keyring passphrase:\n#\n# ls\nbin  boot  dev\tdocker-entrypoint.d  docker-entrypoint.sh  etc\thome  lib  lib64  media  mnt  opt  proc  root  run  sbin  srv  sys  tmp  usr  var\n```\n\n## Obtain a Deployment’s Logs\n<br/>\n```\nakash_logs -f\n```\n\n#### Example/Expected Output\n\n```\nroot@ip-10-0-10-163:~/akash-tools/cli-booster[http://akash-sentry01.skynetvalidators.com:26657][deploymentone][4815131-1-1-web]$ akash_logs -f\n\nEnter keyring passphrase:\n[akash1f53fp8kk470f7k26yr5gztd9npzpczqv4ufud7/4815131/1/1/akash14c4ng96vdle6tae8r4hc2w4ujwrsh3x9tuudk0][web-7d44bb6d6f-n9cbk] /docker-entrypoint.sh: /docker-entrypoint.d/ is not empty, will attempt to perform configuration\n[akash1f53fp8kk470f7k26yr5gztd9npzpczqv4ufud7/4815131/1/1/akash14c4ng96vdle6tae8r4hc2w4ujwrsh3x9tuudk0][web-7d44bb6d6f-n9cbk] /docker-entrypoint.sh: Looking for shell scripts in /docker-entrypoint.d/\n[akash1f53fp8kk470f7k26yr5gztd9npzpczqv4ufud7/4815131/1/1/akash14c4ng96vdle6tae8r4hc2w4ujwrsh3x9tuudk0][web-7d44bb6d6f-n9cbk] /docker-entrypoint.sh: Launching /docker-entrypoint.d/10-listen-on-ipv6-by-default.sh\n[akash1f53fp8kk470f7k26yr5gztd9npzpczqv4ufud7/4815131/1/1/akash14c4ng96vdle6tae8r4hc2w4ujwrsh3x9tuudk0][web-7d44bb6d6f-n9cbk] 10-listen-on-ipv6-by-default.sh: info: Getting the checksum of /etc/nginx/conf.d/default.conf\n[akash1f53fp8kk470f7k26yr5gztd9npzpczqv4ufud7/4815131/1/1/akash14c4ng96vdle6tae8r4hc2w4ujwrsh3x9tuudk0][web-7d44bb6d6f-n9cbk] 10-listen-on-ipv6-by-default.sh: info: Enabled listen on IPv6 in /etc/nginx/conf.d/default.conf\n[akash1f53fp8kk470f7k26yr5gztd9npzpczqv4ufud7/4815131/1/1/akash14c4ng96vdle6tae8r4hc2w4ujwrsh3x9tuudk0][web-7d44bb6d6f-n9cbk] /docker-entrypoint.sh: Launching /docker-entrypoint.d/20-envsubst-on-templates.sh\n[akash1f53fp8kk470f7k26yr5gztd9npzpczqv4ufud7/4815131/1/1/akash14c4ng96vdle6tae8r4hc2w4ujwrsh3x9tuudk0][web-7d44bb6d6f-n9cbk] /docker-entrypoint.sh: Launching /docker-entrypoint.d/30-tune-worker-processes.sh\n[akash1f53fp8kk470f7k26yr5gztd9npzpczqv4ufud7/4815131/1/1/akash14c4ng96vdle6tae8r4hc2w4ujwrsh3x9tuudk0][web-7d44bb6d6f-n9cbk] /docker-entrypoint.sh: Configuration complete; ready for start up\n```","description":null,"slug":"docs/deployments/akash-cli/cli-booster"},{"title":"Installation","body":"\n\n\n\nExplore detailed steps and options of the Akash CLI.  In this guide we will define each environment variable and use within each command.\n\n### Akash Wants to Spotlight Your Work&#x20;\n\nHave an idea for a project to deploy on Akash? Already working on a project? Maybe you’ve already deployed a project (or many projects!) to the network?\n\nWe love seeing what our community is building. Once your deployment is live, head over to our Discord and share the details of your app in our [Deployments channel](https://discord.com/channels/747885925232672829/771909909335506955) and tag @Admin.\n\nOnce you share your app, someone from the Akash team may reach out to spotlight your app across our newsletter, blog, and social media.\n\nThis is a great opportunity to connect with the team at Akash Network and to spotlight your work for our world-class community.\n\n### Overview of Verbose Steps\n\n* [Install Akash](part-1.-install-akash.md)\n* [Create an Account](part-2.-create-an-account.md)\n* [Fund your Account](part-3.-fund-your-account.md)\n* [Configure your Network](part-4.-configure-your-network.md)\n* [Create your Configuration](part-5.-create-your-configuration.md)\n* [Create your Certificate](part-7.-create-your-deployment.md)\n* [Create your Deployment](part-7.-create-your-deployment.md)\n* [View your Bids](../../sandbox/detailed-steps/part-8.-view-your-bids.md)\n* [Create a Lease](part-9.-create-a-lease.md)\n* [Send the Manifest](part-10.-send-the-manifest.md)\n* [Update the Deployment](part-11.-update-the-deployment.md)\n* [Close Deployment](close-deployment.md)\n\n# Install Akash CLI\n\nSelect a tab below to view instructions for MacOS, Linux, or compiling from source.\n\n{% tabs %}\n{% tab title=\"MacOS\" %}\n#### MacOS\n\nThe simplest way to install Akash is using Homebrew using:\n\n```\nbrew untap ovrclk/tap\nbrew tap akash-network/tap\nbrew install akash-provider-services\n```\n\nIf you do not have homebrew, follow the below steps for installing the Akash Binary.\n\n**Download Akash Binary**\n\nThese commands will retrieve the latest, stable version of the Akash software, store the version in a local variable, and install that version.\n\n```\ncd ~/Downloads\n\n#NOTE that this download may take several minutes to complete\ncurl -sfL https://raw.githubusercontent.com/akash-network/provider/main/install.sh | bash\n```\n\n**Move the Akash Binary**\n\nMove the binary file into a directory included in your path\n\n```\nsudo mv ./bin/provider-services /usr/local/bin\n```\n\n**Verify Akash Installation**\n\nVerify the installation by using a simple command to check the Akash version\n\n```\nprovider-services version\n```\n\n**Expect/Example Output**\n\n```\nprovider-services version\n\nv0.4.8\n```\n{% endtab %}\n\n{% tab title=\"Linux\" %}\nThe simplest way to install Akash is using Homebrew using:\n\n```\nbrew untap ovrclk/tap\nbrew tap akash-network/tap\nbrew install akash-provider-services\n```\n\nIf you do not have homebrew, follow the below steps for installing the Akash Binary.\n\n**Download Akash Binary**\n\nThese commands will retrieve the latest, stable version of the Akash software, store the version in a local variable, and install that version.\n\n```\ncd ~\n\napt install jq -y\n\napt install unzip -y\n\ncurl -sfL https://raw.githubusercontent.com/akash-network/provider/main/install.sh | bash\n```\n\n**Add Akash Install Location to User’s Path**\n\nAdd the software’s install location to the user’s path for easy use of Akash commands.\n\n**NOTE:** Below we provide the steps to add the Akash install directory to a user’s path on a Linux Ubuntu server. Please take a look at a guide for your operating system and how to add a directory to a user’s path.\n\nOpen the user’s path file in an editor:\n\n```\nvi /etc/environment\n```\n\nView within text editor prior to the update:\n\n```\nPATH=\"/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin\"\n```\n\nAdd the following directory, which is the Akash install location, to `PATH`. In this example the active user is root. If logged in as another username, replace /root with your current/home directory.\n\n```\n/root/bin\n```\n\nView within the text editor following the update:\n\n```\nPATH=\"/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/root/bin\"\n```\n\n#### Make the Path Active in the Current Session\n\n```\n. /etc/environment\n```\n\n#### Verify Akash Install\n\nDisplay the version of Akash software installed. This confirms the software installed and that the new user path addition worked.\n\n```\nprovider-services version\n```\n\n**Expected/Example Result**\n\n```\nprovider-services version\n\nv0.4.7\n```\n{% endtab %}\n\n{% tab title=\"Source\" %}\n#### From Source\n\nInstalling Akash suite from source:\n\n```\n$ go get -d github.com/akash-network/provider\n$ cd $GOPATH/src/github.com/akash-network/provider\n$ AKASH_NET=\"https://raw.githubusercontent.com/akash-network/net/main/mainnet\"\n$ AKASH_VERSION=\"$(curl -s https://api.github.com/repos/akash-network/provider/releases/latest | jq -r '.tag_name')\"\n$ git checkout \"v$AKASH_VERSION\"\n$ make deps-install\n$ make install\n```\n\nAkash is developed and tested with [golang 1.16+](https://golang.org/). Building requires a working [golang](https://golang.org/) installation, a properly set `GOPATH`, and `$GOPATH/bin` present in `$PATH`.\n\nOnce you have the dependencies properly setup, download and build `akash` using `make install`\n{% endtab %}\n{% endtabs %}\n\n# Create an Account\n\nConfigure the name of your key. The command below will set the name of your key to `myWallet`, run the below command and replace `myWallet` with a name of your choice:\n\n```bash\nAKASH_KEY_NAME=myWallet\n```\n\nVerify you have the shell variables set up . The below command should return the name you've used:\n\n```bash\necho $AKASH_KEY_NAME\n```\n\nWe now need to point Akash to where the keys are stored for your configuration. To do this we will set the AKASH\\_KEYRING\\_BACKEND environmental variable.\n\n```bash\nAKASH_KEYRING_BACKEND=os\n```\n\nCopy and paste this command into Terminal to create an Akash account:\n\n```bash\nprovider-services keys add $AKASH_KEY_NAME\n```\n\nRead the output and save your mnemonic phrase is a safe place. Let's set a Shell Variable in Terminal `AKASH_ACCOUNT_ADDRESS` to save your account address for later.\n\n```bash\nexport AKASH_ACCOUNT_ADDRESS=\"$(provider-services keys show $AKASH_KEY_NAME -a)\"\n\necho $AKASH_ACCOUNT_ADDRESS\n```\n\nNote that if you close your Terminal window this variable will not be saved.\n\n# Fund your Account\n\nA minimum deposit of 5 AKT is required to deploy on Akash, and a small transaction fee is applied to deployment leases paid by the account used to deploy. There are two ways to get funds into your account, buy tokens, and join the Akash community to receive free tokens from the Akash Faucet.\n\n* [Buy AKT from an exchange](../../../tokens-and-wallets/buy.md)\n* [Join the Akash Community to get free AKT](../../../tokens-and-wallets/funding.md)\n\n\n# Configure your Network\n\nFirst configure the base URL (`$AKASH_NET`) for the Akash Network; copy and paste the command below:\n\n```bash\nAKASH_NET=\"https://raw.githubusercontent.com/akash-network/net/main/mainnet\"\n```\n\n## Version\n\nNext configure the version of the Akash Network `AKASH_VERSION`; copy and paste the command below:\n\n```bash\nAKASH_VERSION=\"$(curl -s https://api.github.com/repos/akash-network/provider/releases/latest | jq -r '.tag_name')\"\n```\n\n## Chain ID\n\nThe akash CLI will recogonize `AKASH_CHAIN_ID` environment variable when exported to the shell.\n\n```bash\nexport AKASH_CHAIN_ID=\"$(curl -s \"$AKASH_NET/chain-id.txt\")\"\n```\n\n## Network Node\n\nYou need to select a node on the network to connect to, using an RPC endpoint. To configure the`AKASH_NODE` environment variable use this export command:\n\n```bash\nexport AKASH_NODE=\"$(curl -s \"$AKASH_NET/rpc-nodes.txt\" | shuf -n 1)\"\n```\n\n## Confirm your network variables are setup\n\nYour values may differ depending on the network you're connecting to.\n\n```bash\necho $AKASH_NODE $AKASH_CHAIN_ID $AKASH_KEYRING_BACKEND\n```\n\nYou should see something similar to:\n\n`http://135.181.60.250:26657 akashnet-2 os`\n\n## Set Additional Environment Variables\n\nSet the below set of environment variables to ensure smooth operations\n\n| Variable               | Description                                                                               | Recommended Value |\n| ---------------------- | ----------------------------------------------------------------------------------------- | ----------------- |\n| AKASH\\_GAS             | Gas limit to set per-transaction; set to \"auto\" to calculate sufficient gas automatically | `auto`            |\n| AKASH\\_GAS\\_ADJUSTMENT | Adjustment factor to be multiplied against the estimate returned by the tx simulation     | `1.15`            |\n| AKASH\\_GAS\\_PRICES     | Gas prices in decimal format to determine the transaction fee                             | `0.025uakt`       |\n| AKASH\\_SIGN\\_MODE      | Signature mode                                                                            | `amino-json`      |\n\n```\nexport AKASH_GAS=auto\nexport AKASH_GAS_ADJUSTMENT=1.25\nexport AKASH_GAS_PRICES=0.025uakt\nexport AKASH_SIGN_MODE=amino-json\n```\n\n## Check your Account Balance\n\nCheck your account has sufficient balance by running:\n\n```bash\nprovider-services query bank balances --node $AKASH_NODE $AKASH_ACCOUNT_ADDRESS\n```\n\nYou should see a response similar to:\n\n```\nbalances:\n- amount: \"93000637\"\n  denom: uakt\npagination:\n  next_key: null\n  total: \"0\"\n```\n\nIf you don't have a balance, please see the [funding guide](https://github.com/akash-network/docs/tree/b65f668b212ad1976fb976ad84a9104a9af29770/guides/wallet/funding.md). Please note the balance indicated is denominated in uAKT (AKT x 10^-6), in the above example, the account has a balance of _93 AKT_. We're now setup to deploy.\n\n{% hint style=\"info\" %}\nYour account must have a minimum balance of 5 AKT to create a deployment. This 5 AKT funds the escrow account associated with the deployment and is used to pay the provider for their services. It is recommended you have more than this minimum balance to pay for transaction fees. For more information on escrow accounts, see [here](https://github.com/akash-network/docs/blob/master/guides/cli/detailed-steps/broken-reference/README.md)\n{% endhint %}\n\n\n# Create your Configuration\n\nCreate a deployment configuration [deploy.yaml](https://github.com/akash-network/docs/tree/b65f668b212ad1976fb976ad84a9104a9af29770/guides/deploy/deploy.yml) to deploy the `ovrclk/lunie-light` for [Lunie Light](https://github.com/ovrclk/lunie-light) Node app container using [SDL](https://github.com/akash-network/docs/tree/b65f668b212ad1976fb976ad84a9104a9af29770/sdl/README.md).\n\nYou can use cURL to download the file:\n\n```\ncurl -s https://raw.githubusercontent.com/akash-network/docs/master/guides/deploy/deploy.yml > deploy.yml\n```\n\n### Modify your Configuration\n\nYou may use the sample deployment file as-is or modify it for your own needs as described in our [SDL (Stack Definition Language)](https://github.com/akash-network/docs/blob/master/sdl/README.md) documentation. A typical modification would be to reference your own image instead of our demo app image.\n\n#### EXAMPLE CONFIGURATION:\n\n```bash\ncat > deploy.yml <<EOF\n---\nversion: \"2.0\"\n\nservices:\n  web:\n    image: ovrclk/lunie-light\n    expose:\n      - port: 3000\n        as: 80\n        to:\n          - global: true\n\nprofiles:\n  compute:\n    web:\n      resources:\n        cpu:\n          units: 0.1\n        memory:\n          size: 512Mi\n        storage:\n          size: 512Mi\n  placement:\n    westcoast:\n      attributes:\n        host: akash\n      signedBy:\n        anyOf:\n          - \"akash1365yvmc4s7awdyj3n2sav7xfx76adc6dnmlx63\"\n      pricing:\n        web: \n          denom: uakt\n          amount: 1000\n\ndeployment:\n  web:\n    westcoast:\n      profile: web\n      count: 1\n\nEOF\n```\n\n# Create your Certificate\n\nIn this step we will create a local certificate and then store the certification on the block chain\n\n* NOTE - for those familiar with previous Akash CLI versions the following commands for cert creation have changed. We believe the new command sets in this version make steps more clear.\n* Ensure that prior steps in this guide have been completed and that you have a funded wallet before attempting certificate creation.\n* **Your certificate needs to be created only once per account** and can be used across all deployments.\n\n#### Generate Cert\n\n* Note: If it errors with `Error: certificate error: cannot overwrite certificate`, then add `--overwrite` should you want to overwrite the cert. Normally you can ignore that error and proceed with publishing the cert (next step).\n\n```\nprovider-services tx cert generate client --from $AKASH_KEY_NAME\n```\n\n#### Publish Cert to the Blockchain\n\n```\nprovider-services tx cert publish client --from $AKASH_KEY_NAME\n```\n\n# Create your Deployment\n\n## CPU Support\n\nOnly x86\\_64 processors are officially supported for Akash deployments. This may change in the future and when ARM processors are supported it will be announced and documented.\n\n## Akash Deployment\n\n> _**NOTE**_ - if your current terminal session has been used to create prior deployments, issue the command `unset AKASH_DSEQ` to prevent receipt of error message `Deployment Exists`\n\nTo deploy on Akash, run:\n\n```\nprovider-services tx deployment create deploy.yml --from $AKASH_KEY_NAME \n```\n\nYou should see a response similar to:\n\n```javascript\n{\n  \"height\":\"140325\",\n  \"txhash\":\"2AF4A01B9C3DE12CC4094A95E9D0474875DFE24FD088BB443238AC06E36D98EA\",\n  \"codespace\":\"\",\n  \"code\":0,\n  \"data\":\"0A130A116372656174652D6465706C6F796D656E74\",\n  \"raw_log\":\"[{\\\"events\\\":[{\\\"type\\\":\\\"akash.v1\\\",\\\"attributes\\\":[{\\\"key\\\":\\\"module\\\",\\\"value\\\":\\\"deployment\\\"},{\\\"key\\\":\\\"action\\\",\\\"value\\\":\\\"deployment-created\\\"},{\\\"key\\\":\\\"version\\\",\\\"value\\\":\\\"2b86f778de8cc9df415490efa162c58e7a0c297fbac9cdb8d6c6600eda56f17e\\\"},{\\\"key\\\":\\\"owner\\\",\\\"value\\\":\\\"akash1vn06ycjjnvsvl639fet9lajjctuturrtx7fvuj\\\"},{\\\"key\\\":\\\"dseq\\\",\\\"value\\\":\\\"140324\\\"},{\\\"key\\\":\\\"module\\\",\\\"value\\\":\\\"market\\\"},{\\\"key\\\":\\\"action\\\",\\\"value\\\":\\\"order-created\\\"},{\\\"key\\\":\\\"owner\\\",\\\"value\\\":\\\"akash1vn06ycjjnvsvl639fet9lajjctuturrtx7fvuj\\\"},{\\\"key\\\":\\\"dseq\\\",\\\"value\\\":\\\"140324\\\"},{\\\"key\\\":\\\"gseq\\\",\\\"value\\\":\\\"1\\\"},{\\\"key\\\":\\\"oseq\\\",\\\"value\\\":\\\"1\\\"}]},{\\\"type\\\":\\\"message\\\",\\\"attributes\\\":[{\\\"key\\\":\\\"action\\\",\\\"value\\\":\\\"create-deployment\\\"},{\\\"key\\\":\\\"sender\\\",\\\"value\\\":\\\"akash1vn06ycjjnvsvl639fet9lajjctuturrtx7fvuj\\\"},{\\\"key\\\":\\\"sender\\\",\\\"value\\\":\\\"akash1vn06ycjjnvsvl639fet9lajjctuturrtx7fvuj\\\"}]},{\\\"type\\\":\\\"transfer\\\",\\\"attributes\\\":[{\\\"key\\\":\\\"recipient\\\",\\\"value\\\":\\\"akash17xpfvakm2amg962yls6f84z3kell8c5lazw8j8\\\"},{\\\"key\\\":\\\"sender\\\",\\\"value\\\":\\\"akash1vn06ycjjnvsvl639fet9lajjctuturrtx7fvuj\\\"},{\\\"key\\\":\\\"amount\\\",\\\"value\\\":\\\"5000uakt\\\"},{\\\"key\\\":\\\"recipient\\\",\\\"value\\\":\\\"akash14pphss726thpwws3yc458hggufynm9x77l4l2u\\\"},{\\\"key\\\":\\\"sender\\\",\\\"value\\\":\\\"akash1vn06ycjjnvsvl639fet9lajjctuturrtx7fvuj\\\"},{\\\"key\\\":\\\"amount\\\",\\\"value\\\":\\\"5000000uakt\\\"}]}]}]\",\n  \"logs\":[\n    {\n      \"msg_index\":0,\n      \"log\":\"\",\n      \"events\":[\n        {\n          \"type\":\"akash.v1\",\n          \"attributes\":[\n            {\n              \"key\":\"module\",\n              \"value\":\"deployment\"\n            },\n            {\n              \"key\":\"action\",\n              \"value\":\"deployment-created\"\n            },\n            {\n              \"key\":\"version\",\n              \"value\":\"2b86f778de8cc9df415490efa162c58e7a0c297fbac9cdb8d6c6600eda56f17e\"\n            },\n            {\n              \"key\":\"owner\",\n              \"value\":\"akash1vn06ycjjnvsvl639fet9lajjctuturrtx7fvuj\"\n            },\n            {\n              \"key\":\"dseq\",\n              \"value\":\"140324\"\n            },\n            {\n              \"key\":\"module\",\n              \"value\":\"market\"\n            },\n            {\n              \"key\":\"action\",\n              \"value\":\"order-created\"\n            },\n            {\n              \"key\":\"owner\",\n              \"value\":\"akash1vn06ycjjnvsvl639fet9lajjctuturrtx7fvuj\"\n            },\n            {\n              \"key\":\"dseq\",\n              \"value\":\"140324\"\n            },\n            {\n              \"key\":\"gseq\",\n              \"value\":\"1\"\n            },\n            {\n              \"key\":\"oseq\",\n              \"value\":\"1\"\n            }\n          ]\n        },\n        {\n          \"type\":\"message\",\n          \"attributes\":[\n            {\n              \"key\":\"action\",\n              \"value\":\"create-deployment\"\n            },\n            {\n              \"key\":\"sender\",\n              \"value\":\"akash1vn06ycjjnvsvl639fet9lajjctuturrtx7fvuj\"\n            },\n            {\n              \"key\":\"sender\",\n              \"value\":\"akash1vn06ycjjnvsvl639fet9lajjctuturrtx7fvuj\"\n            }\n          ]\n        },\n        {\n          \"type\":\"transfer\",\n          \"attributes\":[\n            {\n              \"key\":\"recipient\",\n              \"value\":\"akash17xpfvakm2amg962yls6f84z3kell8c5lazw8j8\"\n            },\n            {\n              \"key\":\"sender\",\n              \"value\":\"akash1vn06ycjjnvsvl639fet9lajjctuturrtx7fvuj\"\n            },\n            {\n              \"key\":\"amount\",\n              \"value\":\"5000uakt\"\n            },\n            {\n              \"key\":\"recipient\",\n              \"value\":\"akash14pphss726thpwws3yc458hggufynm9x77l4l2u\"\n            },\n            {\n              \"key\":\"sender\",\n              \"value\":\"akash1vn06ycjjnvsvl639fet9lajjctuturrtx7fvuj\"\n            },\n            {\n              \"key\":\"amount\",\n              \"value\":\"5000000uakt\"\n            }\n          ]\n        }\n      ]\n    }\n  ],\n  \"info\":\"\",\n  \"gas_wanted\":\"100000\",\n  \"gas_used\":\"94653\",\n  \"tx\":null,\n  \"timestamp\":\"\"\n}\n```\n\n### Find your Deployment \\#\n\nFind the Deployment Sequence (DSEQ) in the deployment you just created. You will need to replace the AKASH\\_DSEQ with the number from your deployment to configure a shell variable.\n\n```bash\nexport AKASH_DSEQ=CHANGETHIS\n```\n\nNow set the Order Sequence (OSEQ) and Group Sequence (GSEQ). Note that if this is your first time deploying on Akash, OSEQ and GSEQ will be 1.\n\n```bash\nAKASH_OSEQ=1\nAKASH_GSEQ=1\n```\n\nVerify we have the right values populated by running:\n\n```bash\necho $AKASH_DSEQ $AKASH_OSEQ $AKASH_GSEQ\n```\n\n\n# View your Bids\n\nAfter a short time, you should see bids from providers for this deployment with the following command:\n\n```bash\nprovider-services query market bid list --owner=$AKASH_ACCOUNT_ADDRESS --node $AKASH_NODE --dseq $AKASH_DSEQ --state=open\n```\n\n### Choose a Provider\n\nNote that there are bids from multiple different providers. In this case, both providers happen to be willing to accept a price of _1 uAKT_. This means that the lease can be created using _1 uAKT_ or _0.000001 AKT_ per block to execute the container. You should see a response similar to:\n\n```\nbids:\n- bid:\n    bid_id:\n      dseq: \"140324\"\n      gseq: 1\n      oseq: 1\n      owner: akash1vn06ycjjnvsvl639fet9lajjctuturrtx7fvuj\n      provider: akash10cl5rm0cqnpj45knzakpa4cnvn5amzwp4lhcal\n    created_at: \"140326\"\n    price:\n      amount: \"1\"\n      denom: uakt\n    state: open\n  escrow_account:\n    balance:\n      amount: \"50000000\"\n      denom: uakt\n    id:\n      scope: bid\n      xid: akash1vn06ycjjnvsvl639fet9lajjctuturrtx7fvuj/140324/1/1/akash10cl5rm0cqnpj45knzakpa4cnvn5amzwp4lhcal\n    owner: akash10cl5rm0cqnpj45knzakpa4cnvn5amzwp4lhcal\n    settled_at: \"140326\"\n    state: open\n    transferred:\n      amount: \"0\"\n      denom: uakt\n- bid:\n    bid_id:\n      dseq: \"140324\"\n      gseq: 1\n      oseq: 1\n      owner: akash1vn06ycjjnvsvl639fet9lajjctuturrtx7fvuj\n      provider: akash1f6gmtjpx4r8qda9nxjwq26fp5mcjyqmaq5m6j7\n    created_at: \"140326\"\n    price:\n      amount: \"1\"\n      denom: uakt\n    state: open\n  escrow_account:\n    balance:\n      amount: \"50000000\"\n      denom: uakt\n    id:\n      scope: bid\n      xid: akash1vn06ycjjnvsvl639fet9lajjctuturrtx7fvuj/140324/1/1/akash1f6gmtjpx4r8qda9nxjwq26fp5mcjyqmaq5m6j7\n    owner: akash1f6gmtjpx4r8qda9nxjwq26fp5mcjyqmaq5m6j7\n    settled_at: \"140326\"\n    state: open\n    transferred:\n      amount: \"0\"\n      denom: uakt\n```\n\nFor this example, we will choose `akash10cl5rm0cqnpj45knzakpa4cnvn5amzwp4lhcal` Run this command to set the provider shell variable:\n\n```\nAKASH_PROVIDER=akash10cl5rm0cqnpj45knzakpa4cnvn5amzwp4lhcal\n```\n\nVerify we have the right value populated by running:\n\n```\necho $AKASH_PROVIDER\n```\n\n# Create a Lease\n\nCreate a lease for the bid from the chosen provider above by running this command:\n\n```\nprovider-services tx market lease create --dseq $AKASH_DSEQ --provider $AKASH_PROVIDER --from $AKASH_KEY_NAME\n```\n\n### Confirm the Lease\n\nYou can check the status of your lease by running:\n\n```\nprovider-services query market lease list --owner $AKASH_ACCOUNT_ADDRESS --node $AKASH_NODE --dseq $AKASH_DSEQ\n```\n\nNote the bids will close automatically after 5 minutes, and you may get the response:\n\n```\nbid not open\n```\n\nIf this happens, close your deployment and open a new deployment again. To close your deployment run this command:\n\n```\nprovider-services tx deployment close --dseq $AKASH_DSEQ  --owner $AKASH_ACCOUNT_ADDRESS --from $AKASH_KEY_NAME \n```\n\nIf your lease was successful you should see a response that ends with:\n\n```\n    state: active\n```\n\n{% hint style=\"info\" %}\nPlease note that once the lease is created, the provider will begin debiting your deployment's escrow account, even if you have not completed the deployment process by uploading the manifest in the following step.\n{% endhint %}\n\n# Send the Manifest\n\nUpload the manifest using the values from above step:\n\n```\nprovider-services send-manifest deploy.yml --dseq $AKASH_DSEQ --provider $AKASH_PROVIDER --from $AKASH_KEY_NAME\n```\n\n### Confirm the URL\n\nNow that the manifest is uploaded, your image is deployed. You can retrieve the access details by running the below:\n\n```\nprovider-services lease-status --dseq $AKASH_DSEQ --from $AKASH_KEY_NAME --provider $AKASH_PROVIDER\n```\n\nYou should see a response similar to:\n\n```javascript\n{\n  \"services\": {\n    \"web\": {\n      \"name\": \"web\",\n      \"available\": 1,\n      \"total\": 1,\n      \"uris\": [\n        \"rga3h05jetf9h3p6dbk62m19ck.ingress.ewr1p0.mainnet.akashian.io\"\n      ],\n      \"observed_generation\": 1,\n      \"replicas\": 1,\n      \"updated_replicas\": 1,\n      \"ready_replicas\": 1,\n      \"available_replicas\": 1\n    }\n  },\n  \"forwarded_ports\": {}\n}\n```\n\nYou can access the application by visiting the hostnames mapped to your deployment. Look for a URL/URI and copy it to your web browser.\n\n### View your logs\n\nYou can view your application logs to debug issues or watch progress like so:\n\n```bash\nprovider-services lease-logs \\\n  --dseq \"$AKASH_DSEQ\" \\\n  --provider \"$AKASH_PROVIDER\" \\\n  --from \"$AKASH_KEY_NAME\"\n```\n\n# Send the Manifest\n\nUpload the manifest using the values from above step:\n\n```\nprovider-services send-manifest deploy.yml --dseq $AKASH_DSEQ --provider $AKASH_PROVIDER --from $AKASH_KEY_NAME\n```\n\n### Confirm the URL\n\nNow that the manifest is uploaded, your image is deployed. You can retrieve the access details by running the below:\n\n```\nprovider-services lease-status --dseq $AKASH_DSEQ --from $AKASH_KEY_NAME --provider $AKASH_PROVIDER\n```\n\nYou should see a response similar to:\n\n```javascript\n{\n  \"services\": {\n    \"web\": {\n      \"name\": \"web\",\n      \"available\": 1,\n      \"total\": 1,\n      \"uris\": [\n        \"rga3h05jetf9h3p6dbk62m19ck.ingress.ewr1p0.mainnet.akashian.io\"\n      ],\n      \"observed_generation\": 1,\n      \"replicas\": 1,\n      \"updated_replicas\": 1,\n      \"ready_replicas\": 1,\n      \"available_replicas\": 1\n    }\n  },\n  \"forwarded_ports\": {}\n}\n```\n\nYou can access the application by visiting the hostnames mapped to your deployment. Look for a URL/URI and copy it to your web browser.\n\n### View your logs\n\nYou can view your application logs to debug issues or watch progress like so:\n\n```bash\nprovider-services lease-logs \\\n  --dseq \"$AKASH_DSEQ\" \\\n  --provider \"$AKASH_PROVIDER\" \\\n  --from \"$AKASH_KEY_NAME\"\n```\n\n\n# Close Deployment\n\n### Close the Deployment\n\nShould you need to close the deployment follow this step.\n\n```\nprovider-services tx deployment close --from $AKASH_KEY_NAME\n```\n","description":null,"slug":"docs/deployments/akash-cli/installation"},{"title":"Overview","body":"\nThis guide will walk you through installing the Akash CLI, creating and funding an account on an Akash network, and deploying a single-tier web application.\n\nThe  command-line client is used to interact with deployments. The `provider-services` command prefix and additional command syntax covered in this guide are utilized to query, launch, and update your applications.\n\nWe offer two guides to assist in getting started with the Akash CLI:\n\n* [Detailed Steps](/docs/docs/deployments/akash-cli/installation/)\n* [Akash CLI Booster](/docs/docs/deployments/akash-cli/cli-booster/)\n\nThe Akash CLI Booster was created to make the use of the current command line tooling much simpler for deployments and related activities. In this guide we will review the primary capabilities of this exciting new tool.\n\nThe Akash CLI Booster will receive continued updates and new features.  Review the latest updates and through list of current features [here](https://github.com/arno01/akash-tools/tree/main/cli-booster).\n\n### Akash Wants to Spotlight Your Work&#x20;\n\nHave an idea for a project to deploy on Akash? Already working on a project? Maybe you’ve already deployed a project (or many projects!) to the network?\n\nWe love seeing what our community is building. Once your deployment is live, head over to our Discord and share the details of your app in our [Deployments channel](https://discord.com/channels/747885925232672829/771909909335506955) and tag @Admin.\n\nOnce you share your app, someone from the Akash team may reach out to spotlight your app across our newsletter, blog, and social media.\n\nThis is a great opportunity to connect with the team at Akash Network and to spotlight your work for our world-class community.\n\n","description":null,"slug":"docs/deployments/akash-cli/overview"},{"title":"Apps on Akash","body":"\nAwesome Akash is a curated list of awesome resources people can use to familiarize themselves with [Akash](https://akash.network) and includes examples of several applications they can deploy on the platform. Please submit a pull request if you know any resources that might be helpful to other developers.\n\n**Repository**: [akash-network/awesome-akash](https://github.com/akash-network/awesome-akash)\n\n**Instructions:** [how to deploy](https://docs.akash.network/guides/deploy) the SDL files in this repository\n\nJoin our [discord](https://discord.akash.network) if you have questions or concerns. Our team is always eager to hear from you. Also, follow [@akashnet\\_](https://twitter.com/akashnet\\_) to stay in the loop with updates and announcements.\n\n## Table of Contents\n\n### Official\n\n* [Lunie Wallet for Cosmos SDK](https://github.com/akash-network/awesome-akash/blob/master/lunie-lite)\n* [Cosmos SDK Node](https://github.com/ovrclk/akash-on-akash)\n\n### DeFi\n\nAwesome DeFi apps you can deploy on Akash\n\n* [Serum DEX UI](https://github.com/akash-network/awesome-akash/blob/master/serum)\n* [Uniswap](https://github.com/akash-network/awesome-akash/blob/master/uniswap)\n* [dFed](https://github.com/akash-network/awesome-akash/blob/master/dfed)\n* [Pancake Swap](https://github.com/akash-network/awesome-akash/blob/master/pancake-swap)\n* [Augur](https://github.com/akash-network/awesome-akash/blob/master/augur)\n* [Bancor](https://github.com/akash-network/awesome-akash/blob/master/Bancor)\n* [Balancer](https://github.com/akash-network/awesome-akash/blob/master/Balancer)\n* [Luaswap](https://github.com/akash-network/awesome-akash/blob/master/luaswap)\n* [SushiSwap](https://github.com/akash-network/awesome-akash/blob/master/sushiswap)\n* [Uma Protocol](https://github.com/akash-network/awesome-akash/blob/master/uma-protocol)\n* [Yearn.finance](https://github.com/akash-network/awesome-akash/blob/master/Yearn.finance)\n* [ThorChain BEPSwap](https://github.com/akash-network/awesome-akash/blob/master/Thorchain-BEPSwap)\n* [Curve](https://github.com/akash-network/awesome-akash/blob/master/curve)\n* [Synthetix.Exchange](https://github.com/akash-network/awesome-akash/blob/master/synthetix.exchange)\n* [Ren Protocol](https://github.com/akash-network/awesome-akash/blob/master/renprotocol)\n* [yfii](https://github.com/akash-network/awesome-akash/blob/master/yfii)\n\n### Blogging\n\n* [SteemCN](https://github.com/akash-network/awesome-akash/blob/master/steemcn)\n* [Ghost](https://github.com/akash-network/awesome-akash/blob/master/ghost)\n* [Grav](https://github.com/akash-network/awesome-akash/blob/master/Grav)\n* [Wordpress](https://github.com/akash-network/awesome-akash/blob/master/wordpress)\n* [Confluence](https://github.com/akash-network/awesome-akash/blob/master/confluence)\n* [Drupal](https://github.com/akash-network/awesome-akash/blob/master/drupal)\n* [Wiki.js](https://github.com/akash-network/awesome-akash/blob/master/wikijs)\n\n### Games\n\n* [Tetris](https://github.com/akash-network/awesome-akash/blob/master/tetris)\n* [Pac-Man](https://github.com/akash-network/awesome-akash/blob/master/pacman)\n* [Supermario](https://github.com/akash-network/awesome-akash/blob/master/supermario)\n* [Minesweeper](https://github.com/akash-network/awesome-akash/blob/master/minesweeper)\n* [Tetris2](https://github.com/akash-network/awesome-akash/blob/master/tetris2)\n* [MemoryGame](https://github.com/akash-network/awesome-akash/blob/master/Memorygame)\n* [DOOM](https://github.com/akash-network/awesome-akash/blob/master/doom)\n* [Snake Game](https://github.com/akash-network/awesome-akash/blob/master/snake-game)\n\n### Databases and Administration\n\n* [pgAdmin](https://github.com/akash-network/awesome-akash/blob/master/pgadmin4)\n* [mongoDB](https://github.com/akash-network/awesome-akash/blob/master/mongoDB)\n* [postgresSQL](https://github.com/akash-network/awesome-akash/blob/master/postgres)\n* [adminer](https://github.com/akash-network/awesome-akash/blob/master/adminer)\n* [MySQL](https://github.com/akash-network/awesome-akash/blob/master/MySQL)\n* [CouchDB](https://github.com/akash-network/awesome-akash/blob/master/couchdb)\n* [InfluxDB](https://github.com/akash-network/awesome-akash/blob/master/influxdb)\n\n### Hosting\n\n* [Caddy](https://github.com/akash-network/awesome-akash/blob/master/caddy)\n* [Grafana](https://github.com/akash-network/awesome-akash/blob/master/grafana)\n\n### Continuous Integration\n\n* [Jenkins](https://github.com/akash-network/awesome-akash/blob/master/jenkins)\n* [Bitbucket](https://github.com/akash-network/awesome-akash/blob/master/bitbucket)\n\n### Project Management\n\n* [Jira Software](https://github.com/akash-network/awesome-akash/blob/master/jira)\n* [Redmine](https://github.com/akash-network/awesome-akash/blob/master/redmine)\n\n### Tools\n\n* [DEGO Stats](https://github.com/akash-network/awesome-akash/blob/master/dego-stats)\n* [authsteem](https://github.com/akash-network/awesome-akash/blob/master/authsteem)\n* [microbox](https://github.com/akash-network/awesome-akash/blob/master/microbox)\n* [Mintr](https://github.com/akash-network/awesome-akash/blob/master/mintr)\n* [Folding@home](https://github.com/akash-network/awesome-akash/blob/master/folding-at-home)\n* [owncloud](https://github.com/akash-network/awesome-akash/blob/master/owncloud)\n* [Quill editor](https://github.com/akash-network/awesome-akash/blob/master/quill-editor)\n* [Periodic Table Creator](https://github.com/akash-network/awesome-akash/blob/master/Periodic-Table-Creator)\n* [dart-hello](https://github.com/akash-network/awesome-akash/blob/master/dart)\n\n### Wallet\n\n* [MyetherWallet](https://github.com/akash-network/awesome-akash/blob/master/MyetherWallet)\n* [tronwallet](https://github.com/akash-network/awesome-akash/blob/master/tronwallet)\n\n### Built with Cosmos-SDK\n\n* [Dharani](https://github.com/akash-network/awesome-akash/blob/master/Dharani)\n* [Big Dipper](https://github.com/akash-network/awesome-akash/blob/master/big-dipper)\n\n### Media\n\n* [FreeFlix Nucleus](https://github.com/akash-network/awesome-akash/blob/master/freeflix-nucleus)\n\n### Data Visualization\n\n* [UFO Sightings](https://github.com/akash-network/awesome-akash/blob/master/ufo-data-vis)\n\n### Chat\n\n* [Mattermost](https://github.com/akash-network/awesome-akash/blob/master/mattermost)\n\n\n","description":null,"slug":"docs/deployments/apps-on-akash"},{"title":"Cloudmos Deploy","body":"\nCloudmos Deploy Tool is a web based application which simplifies the deployment process on the Akash Network. Post deployment the tool provides a dashboard to view the status and details of workloads. The dashboard also has the ability to perform administrative tasks including closing the deployment, updating the deployment, redeploying, and increasing the funding available to the deployment.\n\nThis guide will cover the following topics:\n\n* [Cloudmos Deploy Access](#cloudmos-deploy-access)\n* [Minecraft Deployment Example](#minecraft-deployment-example)\n* [Manage Deployments](#manage-deployments)\n* [Define a custom RPC node](#custom-rpc-node)\n\n### Akash Wants to Spotlight Your Work\n\nHave an idea for a project to deploy on Akash? Already working on a project? Maybe you’ve already deployed a project (or many projects!) to the network?\n\nWe love seeing what our community is building. Once your deployment is live, head over to our Discord and share the details of your app in our [Deployments channel](https://discord.com/channels/747885925232672829/771909909335506955) and tag @Admin.\n\nOnce you share your app, someone from the Akash team may reach out to spotlight your app across our newsletter, blog, and social media.\n\nThis is a great opportunity to connect with the team at Akash Network and to spotlight your work for our world-class community.\n\n\n## Cloudmos Deploy Access\n\n### **Before Getting Started**\n\nThe Keplr and Leap browser extensions must be installed and with sufficient funds (5 AKT minimum for a single deployment plus a small amount for transaction fees).\n\nFollow our [Keplr Wallet](/docs/docs/getting-started/token-and-wallets/#keplr-wallet) and [Leap Cosmos Wallet](/docs/docs/getting-started/token-and-wallets/#leap-cosmos-wallet) guides to create your first wallet if necessary.\n\n### **Cloudmos Deploy Access**\n\nThe Cloudmos Deploy web app is available via the following URL:\n\n* [https://deploy.cloudmos.io/](https://deploy.cloudmos.io/)\n\n## Keplr Account Selection\n\nEnsure that an Akash account with sufficient AKT balance is selected in Keplr prior to proceeding with subsequent steps.\n\n\n![](../../../assets/keplr_wallet.png)\n\n## Leap Account Selection\n\nEnsure that an Akash account with sufficient AKT balance is selected in Leap prior to proceeding with subsequent steps.\n\n\n\n![](../../../assets/leapwallet.png)\n\n\n\n## Minecraft Deployment Example\n\nIn this section we will use Cloudmos Deploy to launch an example Minecraft deployment on the Akash Network. You can follow the same process for any other workload so long as it is containerized and you have an appropriate SDL.\n\n#### **STEP 1 -  Create the Deployment**\n\n* From the Dashboard/primary pane click the `CREATE DEPLOYMENT` button\n\n![](../../../assets/cloudCreateDeployment.png)\n\n#### **STEP 2 - Create Certficate**\n\n* A number of checks are performed to ensure necessary funds and certificates are available to launch a deployment.\n* If this is your first deployment with Cloudmos Deploy a `CREATE CERTIFICATE` prompt will be displayed.  Select the `CREATE CERTIFICATE` button and accept transaction fee prompt from Keplr to proceed.\n\n\n![](../../../assets/cloudmosCreateCert.png)\n\n**STEP 3 -  Choose Deployment Template**\n\n* The tool provides several sample templates launch of popular applications\n* Select the `Minecraft` template for our initial deployment\n\n\n![](../../../assets/cloudmosSelectTemplate-1.png)\n\n#### **STEP 4 - Proceed with Deployment**\n\n* At this stage we could review/customize the Akash SDL deployment template if desired but in this example we will proceed with the default settings and by pressing the `DEPLOY` button\n\n\n![](../../../assets/cloudmosProceedWithDeployment.png)\n\n#### **STEP 5 - SDL Verification**\n\n* Screen will appear which provides an additional opportunity to customize the SDL but we will again proceed with no edits by pressing the `CREATE DEPLOYMENT` button\n* Subsequently accept the Deployment Deposit pop up which specifies that 5AKT will be placed into an escrow account for deployment cost and then the Keplr transaction fee prompt\n\n![](../../../assets/cloudmosSDLReview.png)\n\n#### **STEP 6 -  Review/Accept Bid**\n\n* After a minute or so a list of bids will display.&#x20;\n* Select the most affordable, preferred provider and then press `ACCEPT BID`\n\n> _**NOTE**_ - list of bids and providers may be different in your deployment\n\n* Accept the Keplr transaction fee prompt to proceed\n\n\n\n![](../../../assets/cloudmosAcceptBid.png)\n\n#### **STEP 7 -  Post Deployment**\n\n* While the deployment is becoming active the `LOGS` tab is displayed.\n* When the deployment appears to be complete, select the `LEASES` tab.\n* The `LEASES` tab confirms the successful deployment of our example Minecraft application\n\n\n\n![](../../../assets/cloudmosLeasesTab.png)\n\n## Manage Deployments\n\n\n\nThere are a several important management operations you can do with the Cloudmos Deploy tool including:\n\n* [Add funds to existing deployment’s Escrow Account](#add-funding-to-active-deployment)\n* [Close an active deployment](#close-active-deployment)\n\n### **Deployment Dashboard Overview**\n\n* To get an overview of what you have deployed click the `Dashboard` button on the left hand navigation pane\n* From the Active Deployments window you can see the resources that are dedicated to each deployment.\n\n\n\n![](../../../assets/cloudmosDashboard.png)\n\n### **Add Funding to Active Deployment**\n\n* If your escrow for a deployment is running low you will need to add some funds.\n* Within the `Dashboard` pane, isolate the deployment of interest, select the `...` option to expand options, and select `Deposit`\n\n\n![](../../../assets/cloudmosAddFunds.png)\n\n* A dialog box will pop up allowing you to add tokens to the deployment’s escrow account\n* Select the `DEPOSIT` button once you have put in the correct amount\n* As always you must confirm the gas fees and transaction to the blockchain by clicking “APPROVE”\n\n\n\n![](../../../assets/cloudmosSpecifyDeposit.png)\n\n* Notice the balance change in the escrow account for the deployment\n\n\n\n![](../../../assets/cloudmosDepositUpdated.png)\n\n### **Close Active Deployment**\n\nClosing a deployment is very simple.\n\n* Visit the `Dashboard` pane and click the deployment you want to close\n* Within the `Dashboard` pane, isolate the deployment of interest, select the `...` option to expand options, and select `Close`\n* Confirm the transaction to the blockchain\n* The deployment should now be removed from the list\n\n\n\n![](../../../assets/cloudmosDeploymentClose.png)\n\n\n# Custom RPC Node\n\nSpecify a custom RPC or API node within Cloudmos Deploy by using the steps outlined in this section.&#x20;\n\nThe custom node option can point to a RPC node we have created and manage ourselves. Or we can point to an alternative public RPC node that was not selected by Cloudmos Deploy auto selection.\n\n### STEP 1 - Enable Custom Node Use\n\n* Begin by selecting the drop-down next to the current Node and then click the `Custom node`radio button\n\n![](../../../assets/cloudmosCustomNode.png)\n\n### STEP 2 - Edit the RPC and API Nodes\n\nPress the `EDIT` button to enter the screen where we can enter our preferred nodes\n\n\n![](../../../assets/cloudmosSpecifyCustomNode.png)\n\n### STEP 3 - Specify Preferred RPC and API Nodes\n\nUse the `Api Endpoint` and `Rpc Endpoint` fields to define your own managed nodes or preferred public nodes.\\\n\\\nIf you do not maintain your own nodes and want to select from a list of popular public nodes, choose from the RPC node list [here](https://github.com/akash-network/net/blob/master/mainnet/rpc-nodes.txt) and the API node list [here](https://github.com/akash-network/net/blob/master/mainnet/api-nodes.txt).\n\n \n![](../../../assets/cloudmosAcceptCustomNodes.png)\n\n","description":null,"slug":"docs/deployments/cloudmos-deploy"},{"title":"Overview","body":"\nApplications can be deployed onto the Akash network using a platform that best suits your needs and preferences. Explore the following guides for your unique use case.\n\n* [Cloudmos Deploy (web app)](/docs/docs/deployments/cloudmos-deploy/)\n* [Using the Command Line for Akash Deployments ](/docs/docs/deployments/akash-cli/installation/)\n* [Akash Sandbox Use](/docs/docs/deployments/sandbox/introduction/)\n* [Awesome Akash Example Distributed Applications](/docs/docs/deployments/apps-on-akash/)\n\n## Best Practices\n\n### Image Declaration\n\n* Avoid using `:latest` image tags as Akash Providers heavily cache images\n* Additional info on related SDL declarations is available [here](https://docs.akash.network/readme/stack-definition-language#services)","description":null,"slug":"docs/deployments/overview"},{"title":"Installation","body":"\n## Akash CLI for Sandbox Use\n\nExplore detailed steps and options of the Akash CLI.  In this guide we will define each environment variable and use within each command.\n\n### Overview of Detailed Steps\n\n- [Akash CLI for Sandbox Use](#akash-cli-for-sandbox-use)\n  - [Overview of Detailed Steps](#overview-of-detailed-steps)\n- [Install Akash CLI](#install-akash-cli)\n    - [MacOS](#macos)\n    - [Make the Path Active in the Current Session](#make-the-path-active-in-the-current-session)\n    - [Verify Akash Install](#verify-akash-install)\n    - [From Source](#from-source)\n- [Create an Account](#create-an-account)\n- [Fund your Account](#fund-your-account)\n  - [Expected Result](#expected-result)\n- [Configure your Network](#configure-your-network)\n  - [Version](#version)\n  - [Chain ID](#chain-id)\n  - [Network Node](#network-node)\n- [Confirm your network variables are setup](#confirm-your-network-variables-are-setup)\n- [Check your Account Balance](#check-your-account-balance)\n- [Create your Configuration](#create-your-configuration)\n  - [Modify your Configuration](#modify-your-configuration)\n    - [EXAMPLE CONFIGURATION:](#example-configuration)\n- [Create your Certificate](#create-your-certificate)\n    - [Generate Cert](#generate-cert)\n    - [Publish Cert to the Blockchain](#publish-cert-to-the-blockchain)\n- [Create your Deployment](#create-your-deployment)\n  - [CPU Support](#cpu-support)\n  - [Akash Deployment](#akash-deployment)\n  - [Find your Deployment #](#find-your-deployment-)\n- [View your Bids](#view-your-bids)\n  - [Choose a Provider](#choose-a-provider)\n- [Create a Lease](#create-a-lease)\n  - [Confirm the Lease](#confirm-the-lease)\n- [Send the Manifest](#send-the-manifest)\n  - [Confirm the URL](#confirm-the-url)\n  - [View your logs](#view-your-logs)\n- [Update the Deployment](#update-the-deployment)\n  - [Update the Manifest](#update-the-manifest)\n  - [Issue Transaction for On Chain Update](#issue-transaction-for-on-chain-update)\n  - [Send Updated Manifest to Provider](#send-updated-manifest-to-provider)\n- [Close Deployment](#close-deployment)\n  - [Close the Deployment](#close-the-deployment)\n\n\n## Install Akash CLI\n\nSelect a tab below to view instructions for MacOS, Linux, or compiling from source.\n\n{% tabs %}\n{% tab title=\"MacOS\" %}\n#### MacOS\n\nThe simplest way to install Akash is using Homebrew using:\n\n```\nbrew untap ovrclk/tap\nbrew tap akash-network/tap\nbrew install akash-provider-services\n```\n\nIf you do not have homebrew, follow the below steps for installing the Akash Binary.\n\n**Download Akash Binary**\n\nThese commands will retrieve the latest, stable version of the Akash software, store the version in a local variable, and install that version.\n\n```\ncd ~/Downloads\n\n#NOTE that this download may take several minutes to complete\ncurl -sfL https://raw.githubusercontent.com/akash-network/provider/main/install.sh | bash\n```\n\n**Move the Akash Binary**\n\nMove the binary file into a directory included in your path\n\n```\nsudo mv ./bin/provider-services /usr/local/bin\n```\n\n**Verify Akash Installation**\n\nVerify the installation by using a simple command to check the Akash version\n\n```\nprovider-services version\n```\n\n**Expect/Example Output**\n\n```\nprovider-services version\n\nv0.4.6\n```\n{% endtab %}\n\n{% tab title=\"Linux\" %}\nThe simplest way to install Akash is using Homebrew using:\n\n```\nbrew untap ovrclk/tap\nbrew tap akash-network/tap\nbrew install akash-provider-services\n```\n\nIf you do not have homebrew, follow the below steps for installing the Akash Binary.\n\n**Download Akash Binary**\n\nThese commands will retrieve the latest, stable version of the Akash software, store the version in a local variable, and install that version.\n\n```\ncd ~\n\napt install jq -y\n\napt install unzip -y\n\ncurl -sfL https://raw.githubusercontent.com/akash-network/provider/main/install.sh | bash\n```\n\n**Add Akash Install Location to User’s Path**\n\nAdd the software’s install location to the user’s path for easy use of Akash commands.\n\n**NOTE:** Below we provide the steps to add the Akash install directory to a user’s path on a Linux Ubuntu server. Please take a look at a guide for your operating system and how to add a directory to a user’s path.\n\nOpen the user’s path file in an editor:\n\n```\nvi /etc/environment\n```\n\nView within text editor prior to the update:\n\n```\nPATH=\"/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin\"\n```\n\nAdd the following directory, which is the Akash install location, to `PATH`. In this example the active user is root. If logged in as another username, replace /root with your current/home directory.\n\n```\n/root/bin\n```\n\nView within the text editor following the update:\n\n```\nPATH=\"/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/root/bin\"\n```\n\n#### Make the Path Active in the Current Session\n\n```\n. /etc/environment\n```\n\n#### Verify Akash Install\n\nDisplay the version of Akash software installed. This confirms the software installed and that the new user path addition worked.\n\n```\nprovider-services version\n```\n\n**Expected/Example Result**\n\n```\nprovider-services version\n\nv0.4.6\n```\n{% endtab %}\n\n{% tab title=\"Source\" %}\n#### From Source\n\nInstalling Akash suite from source:\n\n```\n$ go get -d github.com/akash-network/provider\n$ cd $GOPATH/src/github.com/akash-network/provider\n$ AKASH_NET=\"https://raw.githubusercontent.com/akash-network/net/main/mainnet\"\n$ AKASH_VERSION=\"$(curl -s https://api.github.com/repos/akash-network/provider/releases/latest | jq -r '.tag_name')\"\n$ git checkout \"v$AKASH_VERSION\"\n$ make deps-install\n$ make install\n```\n\nAkash is developed and tested with [golang 1.16+](https://golang.org/). Building requires a working [golang](https://golang.org/) installation, a properly set `GOPATH`, and `$GOPATH/bin` present in `$PATH`.\n\nOnce you have the dependencies properly setup, download and build `akash` using `make install`\n{% endtab %}\n{% endtabs %}\n\n\n## Create an Account\n\nConfigure the name of your key. The command below will set the name of your key to `myWallet`, run the below command and replace `myWallet` with a name of your choice:\n\n```bash\nAKASH_KEY_NAME=myWallet\n```\n\nVerify you have the shell variables set up . The below command should return the name you've used:\n\n```bash\necho $AKASH_KEY_NAME\n```\n\nWe now need to point Akash to where the keys are stored for your configuration. To do this we will set the AKASH\\_KEYRING\\_BACKEND environmental variable.\n\n```bash\nAKASH_KEYRING_BACKEND=os\n```\n\nCopy and paste this command into Terminal to create an Akash account:\n\n```bash\nprovider-services keys add $AKASH_KEY_NAME\n```\n\nRead the output and save your mnemonic phrase is a safe place. Let's set a Shell Variable in Terminal `AKASH_ACCOUNT_ADDRESS` to save your account address for later.\n\n```bash\nexport AKASH_ACCOUNT_ADDRESS=\"$(provider-services keys show $AKASH_KEY_NAME -a)\"\n\necho $AKASH_ACCOUNT_ADDRESS\n```\n\nNote that if you close your Terminal window this variable will not be saved.\n\n## Fund your Account\n\nUse the Sandbox Faucet to fund your account.  Enter the Akash account created in the prior step (I.e. `akash1xxxxxxxxx` address) as prompted within the faucet.\n\nWe will verify that the faucet properly funded the account in an upcoming, later section.\n\n> _**NOTE**_ - account and funds are only valid for sandbox network use.\n\n* [Sandbox Faucet](https://faucet.sandbox-01.aksh.pw/)\n\n### Expected Result\n\n* The following, example screen will appear if funding from the faucet was successful:\n\n<figure><img src=\"../../../.gitbook/assets/txScreenShot.png\" alt=\"\"><figcaption></figcaption></figure>\n\n\n## Configure your Network\n\nFirst configure the base URL (`$AKASH_NET`) for the Akash Network; copy and paste the command below:\n\n```bash\nAKASH_NET=\"https://raw.githubusercontent.com/akash-network/net/main/sandbox\"\n```\n\n### Version\n\nNext configure the version of the Akash Network `AKASH_VERSION`; copy and paste the command below:\n\n```bash\nAKASH_VERSION=\"$(curl -s https://api.github.com/repos/akash-network/provider/releases/latest | jq -r '.tag_name')\"\n```\n\n### Chain ID\n\nThe akash CLI will recogonize `AKASH_CHAIN_ID` environment variable when exported to the shell.\n\n```bash\nexport AKASH_CHAIN_ID=\"$(curl -s \"$AKASH_NET/chain-id.txt\")\"\n```\n\n### Network Node\n\nYou need to select a node on the network to connect to, using an RPC endpoint. To configure the`AKASH_NODE` environment variable use this export command:\n\n```bash\nexport AKASH_NODE=\"$(curl -s \"$AKASH_NET/rpc-nodes.txt\" | shuf -n 1)\"\n```\n\n## Confirm your network variables are setup\n\nYour values may differ depending on the network you're connecting to.\n\n```bash\necho $AKASH_NODE $AKASH_CHAIN_ID $AKASH_KEYRING_BACKEND\n```\n\nYou should see something similar to:\n\n```\nhttps://rpc.sandbox-01.aksh.pw:443 sandbox-01 os\n```\n\nSet Additional Environment Variables\n\nSet the below set of environment variables to ensure smooth operations\n\n| Variable               | Description                                                                               | Recommended Value |\n| ---------------------- | ----------------------------------------------------------------------------------------- | ----------------- |\n| AKASH\\_GAS             | Gas limit to set per-transaction; set to \"auto\" to calculate sufficient gas automatically | `auto`            |\n| AKASH\\_GAS\\_ADJUSTMENT | Adjustment factor to be multiplied against the estimate returned by the tx simulation     | `1.15`            |\n| AKASH\\_GAS\\_PRICES     | Gas prices in decimal format to determine the transaction fee                             | `0.025uakt`       |\n| AKASH\\_SIGN\\_MODE      | Signature mode                                                                            | `amino-json`      |\n\n```\nexport AKASH_GAS=auto\nexport AKASH_GAS_ADJUSTMENT=1.25\nexport AKASH_GAS_PRICES=0.025uakt\nexport AKASH_SIGN_MODE=amino-json\n```\n\n## Check your Account Balance\n\nCheck your account has sufficient balance by running:\n\n```bash\nprovider-services query bank balances --node $AKASH_NODE $AKASH_ACCOUNT_ADDRESS\n```\n\nYou should see a response similar to:\n\n```\nbalances:\n- amount: \"25000000\"\n  denom: uakt\npagination:\n  next_key: null\n  total: \"0\"\n```\n\nPlease note the balance indicated is denominated in uAKT (AKT x 10^-6), in the above example, the account has a balance of 25 _AKT_. We're now setup to deploy.\n\n{% hint style=\"info\" %}\nYour account must have a minimum balance of 5 AKT to create a deployment. This 5 AKT funds the escrow account associated with the deployment and is used to pay the provider for their services. It is recommended you have more than this minimum balance to pay for transaction fees. For more information on escrow accounts, see [here](../../cli/detailed-steps/broken-reference/)\n{% endhint %}\n\n\n## Create your Configuration\n\nCreate a deployment configuration [deploy.yaml](https://github.com/akash-network/docs/tree/b65f668b212ad1976fb976ad84a9104a9af29770/guides/deploy/deploy.yml) to deploy the `ovrclk/lunie-light` for [Lunie Light](https://github.com/ovrclk/lunie-light) Node app container using [SDL](../../../readme/stack-definition-language/).\n\n### Modify your Configuration\n\nYou may use the sample deployment file as-is or modify it for your own needs as described in our [SDL (Stack Definition Language)](../../../readme/stack-definition-language/) documentation. A typical modification would be to reference your own image instead of our demo app image.\n\n#### EXAMPLE CONFIGURATION:\n\n```bash\ncat > deploy.yml <<EOF\n---\nversion: \"2.0\"\n\nservices:\n  web:\n    image: ovrclk/lunie-light\n    expose:\n      - port: 3000\n        as: 80\n        to:\n          - global: true\n\nprofiles:\n  compute:\n    web:\n      resources:\n        cpu:\n          units: 0.1\n        memory:\n          size: 512Mi\n        storage:\n          size: 512Mi\n  placement:\n    westcoast:\n      pricing:\n        web: \n          denom: uakt\n          amount: 1000000\n\ndeployment:\n  web:\n    westcoast:\n      profile: web\n      count: 1\n\nEOF\n```\n\n## Create your Certificate\n\nIn this step we will create a local certificate and then store the certification on the block chain\n\n* NOTE - for those familiar with previous Akash CLI versions the following commands for cert creation have changed. We believe the new command sets in this version make steps more clear.\n* Ensure that prior steps in this guide have been completed and that you have a funded wallet before attempting certificate creation.\n* **Your certificate needs to be created only once per account** and can be used across all deployments.\n\n#### Generate Cert\n\n* Note: If it errors with `Error: certificate error: cannot overwrite certificate`, then add `--overwrite` should you want to overwrite the cert. Normally you can ignore that error and proceed with publishing the cert (next step).\n\n```\nprovider-services tx cert generate client --from $AKASH_KEY_NAME\n```\n\n#### Publish Cert to the Blockchain\n\n```\nprovider-services tx cert publish client --from $AKASH_KEY_NAME\n```\n\n## Create your Deployment\n\n### CPU Support\n\nOnly x86\\_64 processors are officially supported for Akash deployments. This may change in the future and when ARM processors are supported it will be announced and documented.\n\n### Akash Deployment\n\nTo deploy on Akash, run:\n\n```\nprovider-services tx deployment create deploy.yml --from $AKASH_KEY_NAME \n```\n\nYou should see a response similar to:\n\n```javascript\n{\n  \"height\":\"140325\",\n  \"txhash\":\"2AF4A01B9C3DE12CC4094A95E9D0474875DFE24FD088BB443238AC06E36D98EA\",\n  \"codespace\":\"\",\n  \"code\":0,\n  \"data\":\"0A130A116372656174652D6465706C6F796D656E74\",\n  \"raw_log\":\"[{\\\"events\\\":[{\\\"type\\\":\\\"akash.v1\\\",\\\"attributes\\\":[{\\\"key\\\":\\\"module\\\",\\\"value\\\":\\\"deployment\\\"},{\\\"key\\\":\\\"action\\\",\\\"value\\\":\\\"deployment-created\\\"},{\\\"key\\\":\\\"version\\\",\\\"value\\\":\\\"2b86f778de8cc9df415490efa162c58e7a0c297fbac9cdb8d6c6600eda56f17e\\\"},{\\\"key\\\":\\\"owner\\\",\\\"value\\\":\\\"akash1vn06ycjjnvsvl639fet9lajjctuturrtx7fvuj\\\"},{\\\"key\\\":\\\"dseq\\\",\\\"value\\\":\\\"140324\\\"},{\\\"key\\\":\\\"module\\\",\\\"value\\\":\\\"market\\\"},{\\\"key\\\":\\\"action\\\",\\\"value\\\":\\\"order-created\\\"},{\\\"key\\\":\\\"owner\\\",\\\"value\\\":\\\"akash1vn06ycjjnvsvl639fet9lajjctuturrtx7fvuj\\\"},{\\\"key\\\":\\\"dseq\\\",\\\"value\\\":\\\"140324\\\"},{\\\"key\\\":\\\"gseq\\\",\\\"value\\\":\\\"1\\\"},{\\\"key\\\":\\\"oseq\\\",\\\"value\\\":\\\"1\\\"}]},{\\\"type\\\":\\\"message\\\",\\\"attributes\\\":[{\\\"key\\\":\\\"action\\\",\\\"value\\\":\\\"create-deployment\\\"},{\\\"key\\\":\\\"sender\\\",\\\"value\\\":\\\"akash1vn06ycjjnvsvl639fet9lajjctuturrtx7fvuj\\\"},{\\\"key\\\":\\\"sender\\\",\\\"value\\\":\\\"akash1vn06ycjjnvsvl639fet9lajjctuturrtx7fvuj\\\"}]},{\\\"type\\\":\\\"transfer\\\",\\\"attributes\\\":[{\\\"key\\\":\\\"recipient\\\",\\\"value\\\":\\\"akash17xpfvakm2amg962yls6f84z3kell8c5lazw8j8\\\"},{\\\"key\\\":\\\"sender\\\",\\\"value\\\":\\\"akash1vn06ycjjnvsvl639fet9lajjctuturrtx7fvuj\\\"},{\\\"key\\\":\\\"amount\\\",\\\"value\\\":\\\"5000uakt\\\"},{\\\"key\\\":\\\"recipient\\\",\\\"value\\\":\\\"akash14pphss726thpwws3yc458hggufynm9x77l4l2u\\\"},{\\\"key\\\":\\\"sender\\\",\\\"value\\\":\\\"akash1vn06ycjjnvsvl639fet9lajjctuturrtx7fvuj\\\"},{\\\"key\\\":\\\"amount\\\",\\\"value\\\":\\\"5000000uakt\\\"}]}]}]\",\n  \"logs\":[\n    {\n      \"msg_index\":0,\n      \"log\":\"\",\n      \"events\":[\n        {\n          \"type\":\"akash.v1\",\n          \"attributes\":[\n            {\n              \"key\":\"module\",\n              \"value\":\"deployment\"\n            },\n            {\n              \"key\":\"action\",\n              \"value\":\"deployment-created\"\n            },\n            {\n              \"key\":\"version\",\n              \"value\":\"2b86f778de8cc9df415490efa162c58e7a0c297fbac9cdb8d6c6600eda56f17e\"\n            },\n            {\n              \"key\":\"owner\",\n              \"value\":\"akash1vn06ycjjnvsvl639fet9lajjctuturrtx7fvuj\"\n            },\n            {\n              \"key\":\"dseq\",\n              \"value\":\"140324\"\n            },\n            {\n              \"key\":\"module\",\n              \"value\":\"market\"\n            },\n            {\n              \"key\":\"action\",\n              \"value\":\"order-created\"\n            },\n            {\n              \"key\":\"owner\",\n              \"value\":\"akash1vn06ycjjnvsvl639fet9lajjctuturrtx7fvuj\"\n            },\n            {\n              \"key\":\"dseq\",\n              \"value\":\"140324\"\n            },\n            {\n              \"key\":\"gseq\",\n              \"value\":\"1\"\n            },\n            {\n              \"key\":\"oseq\",\n              \"value\":\"1\"\n            }\n          ]\n        },\n        {\n          \"type\":\"message\",\n          \"attributes\":[\n            {\n              \"key\":\"action\",\n              \"value\":\"create-deployment\"\n            },\n            {\n              \"key\":\"sender\",\n              \"value\":\"akash1vn06ycjjnvsvl639fet9lajjctuturrtx7fvuj\"\n            },\n            {\n              \"key\":\"sender\",\n              \"value\":\"akash1vn06ycjjnvsvl639fet9lajjctuturrtx7fvuj\"\n            }\n          ]\n        },\n        {\n          \"type\":\"transfer\",\n          \"attributes\":[\n            {\n              \"key\":\"recipient\",\n              \"value\":\"akash17xpfvakm2amg962yls6f84z3kell8c5lazw8j8\"\n            },\n            {\n              \"key\":\"sender\",\n              \"value\":\"akash1vn06ycjjnvsvl639fet9lajjctuturrtx7fvuj\"\n            },\n            {\n              \"key\":\"amount\",\n              \"value\":\"5000uakt\"\n            },\n            {\n              \"key\":\"recipient\",\n              \"value\":\"akash14pphss726thpwws3yc458hggufynm9x77l4l2u\"\n            },\n            {\n              \"key\":\"sender\",\n              \"value\":\"akash1vn06ycjjnvsvl639fet9lajjctuturrtx7fvuj\"\n            },\n            {\n              \"key\":\"amount\",\n              \"value\":\"5000000uakt\"\n            }\n          ]\n        }\n      ]\n    }\n  ],\n  \"info\":\"\",\n  \"gas_wanted\":\"100000\",\n  \"gas_used\":\"94653\",\n  \"tx\":null,\n  \"timestamp\":\"\"\n}\n```\n\n### Find your Deployment \\#\n\nFind the Deployment Sequence (DSEQ) in the deployment you just created. You will need to replace the AKASH\\_DSEQ with the number from your deployment to configure a shell variable.\n\n```bash\nexport AKASH_DSEQ=<CHANGETHIS>\n```\n\nNow set the Order Sequence (OSEQ) and Group Sequence (GSEQ). Note that if this is your first time deploying on Akash, OSEQ and GSEQ will be 1.\n\n```bash\nAKASH_OSEQ=1\nAKASH_GSEQ=1\n```\n\nVerify we have the right values populated by running:\n\n```bash\necho $AKASH_DSEQ $AKASH_OSEQ $AKASH_GSEQ\n```\n\n\n## View your Bids\n\nAfter a short time, you should see bids from providers for this deployment with the following command:\n\n```bash\nprovider-services query market bid list --owner=$AKASH_ACCOUNT_ADDRESS --node $AKASH_NODE --dseq $AKASH_DSEQ --state=open\n```\n\n### Choose a Provider\n\nNote that there are bids from multiple different providers. In this case, both providers happen to be willing to accept a price of _1 uAKT_. This means that the lease can be created using _1 uAKT_ or _0.000001 AKT_ per block to execute the container. You should see a response similar to:\n\n```\nbids:\n- bid:\n    bid_id:\n      dseq: \"140324\"\n      gseq: 1\n      oseq: 1\n      owner: akash1vn06ycjjnvsvl639fet9lajjctuturrtx7fvuj\n      provider: akash10cl5rm0cqnpj45knzakpa4cnvn5amzwp4lhcal\n    created_at: \"140326\"\n    price:\n      amount: \"1\"\n      denom: uakt\n    state: open\n  escrow_account:\n    balance:\n      amount: \"50000000\"\n      denom: uakt\n    id:\n      scope: bid\n      xid: akash1vn06ycjjnvsvl639fet9lajjctuturrtx7fvuj/140324/1/1/akash10cl5rm0cqnpj45knzakpa4cnvn5amzwp4lhcal\n    owner: akash10cl5rm0cqnpj45knzakpa4cnvn5amzwp4lhcal\n    settled_at: \"140326\"\n    state: open\n    transferred:\n      amount: \"0\"\n      denom: uakt\n- bid:\n    bid_id:\n      dseq: \"140324\"\n      gseq: 1\n      oseq: 1\n      owner: akash1vn06ycjjnvsvl639fet9lajjctuturrtx7fvuj\n      provider: akash1f6gmtjpx4r8qda9nxjwq26fp5mcjyqmaq5m6j7\n    created_at: \"140326\"\n    price:\n      amount: \"1\"\n      denom: uakt\n    state: open\n  escrow_account:\n    balance:\n      amount: \"50000000\"\n      denom: uakt\n    id:\n      scope: bid\n      xid: akash1vn06ycjjnvsvl639fet9lajjctuturrtx7fvuj/140324/1/1/akash1f6gmtjpx4r8qda9nxjwq26fp5mcjyqmaq5m6j7\n    owner: akash1f6gmtjpx4r8qda9nxjwq26fp5mcjyqmaq5m6j7\n    settled_at: \"140326\"\n    state: open\n    transferred:\n      amount: \"0\"\n      denom: uakt\n```\n\nFor this example, we will choose `akash10cl5rm0cqnpj45knzakpa4cnvn5amzwp4lhcal` Run this command to set the provider shell variable:\n\n```\nAKASH_PROVIDER=akash10cl5rm0cqnpj45knzakpa4cnvn5amzwp4lhcal\n```\n\nVerify we have the right value populated by running:\n\n```\necho $AKASH_PROVIDER\n```\n\n## Create a Lease\n\nCreate a lease for the bid from the chosen provider above by running this command:\n\n```\nprovider-services tx market lease create --dseq $AKASH_DSEQ --provider $AKASH_PROVIDER --from $AKASH_KEY_NAME\n```\n\n### Confirm the Lease\n\nYou can check the status of your lease by running:\n\n```\nprovider-services query market lease list --owner $AKASH_ACCOUNT_ADDRESS --node $AKASH_NODE --dseq $AKASH_DSEQ\n```\n\nNote the bids will close automatically after 5 minutes, and you may get the response:\n\n```\nbid not open\n```\n\nIf this happens, close your deployment and open a new deployment again. To close your deployment run this command:\n\n```\nprovider-services tx deployment close --dseq $AKASH_DSEQ  --owner $AKASH_ACCOUNT_ADDRESS --from $AKASH_KEY_NAME \n```\n\nIf your lease was successful you should see a response that ends with:\n\n```\n    state: active\n```\n\n{% hint style=\"info\" %}\nPlease note that once the lease is created, the provider will begin debiting your deployment's escrow account, even if you have not completed the deployment process by uploading the manifest in the following step.\n{% endhint %}\n\n\n## Send the Manifest\n\nUpload the manifest using the values from above step:\n\n```\nprovider-services send-manifest deploy.yml --dseq $AKASH_DSEQ --provider $AKASH_PROVIDER --from $AKASH_KEY_NAME\n```\n\n### Confirm the URL\n\nNow that the manifest is uploaded, your image is deployed. You can retrieve the access details by running the below:\n\n```\nprovider-services lease-status --dseq $AKASH_DSEQ --from $AKASH_KEY_NAME --provider $AKASH_PROVIDER\n```\n\nYou should see a response similar to:\n\n```javascript\n{\n  \"services\": {\n    \"web\": {\n      \"name\": \"web\",\n      \"available\": 1,\n      \"total\": 1,\n      \"uris\": [\n        \"rga3h05jetf9h3p6dbk62m19ck.ingress.ewr1p0.mainnet.akashian.io\"\n      ],\n      \"observed_generation\": 1,\n      \"replicas\": 1,\n      \"updated_replicas\": 1,\n      \"ready_replicas\": 1,\n      \"available_replicas\": 1\n    }\n  },\n  \"forwarded_ports\": {}\n}\n```\n\nYou can access the application by visiting the hostnames mapped to your deployment. Look for a URL/URI and copy it to your web browser.\n\n### View your logs\n\nYou can view your application logs to debug issues or watch progress like so:\n\n```bash\nprovider-services lease-logs \\\n  --dseq \"$AKASH_DSEQ\" \\\n  --provider \"$AKASH_PROVIDER\" \\\n  --from \"$AKASH_KEY_NAME\"\n```\n\n## Update the Deployment\n\n### Update the Manifest\n\nUpdate the deploy.yml manifest file with the desired change.\n\n_**NOTE:**_\\*\\* Not all attributes of the manifest file are eligible for deployment update. If the hardware specs of the manifest are updated (I.e. CPU count), a re-deployment of the workload is necessary. Other attributes, such as deployment image and funding, are eligible for updates.\n\n### Issue Transaction for On Chain Update\n\n```\nprovider-services tx deployment update deploy.yml --dseq $AKASH_DSEQ --from $AKASH_KEY_NAME \n```\n\n### Send Updated Manifest to Provider\n\n```\nprovider-services send-manifest deploy.yml --dseq $AKASH_DSEQ --provider $AKASH_PROVIDER --from $AKASH_KEY_NAME\n```\n\n\n## Close Deployment\n\n### Close the Deployment\n\nShould you need to close the deployment follow this step.\n\n```\nprovider-services tx deployment close --from $AKASH_KEY_NAME\n```\n","description":null,"slug":"docs/deployments/sandbox/installation"},{"title":"Introduction","body":"The Akash Sandbox network is intended for short-term workload testing and for new user experimentation.\n\n> _**NOTE**_ - sandbox workloads that have been running for more than 24 hours may be taken down.  Please only use the sandbox for short duration testing.\n\nThe Akash Sandbox version matches the current Mainnet version.\n\nUse the following documentation for sandbox use and depending on your preferred deployment tool.\n\n> _**NOTE**_ - this documentation covers the use of the Akash CLI for deployment creation and management.  The [Cloudmos Deploy](/docs/docs/deployments/cloudmos-deploy/) also now support Sandbox network use.  Simply update the network in Settings from `Mainnet` to `Sandbox` to deploy using these web apps.\n\n* [Akash CLI for Sandbox Use](/docs/docs/deployments/akash-cli/installation/)","description":null,"slug":"docs/deployments/sandbox/introduction"},{"title":"Stable Payment Deployments","body":"\n\n\nAkash deployments can be funded using our native crypto currency (AKT) or through stable coins.\n\nIn this guide we will review:\n\n* [Methods to Transfer USDC onto Akash for Stable Payments](#methods-to-transfer-usdc-onto-akash-for-stable-payments)\n* [Akash Deployment Creation using Stable Payments](#akash-deployment-creation-using-stable-payments)\n\n### Currently Supported Stable Coins\n\n* Axelar USDC (axlUSDC)\n\n## Transfer USDC onto Akash using Leap Wallet\n\n### Section Overview\n\nIn this section we will detail methods to transfer Axelar USDC into your Akash account for funding of deployments using stable payments.\n\n### Leap Wallet\n\nWe find that using Leap Wallet to be the simplest method of transferring Axelar USDC into the Akash network and associated accounts as no manual specification of the IBC is necessary.\n\nIn the steps that follow we will swap Akash AKT for Axelar USDC.  Other methods are possible but if you would like to follow these instructions explictly, ensure that you have some available AKT.\n\n#### Install Leap Wallet Browser Plug In\n\n* Follow the instructions provided by Leap Wallet to install the appropriate web browser plug in [here](https://www.leapwallet.io/download).\n\n### IBC Transfer of USDC onto Akash\n\n> _**NOTE**_ - in the current Leap Wallet version swaps of AKT to axlUSDC are only possible in the web app.  Via the instructions below you will be automatically redirected to the Leap Wallet app for the purpose of AKT to axlUSDC swap.  In future versions of Leap Wallet it may become possible to conduct the swap directly within the Leap Wallet browser plug in.\n\n#### STEP 1 - Open Leap Wallet Web App\n\n* Open your Leap Wallet browser extension\n* Select Akash from the network selection drop down as shown\n\n\n\n![](../../../assets/leapWalletWithAkashSelected.png)\n\n#### STEP 2 - Open Leap Wallet Web App to Conduct Swap\n\n* Select the Swap option within Leap Wallet as shown below\n\n\n![](../../../assets/leapWalletInitiateWebApp.png)\n\n* The selection of the Swap option in the prior step will provoke the opening of the Leap Wallet web app\n* Within the app make the selections as indicated in the depiction below.  The source and destination network should be Osmosis.  The source denomination should be `AKT` and the destination denomination should be `axlUSDC` also as shown.\n\n\n![](../../../assets/ibcSwap.png)\n\n#### STEP 3 - Complete the IBC Transfer of axlUSDC onto Akash\n\n* With the Swap now completed in the previous step, return to the Leap Wallet browser plug in.\n* Select the `IBC` option as depicted below.\n\n\n![](../../../assets/initiateIBC.png)\n\n* Complete an IBC transfer of `axlUSDC` onto the Akash Network.\n* Ensure that the Osmosis network is selected during this step as depicted below.\n\n\n\n![](../../../assets/sendOntoAkash.png)\n\n### Verify IBC Transfer and Availability of USDC on Akash Network\n\n* The simplest means of verifying the IBC transfer of USDC was successful and is available in your account on the Akash network is via the Akash CLI.\n* USDC balances can also be verified in Cloudmos Deploy of preferred.\n\n_**CLI Verification/Command Template**_\n\n```\nprovider-services query bank balances <akash-address>\n```\n\n_**Expected/Example Output**_\n\n* Note the appearance and balance of denom `ibc/170C677610AC31DF0904FFE09CD3B5C657492170E7E52372E48756B71E56F2F1`\n* This demon represents available Axelar USDC and indicates availability of stable payment funds to utilize for Akash deployments\n\n```\nprovider-services query bank balances akash1w3k6qpr4uz44py4z68chfrl7ltpxwtkngnc6xk\n\nbalances:\n- amount: \"3068485\"\n  denom: ibc/170C677610AC31DF0904FFE09CD3B5C657492170E7E52372E48756B71E56F2F1\n- amount: \"8650845\"\n  denom: uakt\npagination:\n  next_key: null\n  total: \"0\"\n```\n\n## Akash Deployment Creation using Stable Payments\n\nIn this section we will cover using the following clients to create an Akash deployment using Stable Payments.\n\n> NOTE - when creating an Akash deployment using Stable Payment - ensure that the SDL pricing section reflects the correct denomination as covered [here](/docs/docs/getting-started/stack-definition-language#stable-payment).\n\n* [Cloudmos Deploy](#cloudmos-deploy-stable-payment-use)\n* [Akash CLI](#akash-cli-stable-payment-use)\n\n### Cloudmos Deploy Stable Payment Use\n\n* When using Cloudmos Deploy no change is necessary in the deployment creation when using Stable Payments.\n* When the associated SDL uses the USDC denomination, Cloudmos Deploy will create the deployment with Stable Payment.\n* An indication that Stable Payments are being used in the deployment creation is provided in the `Deployment Deposit` dialog box as shown below.\n\n\n![](../../../assets/cloudmosStable.png)\n\n### Akash CLI Stable Payment Use\n\n* When creating a deployment via the Akash CLI, all steps in the typical deployment creation steps (as covered [here](/docs/docs/deployments/akash-cli/installation/)) remain applicable and valid with the only edit in such steps being this syntax in the `deployment create` step.\n* As shown in this Stable Payment relevant `deployment create` command we must specify the `deposit` switch as the USDC denomination.\n\n#### Create Deployment Command for USDC Stable Payment Use\n\n```\nprovider-services tx deployment create deploy.yml --from $AKASH_KEY_NAME --deposit=5000000ibc/170C677610AC31DF0904FFE09CD3B5C657492170E7E52372E48756B71E56F2F1\n```","description":null,"slug":"docs/deployments/stable-payment-deployments"},{"title":"Akash Network","body":"\n## Decentralized Compute Marketplace\n\nAkash is an open network that facilitates the secure and efficient buying and selling of computing resources. Purpose-built for public utility, it is fully open-source with an active community of contributors.\n\n## Common Questions\n\n### How to use Akash?\n\nYou can get started right from the [**command-line**](/docs/docs/deployments/akash-cli/cli-booster/) :\n\n- Define your Docker image, CPU, Memory, and Storage in a [**deploy.yaml**](/docs/docs/getting-started/stack-definition-language/) file.\n- Set your price, receive bids from providers in seconds, and select the lowest price.\n- Deploy your application without having to set up, configure, or manage servers.\n- Scale your application from a single container to hundreds of deployments.\n\n### What is the Akash Compute Marketplace?\n\nThe [**Akash Compute Marketplace**](/docs/docs/other-resources/marketplace/) is where users lease computing resources from Cloud providers before deploying a Docker container on the Akash Container Platform. The marketplace stores on-chain records of requests, bids, leases, and settlement payments using the Akash Token (AKT). Akash's blockchain is a [**Tendermint**](https://github.com/tendermint/tendermint) based application built on the [**Cosmos SDK**](https://github.com/cosmos/cosmos-sdk).\n\n### What is the Akash Container Platform?\n\nThe [**Akash Container Platform**](/docs/docs/other-resources/containers/) is a deployment platform for hosting and managing [**containers**](/docs/docs/other-resources/containers/), allowing users to run _**any**_ Cloud-Native application. Akash is built with a set of cloud management services, including [**Kubernetes**](https://kubernetes.io), to orchestrate and manage containers.\n\n### What is the cost to use Akash?\n\nThe cost of hosting your application using Akash is about one-third the cost of Amazon AWS, Google Cloud Platform (GCP), and Microsoft Azure. You can check the prices live using the [**Cloudmos.io price comparison tool**](https://cloudmos.io/price-compare).\n\n### How do I use Akash?\n\nIf you're new to Akash, start with our [**deployment guides**](/docs/docs/deployments/cloudmos-deploy/) and go from there. Akash's community has written several more advanced guides for learning about Akash: a [**node operator guide**](/docs/docs/akash-nodes/akash-node-via-helm-chart/), a [**validator guide**](validating/validator.md), a [**cloud provider guide**](broken-reference), and several [**deployment guides**](/docs/docs/deployments/cloudmos-deploy/) for running various apps on Akash.\n\n### Why is Akash different than other Cloud platforms?\n\nThe decentralized cloud is a shift from computing resources being owned and operated by the three large Cloud companies (Amazon, Google, and Microsoft) to a decentralized network of Cloud providers running **open source** **software** developed by a community and creating competition in an **open** **marketplace** with more providers.\n\nLike Airbnb for server hosting, Akash is a marketplace that gives you control over the price you pay and the amenities included (we call them attributes). Akash gives app developers a command-line tool for leasing and deploying apps right from a terminal. Akash taps into the massive market of underutilized resources sitting idle in the estimated 8.4 million data centers globally. Any cloud-native and containerized applications running on the centralized cloud can run faster and at a lower cost on the Akash decentralized cloud.\n\n### Why is Akash different than other decentralized platforms?\n\nAkash hosts [**containers**](/docs/docs/other-resources/containers/) where users can run _**any**_ Cloud-Native application. There is no need to re-write the entire internet in a new proprietary language, and there is no vendor lock-in to prevent you from switching Cloud providers. The deployment file is transferred over a private peer-to-peer network isolated from the blockchain. Asset transfer occurs off-chain over mTLS to provide the security and performance required by mission-critical applications running on the Cloud.\n\n### What is the Stack Definition Language (SDL)?\n\nYou can define the deployment services, data centers, requirements, and pricing parameters in a \"manifest\" file (deploy.yaml). The file is written in a declarative language called [**Stack Definition Language (SDL)**](/docs/docs/getting-started/stack-definition-language/). SDL is a human-friendly data standard for declaring deployment attributes. The SDL file is a \"form\" to request resources from the Network. SDL is compatible with the YAML standard and similar to Docker Compose files.\n\n### How do I configure Networking for my container?\n\nNetworking - allowing connectivity to and between workloads - can be configured via the Stack Definition Language ([**SDL**](/docs/docs/getting-started/stack-definition-language/)) file for a deployment. By default, workloads in a deployment group are isolated - nothing else is allowed to connect to them. This restriction can be relaxed.\n\n**Do I need to close and re-create my deployment if I want to update the deployment?**\n\nNo. You can update your deployment. However, only some fields in the Akash stack definition file are mutable. The **image**, **command**, **args**, **env**, and **exposed ports** can be modified, but compute resources and placement criteria cannot.\n\n\n","description":null,"slug":"docs/getting-started/intro-to-akash/akash-network"},{"title":"Bids and Leases","body":"\n## How does the Marketplace work?\n\nThe Akash Marketplace revolves around [Deployments](#deployment), which fully describe the resources that a tenant is requesting from the network. Deployments contain [Groups](#group), which is a grouping of resources that are meant to be leased together from a single provider.\n\nDeploying applications onto [Akash](https://github.com/ovrclk/akash) involves two types of users:\n\n1. The **Tenant**: the entity that deploys the application.\n2. The **Provider**: the entity that hosts the application.\n\n### What is a Reverse Auction?\n\nAkash uses a reverse auction. Tenants set the price and terms of their deployment, and the Cloud providers bid on the deployments.\n\nIn a very simple reverse auction:\n\n1. A tenant creates orders.\n2. Providers bid on orders.\n3. Tenants choose winning bids and create leases.\n\nA typical application deployment on Akash will follow this flow:\n\n1. The tenant describes their desired deployment in [SDL], called a [deployment](#deployment).\n2. The tenant submits that definition to the blockchain.\n3. Their submission generates an [order](#order) on the marketplace.\n4. Providers that would like to fulfill that order [bid](#bid) on it.\n5. After some period of time, a winning [bid](#bid) for the [order](#order) is chosen, and a [lease](#lease) is created.\n6. Once a [lease](#lease) has been created, the tenant submits a [manifest](/docs/docs/getting-started/stack-definition-language/) to the provider.\n7. The provider executes workloads as instructed by the [manifest](/docs/docs/getting-started/stack-definition-language/).\n8. The workload is running - if it is a web application it can be visited\n9. The provider or tenant eventually closes the [lease](#lease), shutting down the workload.\n\nThe general workflow is:\n\n1. A tenant creates orders.\n2. Providers bid on orders.\n3. Tenants choose winning bids and create leases.\n\n### Lifecycle of a Deployment\n\nThe lifecycle of a typical application deployment is as follows:\n\n1. The tenant describes their desired deployment in [SDL], called a [deployment](#deployment).\n2. The tenant submits that definition to the blockchain.\n3. Their submission generates an [order](#order) on the marketplace.\n4. Providers that would like to fulfill that order [bid](#bid) on it.\n5. After some period of time, a winning [bid](#bid) for the [order](#order) is chosen, and a [lease](#lease) is created.\n6. Once a [lease](#lease) has been created, the tenant submits a [manifest](/docs/docs/getting-started/stack-definition-language/) to the provider.\n7. The provider executes workloads as instructed by the [manifest](/docs/docs/getting-started/stack-definition-language/).\n8. The workload is running - if it is a web application it can be visited\n9. The provider or tenant eventually closes the [lease](#lease), shutting down the workload.\n\n## Payments\n\nLeases are paid from deployment owner (tenant) to the provider through a deposit & withdraw mechanism.\n\nTenants are required to submit a deposit when creating a deployment. Leases will be paid passively from the balance of this deposit. At any time, a lease provider may withdraw the balance owed to them from this deposit.\n\nIf the available funds in the deposit ever reaches zero, a provider may close the lease. A tenant can add funds to their deposit at any time. When a deployment is closed, the unspent portion of the balance will be returned to the tenant.\n\n### Escrow Accounts\n\n[Escrow accounts](escrow.md) are a mechanism that allows for time-based payments from one account to another without block-by-block micropayments. They also support holding funds for an account until an arbitrary event occurs.\n\nEscrow accounts are necessary in Akash for two primary reasons:\n\n1. Leases in Akash are priced in blocks - every new block, a payment from the tenant (deployment owner) to the provider (lease holder) is due. Performance and security considerations prohibit the naive approach of transferring tokens on every block.\n2. Bidding on an order should not be free (for various reasons, including performance and security). Akash requires a deposit for every bid. The deposit is returned to the bidder when the bid is closed.\n\n## Bid Deposits\n\nBidding on an order requires a deposit to be made. The deposit will be returned to the provider account when the [bid](#bid) transitions to state `CLOSED`.\n\nBid deposits are implemented with an escrow account module. See [here](escrow.md) for more information.\n\n## Audited Attributes\n\nAudited attributes allow users deploying applications to be more selective about which providers can run their apps. Anyone on the Akash Network can assign these attributes to Providers via an on-chain transaction.\n\n## On-Chain Parameters\n\n| Name                     | Initial Value | Description                                        |\n| ------------------------ | ------------- | -------------------------------------------------- |\n| deployment_min_deposit   | 5akt          | Minimum deposit to make deployments. Target: ~$10  |\n| bid_min_deposit          | 50akt         | Deposit amount required to bid. Target: ~$100     |\n\n## Transactions\n\n### DeploymentCreate\n\nCreates a [deployment](#deployment), and open [groups](#group) and [orders](#order) for it.\n\n#### Parameters\n\n| Name             | Description                                                    |\n| ---------------- | -------------------------------------------------------------- |\n| DeploymentID     | ID of Deployment.                                              |\n| DepositAmount    | Deposit amount. Must be greater than deployment_min_deposit. |\n| Version          | Hash of the manifest that is sent to the providers.            |\n| Groups           | A list of [group](#group) descriptions.         |\n\n### DeploymentDeposit\n\nAdd funds to a deployment's balance.\n\n#### Parameters\n\n| Name             | Description                                                   |\n| ---------------- | ------------------------------------------------------------- |\n| DeploymentID     | ID of Deployment.                                             |\n| DepositAmount    | Deposit amount. Must be greater than `deployment_min_deposit` |\n\n### GroupClose\n\nCloses a [group](#group) and any [orders](#order) for it. Sent by the tenant.\n\n#### Parameters\n\n| Name | Description  |\n| ---- | ------------ |\n| ID   | ID of Group. |\n\n### GroupPause\n\nPuts a `PAUSED` state, and closes any and [orders](#order) for it. Sent by the tenant.\n\n#### Parameters\n\n| Name | Description  |\n| ---- | ------------ |\n| ID   | ID of Group. |\n\n### GroupStart\n\nTransitions a [group](#group) from state `PAUSED` to state `OPEN`. Sent by the tenant.\n\n#### Parameters\n\n| Name | Description  |\n| ---- | ------------ |\n| ID   | ID of Group. |\n\n### BidCreate\n\nSent by a provider to bid on an open [order](#order). The required deposit will be returned when the bid transitions to state `CLOSED`.\n\n#### Parameters\n\n| name      | description                                 |\n| --------- | ------------------------------------------- |\n| OrderID   | ID of Order                                 |\n| TTL       | Number of blocks this bid is valid for      |\n| Deposit   | Deposit amount. `bid_min_deposit` if empty. |\n\n### BidClose\n\nSent by provider to close a bid or a lease from an existing bid.\n\nWhen closing a lease, the bid's group will be put in state `PAUSED`.\n\n#### Parameters\n\n| name    | description |\n| ------- | ----------- |\n| BidID   | ID of Bid   |\n\n#### State Transitions\n\n| Object | Previous State | New State |\n| ------ | -------------- | --------- |\n| Bid    | `ACTIVE`       | `CLOSED`  |\n| Lease  | `ACTIVE`       | `CLOSED`  |\n| Order  | `ACTIVE`       | `CLOSED`  |\n| Group  | `OPEN`         | `PAUSED`  |\n\n### LeaseCreate\n\nSent by tenant to create a lease.\n\n1. Creates a `Lease` from the given [bid](#bid).\n2. Sets all non-winning [bids](#bid) to state `CLOSED` (deposit returned).\n\n#### Parameters\n\n| name    | description                                      |\n| ------- | ------------------------------------------------ |\n| BidID   | [Bid](#bid) to create a lease from |\n\n### MarketWithdraw\n\nThis withdraws balances earned by providing for leases and deposits of bids that have expired.\n\n#### Parameters\n\n| name    | description                        |\n| ------- | ---------------------------------- |\n| Owner   | Provider ID to withdraw funds for. |\n\n## Models\n\n### Deployment\n\n| Name       | Description                                                                         |\n| ---------- | ----------------------------------------------------------------------------------- |\n| ID.Owner   | account address of tenant                                                            |\n| ID.DSeq    | Arbitrary sequence number that identifies the deployment. Defaults to block height. |\n| State      | State of the deployment.                                                            |\n| Version    | Hash of the manifest that is sent to the providers.                                 |\n\n#### State\n\n| Name     | Description                      |\n| -------- | -------------------------------- |\n| OPEN     | Orders may be created.           |\n| CLOSED   | All groups are closed. Terminal. |\n\n### Group\n\n| Name              | Description                                                         |\n| ----------------- | ------------------------------------------------------------------- |\n| ID.DeploymentID   | [Deployment](#deployment) ID of group.                |\n| ID.GSeq           | Arbitrary sequence number. Internally incremented, starting at `1`. |\n| State             | State of the group.                                                 |\n\n#### State\n\n| Name     | Description                               |\n| -------- | ----------------------------------------- |\n| OPEN     | Has an open or active order.              |\n| PAUSED   | Bid closed by provider. May be restarted. |\n| CLOSED   | No open or active orders. Terminal.       |\n\n### Order\n\n| Name         | Description                                                         |\n| ------------ | ------------------------------------------------------------------- |\n| ID.GroupID   | [Group](#group) ID of group.                          |\n| ID.OSeq      | Arbitrary sequence number. Internally incremented, starting at `1`. |\n| State        | State of the order.                                                 |\n\n#### State\n\n| Name     | Description                                        |\n| -------- | -------------------------------------------------- |\n| OPEN     | Accepting bids.                                    |\n| ACTIVE   | Open lease has been created.                       |\n| CLOSED   | No active leases and not accepting bids. Terminal. |\n\n### Bid\n\n| Name          | Description                                                |\n| ------------- | ---------------------------------------------------------- |\n| ID.OrderID    | [Group](#group) ID of group.                 |\n| ID.Provider   | Account address of provider.                               |\n| State         | State of the bid.                                          |\n| EndsOn        | Height at which the bid ends if it is not already matched. |\n| Price         | Bid price - amount to be paid on every block.              |\n\n#### State\n\n| Name     | Description                              |\n| -------- | ---------------------------------------- |\n| OPEN     | Awaiting matching.                       |\n| ACTIVE   | Bid for an active lease (winner).        |\n| CLOSED   | No active leases for this bid. Terminal. |\n\n### Lease\n\n| Name    | Description                                                 |\n| ------- | ----------------------------------------------------------- |\n| ID      | The same as the [bid](#bid) ID for the lease. |\n| State   | State of the bid.                                           |\n\n#### State\n\n| Name     | Description                                              |\n| -------- | -------------------------------------------------------- |\n| ACTIVE   | Active lease - tenant is paying provider on every block. |\n| CLOSED   | No payments being made. Terminal.                        |\n","description":null,"slug":"docs/getting-started/intro-to-akash/bids-and-leases"},{"title":"DSEQ, GSEQ and OSEQ","body":"\n## GSEQ - Group Sequence\n\n- Akash GSEQ distinguishes “groups” of containers in a deployment, allowing each group to be leased independently. Orders, bids, and leases all act on a single group.\n- Typically, Akash deployments use GSEQ=1, with all pods associated with the deployment using a single provider.\n- An example SDL section that specifies a GSEQ other than 1 is provided below. It requests bids from multiple providers via the declaration of multiple placement sections (`westcoast` and `eastcoast`).\n- In this example, the westcoast placement section has an attribute of `region: us-west`, and the eastcoast placement has `region: us-east`, ensuring the pods land in the desired regions.\n\n```yaml\nplacement:\n  westcoast:\n    attributes:\n      region: us-west\n    pricing:\n      grafana-profile:\n        denom: uakt\n        amount: 10000\n  eastcoast:\n    attributes:\n      region: us-east\n    pricing:\n      postgres-profile:\n        denom: uakt\n        amount: 10000\n```\n\n- When the deployment uses multiple placement sections, GSEQ defines individual, unique orders. The GSEQ values distinguish orders, as shown in the deployment creation output below. Note the value of GSEQ=1 in the first order and GSEQ=2 in the second order.\n\n```json\n{\"order-created\"},{\"key\":\"owner\",\"value\":\"akash1ggk74pf9avxh3llu30yfhmr345h2yrpf7c2cdu\"},{\"key\":\"dseq\",\"value\":\"9507298\"},{\"key\":\"gseq\",\"value\":\"1\"},{\"key\":\"oseq\",\"value\":\"1\"}\n\n{\"order-created\"},{\"key\":\"owner\",\"value\":\"akash1ggk74pf9avxh3llu30yfhmr345h2yrpf7c2cdu\"},{\"key\":\"dseq\",\"value\":\"9507298\"},{\"key\":\"gseq\",\"value\":\"2\"},{\"key\":\"oseq\",\"value\":\"1\"}\n```\n\n## OSEQ - Order Sequence\n\n- Akash OSEQ distinguishes multiple orders associated with a single deployment.\n- Typically, Akash deployments use OSEQ=1 with only a single order associated with the deployment.\n- OSEQ is incremented when a lease associated with an existing deployment is closed, and a new order is generated.\n- _**NOTE**_: OSEQ increments only when the deployment is left open, and the lease is closed (via `lease close`). It does not increment when the deployment is closed and created anew.\n- To illustrate OSEQ usage, consider the following example:\n  - Initially, we create a typical deployment:\n\n```bash\nprovider-services tx deployment create deploy.yml --from $AKASH_KEY_NAME\n```\n\n  - After creating the deployment, we receive standard DSEQ, GSEQ, and OSEQ values:\n\n```json\n{\"order-created\"},{\"key\":\"owner\",\"value\":\"akash1ggk74pf9avxh3llu30yfhmr345h2yrpf7c2cdu\"},{\"key\":\"dseq\",\"value\":\"9507524\"},{\"key\":\"gseq\",\"value\":\"1\"},{\"key\":\"oseq\",\"value\":\"1\"}]\n```\n\n  - We proceed through lease creation with the desired provider.\n  - Later, if we decide to move the deployment to a new provider and prefer to leave the deployment itself open, only closing the current lease, we use:\n\n```bash\nprovider-services tx market lease close --node $AKASH_NODE --dseq $AKASH_DSEQ --provider $AKASH_PROVIDER --from $AKASH_KEY_NAME\n```\n\n  - With the lease of the former provider now closed, a new order is generated, and the OSEQ is incremented to 2. The bid list from the new order displays this increment, as shown in the output below.\n  - _**NOTE**_: To display this bid list, use the following command syntax, where env variables oseq and gseq are set to `0`. This syntax displays all bids for a deployment regardless of the OSEQ/GSEQ current sequence number.\n\n### Command Used:\n\n```bash\nprovider-services query market bid list --owner=$AKASH_ACCOUNT_ADDRESS --node $AKASH_NODE --dseq $AKASH_DSEQ  --gseq 0 --oseq 0\n```\n\n### Output:\n\n```yaml\nbid:\n    bid_id:\n      dseq: \"9507524\"\n      gseq: 1\n      oseq: 2\n      owner: akash1ggk74pf9avxh3llu30yfhmr345h2yrpf7c2cdu\n      provider: akash1lmaulqyvlj0wwcjm5dgqn5wv5j957g672g20ht\n    created_at: \"9507559\"\n```","description":null,"slug":"docs/getting-started/intro-to-akash/dseq-gseq-oseq"},{"title":"Providers","body":"\n\n\nAkash providers are entities that contribute computing resources to the Akash Network, a decentralized cloud computing marketplace. They can be individuals or organizations with underutilized computing resources, such as data centers or personal servers. Providers participate in the network by running the Akash node software and setting the price for their services. Users can then choose a provider based on factors such as cost, performance, and location.\n\n## Key Components\n\n1. **Node Software**: Providers run the Akash node software to connect to the network and offer their computing resources. This software allows providers to set their pricing, monitor usage, and manage the resources they offer.\n\n2. **Staking**: Providers are required to stake AKT tokens as collateral to participate in the marketplace. This ensures that providers have a vested interest in the network's success and incentivizes them to offer reliable services.\n\n3. **Reverse Auction**: The Akash Network uses Reverse Auction to automate the process of buying and selling computing resources. Providers interact with tenants and the blockchain to offer their services and receive payment.\n\nIn summary, Akash providers are a crucial part of the Akash Network, contributing computing resources to the decentralized cloud computing marketplace. They help make the network more accessible, affordable, and secure for users who require cloud computing services.\n","description":null,"slug":"docs/getting-started/intro-to-akash/providers"},{"title":"RPC Nodes","body":"\n\nA Remote Procedure Call or RPC node is a type of computer server that allows users to read data on the blockchain and send transactions to different networks.\nThere are many different implementations of RPC such as JSON-RPC, gRPC, XML-RPC, and many more.","description":null,"slug":"docs/getting-started/intro-to-akash/rpc-nodes"},{"title":"Validator Nodes","body":"\n\nValidator nodes play a crucial role in maintaining the security and integrity of the Akash Network.\n\nValidator nodes in the Akash ecosystem are responsible for validating transactions, proposing new blocks, and maintaining the consensus state of the network. They achieve this by participating in the consensus algorithm, which is based on the Tendermint Core engine and the Cosmos SDK.\n\n## Key Responsibilities\n\n1. **Block Validation**: Validator nodes verify the transactions included in a block and ensure they adhere to the network's rules and protocol.\n2. **Block Propagation**: Validator nodes propagate the validated blocks to other nodes in the network.\n3. **Consensus**: Validator nodes participate in the consensus process, using the Tendermint Proof-of-Stake (PoS) algorithm to elect a block proposer and confirm the validity of proposed blocks.\n4. **Governance**: Validator nodes take part in the governance process by voting on proposals to modify network parameters, upgrade the protocol, or distribute community funds.\n\n## Validator Rewards and Risks\n\nValidator nodes receive rewards in the form of newly minted AKT tokens and transaction fees for participating in the network. However, they also face potential risks:\n\n- **Uptime Risk**: Validators with poor uptime or poor network connectivity might be penalized by having their staked AKT tokens slashed.\n- **Double Signing Risk**: Validators that sign conflicting blocks might have their staked AKT tokens slashed as well.\n- **Commission Rate Risk**: Validators with uncompetitive commission rates might not attract enough delegators and thus have lower voting power in the network.\n\nIt's crucial to ensure a secure, stable, and well-maintained validator node to avoid penalties and maximize rewards.\n","description":null,"slug":"docs/getting-started/intro-to-akash/validator-nodes"},{"title":"Stack Definition Language (SDL)","body":"\nCustomers/tenants define the deployment services, datacenters, requirements, and pricing parameters in a \"manifest\" file (`deploy.yaml`). The file is written in a declarative language called Software Definition Language (SDL). SDL is a human-friendly data standard for declaring deployment attributes. The SDL file is a \"form\" to request resources from the Network. SDL is compatible with the [YAML](https://yaml.org/) standard and similar to Docker Compose files.\n\nConfiguration files may end in `.yml` or `.yaml`.\n\nA complete deployment has the following sections:\n\n- [Networking](#networking)\n- [Version](#version)\n- [Services](#services)\n  - [services.env](#servicesenv)\n  - [services.expose](#servicesexpose)\n  - [services.expose.to](#servicesexposeto)\n- [Profiles](#profiles)\n  - [profiles.compute](#profilescompute)\n  - [profiles.placement](#profilesplacement)\n  - [profiles.placement.signedBy](#profilesplacementsignedby)\n- [Deployment](#deployment)\n- [GPU Support](#gpu-support)\n- [Stable Payment](#stable-payment)\n\nAn example deployment configuration can be found [here](https://github.com/akash-network/docs/tree/62714bb13cfde51ce6210dba626d7248847ba8c1/sdl/deployment.yaml).\n\n## Networking\n\nNetworking, allowing connectivity to and between workloads, can be configured via the Stack Definition Language (SDL) file for a deployment. By default, workloads in a deployment group are isolated - nothing else is allowed to connect to them. This restriction can be relaxed.\n\n## Version\n\nIndicates the version of the Akash configuration file. Currently, only `\"2.0\"` is accepted.\n\n## Services\n\nThe top-level `services` entry contains a map of workloads to be run on the Akash deployment. Each key is a service name; values are a map containing various keys for configuration.\n\n### services.env\n\nA list of environment variables to expose to the running container.\n\n```yaml\nenv:\n  - API_KEY=0xcafebabe\n  - CLIENT_ID=0xdeadbeef\n```\n\n### services.expose\n\nThe `expose` key is a list describing what can connect to the service. Each entry is a map containing one or more of the following fields:\n\n- `port`: Container port to expose\n- `as`: Port number to expose the container port as\n- `accept`: List of hosts to accept connections for\n- `proto`: Protocol type. Valid values = `tcp` or `udp`\n- `to`: List of entities allowed to connect. See [services.expose.to](#services.expose.to)\n\n### services.expose.to\n\n`expose.to` is a list of clients to accept connections from. Each item is a map with one or more of the following entries:\n\n- `service`: A service in this deployment\n- `global`: `true` or `false`\n\n## Profiles\n\nThe `profiles` section contains named compute and placement profiles to be used in the [deployment](#deployment).\n\n### profiles.compute\n\n`profiles.compute` is a map of named compute profiles, specifying resources for each service instance.\n\nExample:\n\n```yaml\nweb:\n  cpu: 2\n  memory: \"2Gi\"\n  storage: \"5Gi\"\n```\n\n### profiles.placement\n\n`profiles.placement` is a map of named datacenter profiles, specifying required attributes and pricing configuration for each [compute profile](#profiles.compute).\n\nExample:\n\n```yaml\nwestcoast:\n  attributes:\n    region: us-west\n  pricing:\n    web:\n      denom: uakt\n      amount: 8\n    db:\n      denom: uakt\n      amount: 100\n```\n\n### profiles.placement.signedBy\n\nOptional section allowing the specification of attributes that must be signed by one or more accounts.\n\n## Deployment\n\nThe `deployment` section defines how to deploy the services. It is a mapping of service name to deployment configuration.\n\nExample:\n\n```yaml\nweb:\n  westcoast:\n    profile: web\n    count: 20\n```\n\nThis deploys 20 instances of the `web` service to a datacenter matching the `westcoast` datacenter profile.\n\n## GPU Support\n\nGPUs can be added to your workload via inclusion in the compute profile section.\n\n## Stable Payment\n\nUse of Stable Payments is supported in the Akash SDL and is declared in the placement section of the SDL.","description":null,"slug":"docs/getting-started/stack-definition-language"},{"title":"Technical Support","body":"\n## Support Chat on Discord\n\nThe [Akash server on Discord](https://discord.akash.network) is a great place to find friendly help from the Akash developer community. Come say hello and ask a question!\n\n**For help with:**\n\n* **Deployments:** Message the **#deployments** channel\n* **Providers:** Message the **#providers** channel\n* **Validators:** Message the **#validators** channel","description":null,"slug":"docs/getting-started/technical-support"},{"title":"Tokens and Wallets","body":"\n## Why Do I Need a Wallet?\n\nWhen using cryptocurrency, setting up a wallet is crucial to hold and utilize your coins. A wallet, whether software or hardware, allows you to transfer cryptocurrency from an exchange to a usable location. Using your own wallet, as opposed to storing crypto on an exchange, reduces the risk of losing funds due to an exchange hack. Additionally, it enables you to deploy your crypto into Decentralized Finance (DeFi) applications like [Osmosis DEX](https://app.osmosis.zone) or stake your funds at higher rates. Exchange wallets are suitable for buying crypto, but non-exchange wallets are better for various other purposes.\n\n## Compatible Wallets in the Cosmos Ecosystem\n\nThe Cosmos Ecosystem offers several compatible wallets. Two particularly user-friendly options are [Leap](#leap-cosmos-wallet) and [Keplr](#keplr-wallet). These wallets come pre-installed with support for various networks, including Cosmos, Osmosis, Secret Network, Akash, Crypto.org, Starname, Sifchain, Certik, Irisnet, Regen, Persistence, Sentinel, Kava, Cyber, and Straightedge. Moreover, you can add any Cosmos-based token through a few extra steps. Follow the Leap or Keplr tutorial in this section.\n\n## Developer Tokens\n\nDevelopers can start experimenting with Akash Deployments within our Sandbox network. The Sandbox allows short-term deployments (may be destroyed after 24 hours of runtime) without spending actual funds to get started.\n\n\n\n## Purchasing Akash Tokens\n\n### Decentralized Exchange (DEX)\n\nThe easiest way to purchase Akash Tokens ($AKT) is to buy Cosmos tokens ($ATOM) and swap your $ATOM for $AKT using a decentralized exchange (DEX). From there, you can send the ATOM to a Keplr wallet and use [Osmosis](https://app.osmosis.zone/) to convert it into AKT. For currently supported tokens in Osmosis, check the token types in the Osmosis web app.\n\nFunding your network account is required to use the network. All messages charge a transaction fee, and deployment leases are paid by the account used to create them. You can fund your account by buying tokens or applying for awards from the community.\n\n### Centralized Exchange (CEX)\n\nTokens can be purchased on [exchanges listed here](https://akash.network/token). After purchasing, you can send tokens to your Akash account address.\n\n---\n\n*Description: Original content produced by Anthony Rosa as part of the community challenge.*\n\n# Keplr Wallet\n\n**Here is a [YouTube version](https://www.youtube.com/watch?v=KGu3wiwcxNc\\&t=642s) of this guide where I show you how to download, fund, and trade Akash with a Keplr wallet.**\n\n## How to Download and Install Keplr for Google Chrome:\n\n1. Navigate to the [Google Chrome Extension Store](https://chrome.google.com/webstore/category/extensions?hl=en).\n2. Search for [Keplr](https://chrome.google.com/webstore/detail/keplr/dmkamcknogkgcdfhhbddcghachkejeap?hl=en).\n3. Click “Add to Chrome”\n\n   ![Image](https://miro.medium.com/max/1400/1*b-gl3aaJpxx4_VAK4T58QA.png)\n\n4. Click the puzzle piece icon in the top right corner of the browser. These are your extensions.\n\n   ![Image](https://miro.medium.com/max/408/1*6TXuj66rkr9uDZ3K3U6x_A.png)\n\n5. Pin Keplr.\n\n   ![Image](https://miro.medium.com/max/600/1*A3LlAK2TNjx4jGEgK5HCiw.png)\n\n6. Click the Keplr icon.\n\n   ![Image](https://miro.medium.com/max/272/1*fUjYWaDxVltwkbu_LWjsvg.png)\n\n7. Click “Create new account”\n\n   ![Image](https://miro.medium.com/max/1288/1*eu6QM_p5jbeorJQTWXMNXg.png)\n\n8. **Write down your Mnemonic Seed 12 word phrase on a piece of paper.** You’ll need these for the next step and if you lose your account information, the mnemonic is the only way to get it back. Give your account a name and type in a password of choice. Click Next.\n\n   ![Image](https://miro.medium.com/max/1400/1*3j8sS3D8YIJbBhNsb2uiig.png)\n\n9. Use your piece of paper to click the boxes to arrange the Mnemonic Seed in its proper order. Once complete, **store your Mnemonic Seed in a safe, protected place**. Click “Register”.\n\n   ![Image](https://miro.medium.com/max/1400/1*8ax_97-t6IRo2mWw7HV1cg.png)\n\n10. Click the Keplr icon.\n\n   ![Image](https://miro.medium.com/max/272/1*OFgsSrkW2yYXQHujo9uI4Q.png)\n\n11. Click the dropdown Cosmos menu. Click “Akash”.\n\n   ![Image](https://miro.medium.com/max/744/1*PPobLOFbdYCNhXEyWmTxxw.png)\n\n12. You have now downloaded Keplr wallet and are ready to fund your wallet with Akash!\n\n   ![Image](https://miro.medium.com/max/736/1*kF2UUXQgSei23dqutkDNVg.png)\n\n13. Fund Wallet.\n\n   All that is left is to send AKT tokens to the wallet address provided by Keplr.\n\n# Leap Cosmos wallet\n\n**How to Install the Leap Cosmos Wallet Extension:**\n\nNavigate to the [official site](https://www.leapwallet.io/#inpage-download).\n\nWe are using **Google Chrome** in this example:\n\n1. Click on the **Chrome** icon.\n\n   ![Image](https://github.com/Dimokus88/docs/assets/23629420/2e23aa24-6ea4-42a2-a70f-e4497fd911b0)\n\n2. Click “**Install**”.\n\n   ![Image](https://github.com/Dimokus88/docs/assets/23629420/4443a3f0-70fa-4e82-a732-fcc4552561a4)\n   \n3. Click the puzzle piece icon in the top right corner of the browser. These are your extensions.\n\n   ![Image](https://github.com/Dimokus88/docs/assets/23629420/e2a09fbf-9cab-4fe2-af15-e3990b789800)\n\n4. Pin the Leap Cosmos Wallet icon.\n\n   ![Image](https://github.com/Dimokus88/docs/assets/23629420/4c213adc-9926-4fd2-\n\n9094-bbd2120bcf3a)\n\n5. Click the Leap Cosmos Wallet icon.\n\n   ![Image](https://github.com/Dimokus88/docs/assets/23629420/2168c244-2253-4e37-9d00-bc29a50e1dbb)\n\n6. Click “**Create new wallet**”.\n\n   ![Image](https://github.com/Dimokus88/docs/assets/23629420/d6c722fc-e526-479b-ae63-f83ce0696dd9)\n\n7.  ⚠️ ✍️ **Write down your Mnemonic Seed 12 word phrase on a piece of paper.** ✍️⚠️  \n\n   You’ll need these for the next step and if you lose your account information, the mnemonic is the only way to get it back. \n\n   Click \"**I have saved it somewhere safe.**\".\n\n   ![Image](https://github.com/Dimokus88/docs/assets/23629420/08048f1c-4293-48a3-91fa-e0ec16c199f4)\n\n8. Go through the procedure of checking the record of your mnemonic phrase, add the missing words to the fields. \n\n   Click \"**Confirm**\".\n\n   ![Image](https://github.com/Dimokus88/docs/assets/23629420/42638118-746d-40e0-ba07-e4fcb5bdfb0a)\n\n9. Set a password for Leap Cosmos Wallet. \n\n   If you forget this password, you can always reset it by recovering your account from scratch using your Mnemonic Seed you have written down somewhere safe before.\n\n   ![Image](https://github.com/Dimokus88/docs/assets/23629420/5972f45b-7034-4dc0-85b4-1f8b92992fdf)\n\n10. Click the Leap Cosmos Wallet icon.\n\n    ![Image](https://github.com/Dimokus88/docs/assets/23629420/2168c244-2253-4e37-9d00-bc29a50e1dbb)\n\n11. Set Akash Network.\n\n    ![Image](https://github.com/Dimokus88/docs/assets/23629420/d0479e4e-ddeb-4374-a6d5-27fd34ec4b3a)\n    ![Image](https://github.com/Dimokus88/docs/assets/23629420/6cb03209-a7f3-422e-9bb6-917f27d91d9c)\n\n12. You have now configured Leap Cosmos Wallet!\n\n    ![Image](https://github.com/Dimokus88/docs/assets/23629420/511254dc-9cf7-4b1e-ba21-5dca3c831a08)\n\n13. Get address **AKT** and fund your wallet.\n\n    All that is left is to send AKT tokens to the wallet address provided by Leap Cosmos Wallet. \n    You can purchase AKT tokens [here](https://akash.network/token/#buying-akt).\n","description":null,"slug":"docs/getting-started/token-and-wallets"},{"title":"Chia on Akash","body":"\n## Why use Akash?\n\nWelcome [Chia](https://www.chia.net/) community! We are excited to announce support for Chia on the [Akash](https://akash.network/) network! You can now run nodes, plotting, and farming on our marketplace of compute. Below you will find details on how to configure your deployment for different use cases. Akash is a part of the [Cosmos](https://cosmos.network/) ecosystem of blockchains.\n\n## Summer Sale\n\n![](<../../../assets/Summer Sale for Chia Plots 5.png>)\n\n### Providers\n\nFor the following providers who are participating in the sale, expect to see these prices in Cloudmos! Each provider has been benchmarked and tested to create a $0.10/plot.\n\n| On-Sale Providers      | BladeBit Price / Month  | MadMax Price / Month   |\n| ---------------------- | ----------------------- | ---------------------- |\n| bigtractorplotting.com | $556 \\| 8 Minute Plots  | $59 \\| 71 Minute Plots |\n| xch.computer           | $363 \\| 12 Minute Plots | $44 \\| 95 Minute Plots |\n| akash.world            | $174 \\| 24 Minute Plots | $42 \\| 99 Minute Plots |\n\n### Required SDL\n\nTo make sure you get the sale price from the providers, please Copy and Paste the SDL into [Cloudmos](broken-reference) :\n\n[BladeBit Summer Sale SDL](#bladebit-ram-plotting)\n\n[MadMax Summer Sale SDL](#madmax-disk-plotting)\n\nPlease wait up to 60 seconds to see bids from all the providers.\n\n## Windows/Linux/Mac Users\n\n1. Install [Keplr](https://chrome.google.com/webstore/detail/keplr/dmkamcknogkgcdfhhbddcghachkejeap?hl=en) wallet as a browser plugin\n2. Install [Cloudmos Deploy](https://cloudmos.io/cloud-deploy) and import your AKT wallet address from Keplr.\n3. [Fund your wallet](https://github.com/akash-network/awesome-akash/tree/chia/chia#Quickest-way-to-get-more-AKT)\n\nFor additional help we recommend you [follow our full deployment guide](broken-reference) in parallel with this guide.\n\n## How does this work?\n\nAkash uses its blockchain to manage your container deployment and accounting. To deploy on Akash you will need to fund your wallet with at least 10 AKT. Each time you create a deployment, 5 AKT will be used for escrow and to fund the deployment. If the deployment is canceled, the balance of the escrow is returned to you. You can spin up deployments without worrying about any long term contracts and you can cancel anytime.\n\n## Plotting Demo\n\n{% embed url=\"https://www.youtube.com/watch?v=xCNoXI6_Tf8\" %}\n@DigitalSpaceport\n{% endembed %}\n\n{% embed url=\"https://www.youtube.com/watch?v=HLhrSeDemBI\" %}\nJonmichael Hands - VP of Storage Business Development at Chia\n{% endembed %}\n\n{% embed url=\"https://youtu.be/RY2cjiizk5k?t=1434\" %}\nAndrew Mello - Head of Mining at Akash\n{% endembed %}\n\n## Default wallet\n\nAkash uses [Keplr](https://chrome.google.com/webstore/detail/keplr/dmkamcknogkgcdfhhbddcghachkejeap?hl=en) as the desktop wallet. Advanced users can follow the [CLI instruction](/docs/docs/deployments/akash-cli/installation/)s.\n\nOnce you have set up your Keplr wallet and imported the address to Cloudmos you are ready to create your first deployment. Follow the instructions in Cloudmos to create a certificate, then click on _Create Deployment_.\n\n## Quickest way to get more AKT\n\nTo fund your deployment you will need AKT in your account. The fastest way to do that is in one of the following two ways.\n\n### Buy on an Exchange\n\n1. Install [Keplr](https://chrome.google.com/webstore/detail/keplr/dmkamcknogkgcdfhhbddcghachkejeap?hl=en)\n2. Buy AKT on an [exchange](https://www.coingecko.com/en/coins/akash-network#markets)\n3. Withdraw your AKT to your Keplr wallet\n\n### Swap from ATOM to AKT\n\n1. Install [Keplr](https://chrome.google.com/webstore/detail/keplr/dmkamcknogkgcdfhhbddcghachkejeap?hl=en)\n2. Send 10 ATOM to your new Cosmos wallet address inside Keplr (this address will start with cosmos)\n3. Go to [Osmosis Assets](https://app.osmosis.zone/assets) > next to Cosmos Hub - ATOM click on Deposit. This step will deposit ATOM from your Keplr wallet onto the Osmosis platform. Press Connect Wallet to connect your Keplr wallet to Osmosis.\n4. Now go back to [Osmosis Homepage](https://app.osmosis.zone/?from=ATOM\\&to=AKT) and ensure ATOM > AKT is selected to complete the swap. This step swaps your ATOM you deposited onto the Osmosis platform into any other supported coin.\n5. Return to the [Osmosis Assets](https://app.osmosis.zone/assets) page to withdraw your AKT to your Keplr wallet. This step withdraws AKT from the Osmosis platform back into your Keplr wallet. You can now send AKT to Cloudmos.\n\nHave more questions? Find our team in [Discord](https://discord.com/invite/DxftX67) and [Telegram](https://t.me/AkashNW).\n\n## MadMax Disk Plotting\n\n**Recommended MadMax CPU Settings for 1Gbps Connections:**\\\n\\*\\*\\*\\*\\~75 minute plots = 8 cpu / 815Gi Storage\n\n```yaml\n---\nversion: \"2.0\"\n\nservices:\n  chia:\n    image: cryptoandcoffee/akash-chia:262\n    expose:\n      - port: 8080\n        as: 80\n        proto: tcp\n        to:\n          - global: true\n    env:\n    #############################REQUIRED##############################\n      - VERSION=1.5.1\n     #Always check https://github.com/Chia-Network/chia-blockchain/releases\n      - CONTRACT=\n      - FARMERKEY=\n      - PLOTTER=madmax\n     #Choose your plotter software - madmax, bladebit, bladebit-disk\n      - FINAL_LOCATION=local\n     #Set to \"local\" to access finished plots through web interface.\n     #Set to \"upload\" and finished plots will be uploaded to a SSH destination like user@ip:/home/user/plots\n      - CPU_UNITS=8\n      - MEMORY_UNITS=6Gi\n      - STORAGE_UNITS=815Gi\n     #Must match CPU/Memory/Storage units defined in resources.\n    #############################OPTIONAL##############################\n     #Uncomment the variables below when set FINAL_LOCATION=upload to enable remote uploading\n      #- REMOTE_HOST=changeme.com #SSH upload host\n      #- REMOTE_LOCATION=changeme #SSH upload location like /root/plots\n      #- REMOTE_PORT=22 #SSH upload port\n      #- REMOTE_USER=changeme #SSH upload user\n      #- REMOTE_PASS=changme #SSH upload password\n      #- UPLOAD_BACKGROUND=true\n     #Change to true to enable multiple background uploading of plots, this is the best option to use use 100% of your bandwidth.\n      #- RAMCACHE=32G\n      #Used only for PLOTTER=bladebit-disk, you must increase the memory resources requested below with this additional cache size.\n      #- RCLONE=false\n     #When true must also update JSON_RCLONE and add any destination in same format.\n      #- TOTAL_UPLOADS=1000\n     #Set the total number of parallel uploads allowed to an rclone destination\n      #- ENDPOINT_LOCATION=\n     #Only used for RCLONE=true\n      #- ENDPOINT_DIR=\n     #Only used for RCLONE=true\n      #- JSON_RCLONE=\n      #  [storj]\\n\n      #  type = storj\\n\n      #  api_key = x\\n\n      #  passphrase = x\\n\n      #  satellite_address = x@x:7777\\n\n      #  access_grant = replaceme\n     #Example of STORJ config for RCLONE=true.  If you want to use your own endpoint please escape each line with a backslash n, like in the example.\nprofiles:\n  compute:\n    chia:\n      resources:\n        cpu:\n          units: 8\n        memory:\n          size: 6Gi\n        storage:\n          size: 815Gi\n  placement:\n    akash:\n      signedBy:\n        anyOf:\n          - \"akash1365yvmc4s7awdyj3n2sav7xfx76adc6dnmlx63\"\n          - \"akash18qa2a2ltfyvkyj0ggj3hkvuj6twzyumuaru9s4\"\n      attributes:\n        chia-plotting: \"true\"\n      pricing:\n        chia:\n          denom: uakt\n          amount: 100000\ndeployment:\n  chia:\n    akash:\n      profile: chia\n      count: 1\n```\n\n## Bladebit RAM Plotting\n\nPlotting with Bladebit has never been easier! There are a few things to note before you start using Bladebit instead of Madmax. Bladebit is so fast it can create plots faster than most home/consumer internet connections (1Gbps) can download them. To compensate for this we can adjust the Bladebit plotting speed by changing the CPU count of the deployment.\\\n\\\n**Recommended Bladebit CPU Settings for 1Gbps Connections:**\\\n\\*\\*\\*\\*\\~20 minute plots = 8 cpu / 915Gi Storage\\\n\\~15 minute plots = 16 cpu / 915Gi Storage\\\n\\~10 minute plots = 32 cpu / 2Ti Storage\\\n\\\n**Recommended Bladebit CPU Settings for Multi-Gigabit Connections:**\\\n\\*\\*\\*\\*\\~7 minutes plots = 64 CPU / 2Ti Storage\\\n\\~4 minutes plots = 100 cpu / 4Ti Storage\\\n\\~3 minutes plots = 186 cpu / 6Ti Storage\n\n\\\nFor a standard 1Gbps connection use the settings below, otherwise adjust the CPU units to match the plot time you want to achieve.\n\n```yaml\n---\nversion: \"2.0\"\n\nservices:\n  chia:\n    image: cryptoandcoffee/akash-chia:262\n    expose:\n      - port: 8080\n        as: 80\n        proto: tcp\n        to:\n          - global: true\n    env:\n    #############################REQUIRED##############################\n      - VERSION=1.5.1\n     #Always check https://github.com/Chia-Network/chia-blockchain/releases\n      - CONTRACT=\n      - FARMERKEY=\n      - PLOTTER=bladebit\n     #Choose your plotter software - madmax, bladebit, bladebit-disk\n      - FINAL_LOCATION=local\n     #Set to \"local\" to access finished plots through web interface.\n     #Set to \"upload\" and finished plots will be uploaded to a SSH destination like user@ip:/home/user/plots\n      - CPU_UNITS=32\n      - MEMORY_UNITS=420Gi\n      - STORAGE_UNITS=1200Gi\n     #Must match CPU/Memory/Storage units defined in resources.\n    #############################OPTIONAL##############################\n     #Uncomment the variables below when set FINAL_LOCATION=upload to enable remote uploading\n      #- REMOTE_HOST=changeme.com #SSH upload host\n      #- REMOTE_LOCATION=changeme #SSH upload location like /root/plots\n      #- REMOTE_PORT=22 #SSH upload port\n      #- REMOTE_USER=changeme #SSH upload user\n      #- REMOTE_PASS=changme #SSH upload password\n      #- UPLOAD_BACKGROUND=true\n     #Change to true to enable multiple background uploading of plots, this is the best option to use use 100% of your bandwidth.\n      #- RAMCACHE=32G\n      #Used only for PLOTTER=bladebit-disk, you must increase the memory resources requested below with this additional cache size.\n      #- RCLONE=false\n     #When true must also update JSON_RCLONE and add any destination in same format.\n      #- TOTAL_UPLOADS=1000\n     #Set the total number of parallel uploads allowed to an rclone destination\n      #- ENDPOINT_LOCATION=\n     #Only used for RCLONE=true\n      #- ENDPOINT_DIR=\n     #Only used for RCLONE=true\n      #- JSON_RCLONE=\n      #  [storj]\\n\n      #  type = storj\\n\n      #  api_key = x\\n\n      #  passphrase = x\\n\n      #  satellite_address = x@x:7777\\n\n      #  access_grant = replaceme\n     #Example of STORJ config for RCLONE=true.  If you want to use your own endpoint please escape each line with a backslash n, like in the example.\nprofiles:\n  compute:\n    chia:\n      resources:\n        cpu:\n          units: 32\n        memory:\n          size: 420Gi\n        storage:\n          size: 1200Gi\n  placement:\n    akash:\n      signedBy:\n        anyOf:\n          - \"akash1365yvmc4s7awdyj3n2sav7xfx76adc6dnmlx63\"\n          - \"akash18qa2a2ltfyvkyj0ggj3hkvuj6twzyumuaru9s4\"\n      attributes:\n        chia-plotting: \"true\"\n      pricing:\n        chia:\n          denom: uakt\n          amount: 100000\ndeployment:\n  chia:\n    akash:\n      profile: chia\n      count: 1\n```\n\n## Downloading plots\n\nTo access the Chia Plot Manager, click on the \\`Uri\\` link on the deployment detail page.\\\nTo download plots, click an invididual plot in the Chia Plot Manager and click on Download/Open.\n\n![Chia Plot Manager](<../../../assets/image (13).png>)\n\n\\*Once your download has finished - Delete the plot from the container - to make room for new plots! Plots will continue to be created as long as there is enough free space available in the container (Max 32Tb) and the deployment is fully funded.\n\n## Uploading plots\n\nIf you want to upload plots created on Akash directly to a remote destination such as your farm or a storage provider, you have 2 main options.\n\n### SSH\n\nUpload your plots to any SSH destination by modifying the `env:`\n\n```yaml\n    env:\n    #############################REQUIRED##############################\n      - VERSION=1.5.1\n     #Always check https://github.com/Chia-Network/chia-blockchain/releases\n      - CONTRACT=\n      - FARMERKEY=\n      - PLOTTER=bladebit\n     #Choose your plotter software - madmax, bladebit, bladebit-disk\n      - FINAL_LOCATION=local\n     #Set to \"local\" to access finished plots through web interface.\n     #Set to \"upload\" and finished plots will be uploaded to a SSH destination like user@ip:/home/user/plots\n      - CPU_UNITS=32\n      - MEMORY_UNITS=420Gi\n      - STORAGE_UNITS=1200Gi\n     #Must match CPU/Memory/Storage units defined in resources.\n    #############################OPTIONAL##############################\n     #Uncomment the variables below when set FINAL_LOCATION=upload to enable remote uploading\n      - REMOTE_HOST=changeme.com #SSH upload host\n      - REMOTE_LOCATION=changeme #SSH upload location like /root/plots\n      - REMOTE_PORT=22 #SSH upload port\n      - REMOTE_USER=changeme #SSH upload user\n      - REMOTE_PASS=changme #SSH upload password\n      - UPLOAD_BACKGROUND=true\n     #Change to true to enable multiple background uploading of plots, this is the best option to use use 100% of your bandwidth.\n      #- RAMCACHE=32G\n      #Used only for PLOTTER=bladebit-disk, you must increase the memory resources requested below with this additional cache size.\n      #- RCLONE=false\n     #When true must also update JSON_RCLONE and add any destination in same format.\n      #- TOTAL_UPLOADS=1000\n     #Set the total number of parallel uploads allowed to an rclone destination\n      #- ENDPOINT_LOCATION=\n     #Only used for RCLONE=true\n      #- ENDPOINT_DIR=\n     #Only used for RCLONE=true\n      #- JSON_RCLONE=\n      #  [storj]\\n\n      #  type = storj\\n\n      #  api_key = x\\n\n      #  passphrase = x\\n\n      #  satellite_address = x@x:7777\\n\n      #  access_grant = replaceme\n     #Example of STORJ config for RCLONE=true.  If you want to use your own endpoint please escape each line with a backslash n, like in the example.\n```\n\n### Rclone\n\nUpload your plots to any [Rclone](https://rclone.org/) endpoint! You need to first create a connection to your endpoint on a standard client so that you have a valid configuration in `~/.config/rclone/rclone.conf` You need to modify this block and add  to the end of each line to make it valid for Akash. Below you can find examples of how the `env:` should look.\n\n### Rclone to Dropbox\n\nChange the `replaceme` values below to match your client settings\n\n```yaml\n    env:\n    #############################REQUIRED##############################\n      - VERSION=1.5.1 \n     #Always check https://github.com/Chia-Network/chia-blockchain/releases\n      - CONTRACT=\n      - FARMERKEY=\n      - PLOTTER=bladebit\n     #Choose your plotter software - madmax, bladebit, bladebit-disk\n      - FINAL_LOCATION=upload\n     #Set to \"local\" to access finished plots through web interface.\n     #Set to \"upload\" and finished plots will be uploaded to a SSH destination like user@ip:/home/user/plots\n      - CPU_UNITS=42\n      - MEMORY_UNITS=430Gi\n      - STORAGE_UNITS=1200Gi\n     #Must match CPU/Memory/Storage units defined in resources.\n    #############################OPTIONAL##############################\n     #Uncomment the variables below when set FINAL_LOCATION=upload to enable remote uploading\n      #- REMOTE_HOST=changeme.com #SSH upload host\n      #- REMOTE_LOCATION=changeme #SSH upload location like /root/plots\n      #- REMOTE_PORT=22 #SSH upload port\n      #- REMOTE_USER=changeme #SSH upload user\n      #- REMOTE_PASS=changme #SSH upload password\n      #- UPLOAD_BACKGROUND=true\n     #Change to true to enable multiple background uploading of plots, this is the best option to use use 100% of your bandwidth.\n      #- RAMCACHE=32G\n     #Used only for PLOTTER=bladebit-disk, you must increase the memory resources requested below with this additional cache size.\n      - RCLONE=true\n     #When true must also update JSON_RCLONE and add any destination in same format.\n      - TOTAL_UPLOADS=8\n     #Set the total number of parallel uploads allowed to an Rclone destination\n      - ENDPOINT_LOCATION=dropbox\n     #Name of Rclone endpoint\n      - ENDPOINT_DIR=replaceme\n        #Upload directory on Dropbox\n      - JSON_RCLONE=\n        [dropbox]\\n\n        type = dropbox\\n\n        client_id = replaceme\\n\n        client_secret = replaceme\\n\n        token = {\"access_token\":\"replaceme\",\"expiry\":\"replaceme\"}\n     #Example of Dropbox config for RCLONE=true.  If you want to use your own endpoint please escape each line with a backslash n, like in the example.\n```\n\n### Rclone to Google Drive\n\nChange the `replaceme` values below to match your client settings\n\n```yaml\n    env:\n    #############################REQUIRED##############################\n      - VERSION=1.5.1 \n     #Always check https://github.com/Chia-Network/chia-blockchain/releases\n      - CONTRACT=\n      - FARMERKEY=\n      - PLOTTER=bladebit\n     #Choose your plotter software - madmax, bladebit, bladebit-disk\n      - FINAL_LOCATION=upload\n     #Set to \"local\" to access finished plots through web interface.\n     #Set to \"upload\" and finished plots will be uploaded to a SSH destination like user@ip:/home/user/plots\n      - CPU_UNITS=42\n      - MEMORY_UNITS=430Gi\n      - STORAGE_UNITS=1200Gi\n     #Must match CPU/Memory/Storage units defined in resources.\n    #############################OPTIONAL##############################\n     #Uncomment the variables below when set FINAL_LOCATION=upload to enable remote uploading\n      #- REMOTE_HOST=changeme.com #SSH upload host\n      #- REMOTE_LOCATION=changeme #SSH upload location like /root/plots\n      #- REMOTE_PORT=22 #SSH upload port\n      #- REMOTE_USER=changeme #SSH upload user\n      #- REMOTE_PASS=changme #SSH upload password\n      #- UPLOAD_BACKGROUND=true\n     #Change to true to enable multiple background uploading of plots, this is the best option to use use 100% of your bandwidth.\n      #- RAMCACHE=32G\n     #Used only for PLOTTER=bladebit-disk, you must increase the memory resources requested below with this additional cache size.\n      - RCLONE=true\n     #When true must also update JSON_RCLONE and add any destination in same format.\n      - TOTAL_UPLOADS=8\n     #Set the total number of parallel uploads allowed to an Rclone destination\n      - ENDPOINT_LOCATION=google\n     #Name of Rclone endpoint\n      - ENDPOINT_DIR=replaceme\n     #Upload directory on Google\n      - JSON_RCLONE=\n        [google]\\n\n        type = drive\\n\n        scope = drive\\n\n        token = {\"access_token\":\"replaceme\",\"token_type\":\"Bearer\",\"refresh_token\":\"replaceme\",\"expiry\":\"replaceme\"}\\n\n        root_folder_id = replaceme\n     #Example of Google config for RCLONE=true.  If you want to use your own endpoint please escape each line with a backslash n, like in the example.\n```\n\n### Rclone to Storj\n\nChange the `replaceme` values below to match your client settings\n\n```yaml\n    env:\n    #############################REQUIRED##############################\n      - VERSION=1.5.1 \n     #Always check https://github.com/Chia-Network/chia-blockchain/releases\n      - CONTRACT=\n      - FARMERKEY=\n      - PLOTTER=bladebit\n     #Choose your plotter software - madmax, bladebit, bladebit-disk\n      - FINAL_LOCATION=upload\n     #Set to \"local\" to access finished plots through web interface.\n     #Set to \"upload\" and finished plots will be uploaded to a SSH destination like user@ip:/home/user/plots\n      - CPU_UNITS=42\n      - MEMORY_UNITS=430Gi\n      - STORAGE_UNITS=1200Gi\n     #Must match CPU/Memory/Storage units defined in resources.\n    #############################OPTIONAL##############################\n     #Uncomment the variables below when set FINAL_LOCATION=upload to enable remote uploading\n      #- REMOTE_HOST=changeme.com #SSH upload host\n      #- REMOTE_LOCATION=changeme #SSH upload location like /root/plots\n      #- REMOTE_PORT=22 #SSH upload port\n      #- REMOTE_USER=changeme #SSH upload user\n      #- REMOTE_PASS=changme #SSH upload password\n      #- UPLOAD_BACKGROUND=true\n     #Change to true to enable multiple background uploading of plots, this is the best option to use use 100% of your bandwidth.\n      #- RAMCACHE=32G\n     #Used only for PLOTTER=bladebit-disk, you must increase the memory resources requested below with this additional cache size.\n      - RCLONE=true\n     #When true must also update JSON_RCLONE and add any destination in same format.\n      - TOTAL_UPLOADS=8\n     #Set the total number of parallel uploads allowed to an Rclone destination\n      - ENDPOINT_LOCATION=storj\n     #Name of Rclone endpoint\n      - ENDPOINT_DIR=replaceme\n     #Upload directory on Storj\n      - JSON_RCLONE=\n        [storj]\\n\n        type = storj\\n\n        api_key = replaceme\\n\n        passphrase = replaceme\\n\n        satellite_address = replaceme\\n\n        access_grant = replaceme\n     #Example of Storj config for RCLONE=true.  If you want to use your own endpoint please escape each line with a backslash n, like in the example.\n```\n\n### Rclone to multiple endpoints\n\n\\* You must use cryptoandcoffee/akash-chia:246 or later!\\\n\\\nIt's possible to configure multiple endpoints. Add the following to your variables and your endpoints will be shuffled and a random endpoint will be chosen for the upload destination.\n\n```\n      - SHUFFLE_RCLONE_ENDPOINT=true\n      - JSON_RCLONE=\n        [google]\\n\n        type = drive\\n\n        scope = drive\\n\n        token = {\"access_token\":\"replaceme\",\"token_type\":\"Bearer\",\"refresh_token\":\"replaceme\",\"expiry\":\"replaceme\"}\\n\n        root_folder_id = replaceme\\n\n        [dropbox]\\n\n        type = dropbox\\n\n        client_id = replaceme\\n\n        client_secret = replaceme\\n\n        token = {\"access_token\":\"replaceme\",\"expiry\":\"replaceme\"}\\n\n        [storj]\\n\n        type = storj\\n\n        api_key = replaceme\\n\n        passphrase = replaceme\\n\n        satellite_address = replaceme\\n\n        access_grant = replaceme\n```\n\nIn this example the finished plots will be uploaded to Google/Dropbox/Storj at random. Please ensure your ENDPOINT\\_DIR (folder) exists on all the endpoints.\n\n### Rclone to multiple folders\n\n\\* You must use cryptoandcoffee/akash-chia:246 or later!\\\n\\\nIt's possible to configure multiple folders. Add the following to your variables and your folders will be shuffled and a random folder will be chosen for the upload destination.\n\n```\n      - SHUFFLE_RCLONE_DIR=true\n      - ENDPOINT_DIR=\"plotz-1 plotz-2 plotz-3 plotz-4 plotz-5\"\n```\n\nIn this example the finished plots will be uploaded to a single ENDPOINT\\_LOCATION but the folder will be randomly chosen from your list. Use a single space to upload to multiple folders.\n\n## Speed up downloads\n\nWindows/Mac/Linux : Use a download manager like [DownThemAll](https://www.downthemall.net/) on Chrome/Firefox/Opera\\\nLinux/CLI: Use [aria2](https://aria2.github.io/) : `apt-get install aria2`\n\n```\naria2c -c -s 16 -x 16 -k 64M -j 1 $plot_download_url\n```\n\n## Waiting for bids...\n\nIf Cloudmos hangs on \"Waiting for Bids\" that could be an indication that all providers are full at the moment. The recommended fix is to reduce the requested storage amount from `1Ti` to something more reasonable like `768Gi`\n\n```\nprofiles:\n  compute:\n    chia:\n      resources:\n        cpu:\n          units: 8.0\n        memory:\n          size: 6Gi\n#Chia blockchain is currently ~40gb as of November 2021 / if you are plotting please use at least 256Gi\n        storage:\n          size: 768Gi\n```\n\n## How to speed up plotting?\n\n### Use only providers with the _chia-plotting_ attribute\n\nTo limit the selection of providers to those with fast storage that meets the requirements for plotting, we recommend you keep the _chia-plotting_ attribute to the placement section of deploy.yaml file.\n\n```\nplacement:\n  akash:\n    attributes:\n      chia-plotting: \"true\"\n    pricing:\n      chia:\n        denom: uakt\n        amount: 100000\n```\n\n### Create more deployments\n\nEasily scale your total plotting output by creating a new deployment. Click the three dots next to the Add Funds button on the deployment details page and click Redeploy.","description":null,"slug":"docs/guides/chia-on-akash"},{"title":"Helium Validator","body":"\n**Repository:** [**tombeynon/helium-on-akash**](https://github.com/tombeynon/helium-on-akash)\\*\\*\\*\\*\n\nThis repository includes everything needed to run a Helium validator on Akash. The container will connect to an S3 bucket to upload/download the swarm\\_key on boot.\n\nThe main files to understand are:\n\n* `Dockerfile` - Installs AWS CLI on top of the [Helium validator docker image](https://quay.io/team-helium/validator) and sets boot.sh to run whenever the container starts.\n* `boot.sh` - Downloads the swarm\\_key from S3 (if it exists), starts the miner and prints the address. It then uploads the swarm\\_key if it didn't download it earlier (new miner).\n* `deploy.yml` - Example/working Akash deployment configuration. This is setup to use my image which may or may not be up to date. See below to create and host your own image if needed.\n\n## Requirements\n\n* [S3 bucket and IAM user](https://docs.aws.amazon.com/AmazonS3/latest/userguide/example-walkthroughs-managing-access-example1.html#grant-permissions-to-user-in-your-account-step1), with access key and secret.\n* [Dockerhub account](https://hub.docker.com/signup) to host your own container image, if required.\n* [Testnet wallet](https://docs.helium.com/mine-hnt/validators/validator-deployment-guide#create-testnet-wallet) to stake to your validator and claim it.\n\n## Run the container locally\n\n```\ndocker run --publish 2154:2154/tcp -e AWS_ACCESS_KEY=mykey -e AWS_SECRET_KEY=mysecret -e S3_KEY_PATH=mybucket/miner1/swarm_key tombeynon/helium-on-akash\n```\n\n## Deployment\n\nYou can deploy the validator on Akash using the example deploy.yml. Note that to use your own image which you can keep up to date, check the next section.\n\nEither clone this repository or create a `deploy.yml` file. Enter your S3 bucket and IAM credentials into the `env` section. If you have a swarm\\_key already, make sure this is uploaded to S3 in the same location as S3\\_KEY\\_PATH.\n\nDeploy as [per the docs](https://docs.akash.network/guides/deploy) or using a [deploy UI](https://github.com/tombeynon/akash-deploy).\n\nOnce the container is deployed, check the logs to see your address once the server starts (can take a while). If your swarm\\_key didn't exist in S3 before, the new one should have been uploaded. Subsequent deploys using the same S3 details will now use the same swarm\\_key.\n\n## Build your own image\n\nThere are a couple of reasons to run your own image:\n\n* Akash requires a version-specific tag to update the container image. If you use e.g. `latest`, updating the deployment won't pull the latest version of the tag. Helium currently publishes their docker images using the `latest` format only, so to tag the image as required we need to publish our own.\n* Testnet moves quickly and I might not keep my image up to date.\n\nCreate a dockerhub account first, then build the image as follows:\n\n```\ngit clone git@github.com:tombeynon/helium-on-akash.git\ncd helium-on-akash\ndocker build . -t mydockerhubuser/helium-on-akash:v0.0.1\ndocker push mydockerhubuser/helium-on-akash:v0.0.1\n```\n\nYou can then change the `image` value in deploy.yml to your repository and version above.\n\nTo update the miner on Akash, run the above to build it with the latest Helium image, incrementing the version number. Then close and re-deploy on Akash using the new version number. This process could be scripted easily.\n\n## Caveats\n\n* Updating the container isn't ideal, this could be improved in the future.\n* Currently, only the swarm\\_key is synced to S3, meaning the entire blockchain needs to be downloaded each time you run the miner. It takes about 30 minutes currently with the suggested deploy.yml.\n* There is a delay between 'Starting miner...' and the logs for the miner showing. The miner is running during this time, just the logs don't show. They appear after 5-10 minutes.\n* The miner currently shows as a relay on the Helium explorer, this might be possible to improve?\n\n## References\n\n* [https://docs.helium.com/mine-hnt/validators/validator-deployment-guide](https://docs.helium.com/mine-hnt/validators/validator-deployment-guide)\n* [https://explorer.helium.wtf/validators](https://explorer.helium.wtf/validators)\n* [https://testnet-api.helium.wtf/v1/validators/](https://testnet-api.helium.wtf/v1/validators/%7B%7BADDRESS%7D%7D)","description":null,"slug":"docs/guides/helium-validator"},{"title":"Kava RPC Node Deployment","body":"\nKava RPC Nodes can be installed easily within an Akash deployment following the step by step instructions found in this guide.\n\n* [Akash Console Access and Initial Setup](#akash-console-access-and-initial-setup)\n* [Kava RPC Node Deployment](#kava-rpc-node-deployment)\n* [Kava RPC Node Health Check](#kava-rpc-node-health-check)\n\n\n## Akash Console Access and Initial Setup\n\n### Akash Console Access\n\nThe Akash Console can be accessed [here](https://console.akash.network/).\n\n### Akash Console Initial Setup\n\n#### Connect Wallet\n\n* The Akash Console currently supports Keplr wallets\n* If Keplr is not installed as a browser extension and/or a funded Akash account is not available, follow the instructions in our [Keplr Guide](/docs/docs/getting-started/token-and-wallets/#keplr-wallet)\n* Select the desired Akash account in Keplr and then select the `Connect Wallet` option within the Akash Console as shown below\n\n\n![](../../../assets/akashConsoleWallet.png)\n\n## Kava RPC Node Deployment\n\n### Create a Kava RPC Node as an Akash Deployment\n\n* Within the Akash Console template gallery, locate the Kava card and select the `Deploy Now` option\n\n![](../../../assets/akashConsoleDeployment.png)\n\n* Proceed with the deployment of the Kava Node by selecting the `Deploy Now` option\n\n\n![](../../../assets/akashConsoleProceedWithDeployment.png)\n\n\n* Assign the Deployment an appropriate name and then click `Review SDL`\n\n![](../../../assets/akashConsoleEditSDL.png)\n\n* The Kava RPC Node snapshot is updated every 24 hours and must be changed in the Akash SDL\n* Obtain the latest snapshot URL [here](https://polkachu.com/tendermint\\_snapshots/kava).  Find the `DOWNLOAD` hyperlink > right click > and Copy Link Address.\n* Replace the snapshot URL in the field highlighted in the depiction below with the new URL.  Ensure the `- SNAPSHOT_URL=` portion of the field is left in place followed by the actual URL such as:\n\n`- SNAPSHOT_URL=https://<CURRENT_SNAPSHOT_URL>`\n\n* Select `Save & Close` when this single Akash SDL update is in place.\n\n\n![](../../../assets/akashConsoleSnapshotUpdate.png)\n\n* Proceed by selecting `Create Deployment`&#x20;\n\n![](../../../assets/akashConsoleCreateDeployment.png)\n\n* The Akash Console will conduct necessary pre-deployment verifications to ensure that a wallet is connected with sufficient funds and that a certificate exists to communicate with the deployment\n* If all pre-deployment checks pass, select the `Next` option to proceed\n\n\n![](../../../assets/akashConsolePreflightCheck.png)\n\n* A Keplr wallet prompt will display requesting approval of a small blockchain fee to proceed with deployment creation\n* Select the `Approve` option to proceed\n\n\n![](../../../assets/akashConsoleDeploymentFees.png)\n\n* The Akash open marketplace displays available cloud providers to deploy your Kava RPC Node on\n* Select the cloud provider of your preference\n* Once the cloud provider is selected, select the `Submit Deploy Request` option\n\n> _**NOTE -**_ the cloud providers available for your deployment may be different than those shown in the example below\n\n\n![](../../../assets/akashConsoleSelectProvider.png)\n\n* Accept the Keplr prompt to approve small blockchain fee for lease creation with the selected cloud provider\n\n\n![](../../../assets/akashConsoleLeaseFees.png)\n\n## Kava RPC Node Deployment Complete\n\n* When the deployment of the Kava RPC Node is complete and live on the selected cloud provider, a verification screen will display\n* Proceed to the [Kava RPC Node Health Check](#kava-rpc-node-health-check) section to conduct a health/status check of the node\n\n\n![](../../../assets/akashConsoleLeaseStatus.png)\n\n\n## Kava RPC Node Health Check\n\n### Kava RPC Node Status Page\n\n* In the Akash Console a URL for the deployment is displayed\n* Click on the URL hyperlink\n\n\n![](../../../assets/akashConsoleDeploymentURI.png)\n\n* From the displayed web page, select the `status` link to view the current state of the Kava RPC Node\n\n\n![](../../../assets/akashConsoleNodeStatus.png)\n\n#### Expected Status When Node is in Sync\n\n* When the Kava RPC node is in sync the following, example status should be displayed when the status hyperlink is visited\n* Specifically look for `\"catching_up\":false` status indicating that the node is in full sync\n\n> _**NOTE**_ - following the snapshot download the RPC Node may take a couple of hours to catch up on blocks that were written between the time of the snapshot capture and the current state\n\n> _**NOTE**_ - the status output provided below is an example and the block height/other attributes will be different in your use\n\n```\n{\"jsonrpc\":\"2.0\",\"id\":-1,\"result\":{\"node_info\":{\"protocol_version\":{\"p2p\":\"8\",\"block\":\"11\",\"app\":\"0\"},\"id\":\"070d39ea8b993b887f817b3fe6dcfd49cdb4bdf4\",\"listen_addr\":\"tcp://0.0.0.0:26656\",\"network\":\"kava_2222-10\",\"version\":\"v0.34.24\",\"channels\":\"40202122233038606100\",\"moniker\":\"my-moniker-1\",\"other\":{\"tx_index\":\"on\",\"rpc_address\":\"tcp://0.0.0.0:26657\"}},\"sync_info\":{\"latest_block_hash\":\"E7069706908F8122C96D87CBBB116DE5AA47503FF468F145411B3871D77320E9\",\"latest_app_hash\":\"580AE91330C0ADA05FA759C5F8C9B57359275EC494C784C8C4018F921A39C856\",\"latest_block_height\":\"3974035\",\"latest_block_time\":\"2023-03-14T19:01:21.683269884Z\",\"earliest_block_hash\":\"17FD31C78361C31ABDA818174062E72D4094E799E90C82996194C6EAC89AAD35\",\"earliest_app_hash\":\"CCD5D5D23E985B5DDCE0446662EAF26DEBF26DD4EA322DA1789991C9B974B5B0\",\"earliest_block_height\":\"3967596\",\"earliest_block_time\":\"2023-03-14T07:28:43.971699061Z\",\"catching_up\":false},\"validator_info\":{\"address\":\"B45D70839692CE2F731906753A71B867C2B1E7D0\",\"pub_key\":{\"type\":\"tendermint/PubKeyEd25519\",\"value\":\"s0xP4O/qscJ7Ez2KTiNAANkHNAUToWEETwvh6Oq0oAw=\"},\"voting_power\":\"0\"}}}\n```\n\n","description":null,"slug":"docs/guides/kava-rpc-node-deployment"},{"title":"Mine Raptoreum on Akash Network","body":"\n\n![](../../../assets/raptoreumAkashlytics.png)\n\n## Why use Akash?\n\nWelcome [**Raptoreum**](https://raptoreum.com) \\*\\*\\*\\* miners! [**Akash**](https://akash.network) is a decentralized marketplace of compute with thousands of CPU's ready for small and large deployments. Raptoreum mining can be deployed on the network successfully using this guide. Akash is a part of the [**Cosmos**](https://cosmos.network) ecosystem of blockchains.\n\n## Windows/Linux/Mac Users\n\n1. Install [**Keplr**](https://chrome.google.com/webstore/detail/keplr/dmkamcknogkgcdfhhbddcghachkejeap?hl=en) \\*\\*\\*\\* wallet as a browser plugin\n2. Install [**Cloudmos Deploy**](https://cloudmos.io/cloud-deploy) \\*\\*\\*\\* and import your AKT wallet address from Keplr\n3. [**Fund your wallet**](https://github.com/akash-network/awesome-akash/blob/raptoreum/raptoreum-miner/README.md#Quickest-way-to-get-more-AKT)\n\nFor additional help we recommend you [**follow our full deployment guide**](https://docs.akash.network/guides/deploy) \\*\\*\\*\\* in parallel with this guide.\n\n## How does this work?\n\nAkash uses its blockchain to manage your container deployment and accounting. To deploy on Akash you will need to fund your wallet with at least 10 AKT. Each time you create a deployment, 5 AKT will be used for escrow and to fund the deployment. If the deployment is cancelled, the balance of the escrow is returned to you. You can spin up deployments without worrying about any long term contracts and you can cancel anytime.\n\n## Default wallet\n\nAkash uses [**Keplr**](https://chrome.google.com/webstore/detail/keplr/dmkamcknogkgcdfhhbddcghachkejeap?hl=en) as the desktop wallet. Advanced users can follow the \\*\\*\\*\\* [**CLI wallet instructions**](https://docs.akash.network/guides/cli).\n\n## Quickest way to get more AKT\n\nTo fund your deployment you will need AKT in your account. The fastest way to do that is in one of the following two ways.\n\n### Buy on an Exchange\n\n1. Install \\*\\*\\*\\* [**Keplr**](https://chrome.google.com/webstore/detail/keplr/dmkamcknogkgcdfhhbddcghachkejeap?hl=en)\n2. Buy AKT on an \\*\\*\\*\\* [**exchange**](https://www.coingecko.com/en/coins/akash-network#markets)\n3. Withdraw your AKT to your Keplr wallet\n\n### Swap from ATOM to AKT\n\n1. Install \\*\\*\\*\\* [**Keplr**](https://chrome.google.com/webstore/detail/keplr/dmkamcknogkgcdfhhbddcghachkejeap?hl=en)**.**\n2. Send 10 ATOM to your new Cosmos wallet address inside Keplr (this address will start with _cosmos_).\n3. Go to \\*\\*\\*\\* [**Osmosis Assets**](https://app.osmosis.zone/assets) > next to _Cosmos Hub - ATOM_ click on _Deposit_. This step will deposit ATOM from your Keplr wallet onto the Osmosis platform. Press _Connect Wallet_ to connect your Keplr wallet to Osmosis.\n4. Now go back to \\*\\*\\*\\* [**Osmosis Homepage**](https://app.osmosis.zone/?from=ATOM\\&to=AKT) \\*\\*\\*\\* and ensure _ATOM > AKT_ is selected to complete the swap. This step swaps your ATOM you deposited onto the Osmosis platform into any other supported coin.\n5. Return to the \\*\\*\\*\\* [**Osmosis Assets**](https://app.osmosis.zone/assets) page to withdraw your AKT to your Keplr wallet. This step withdraws AKT from the Osmosis platform back into your Keplr wallet. You can now send AKT to Cloudmos\n\nHave more questions? Find our team in \\*\\*\\*\\* [**Discord**](https://discord.com/invite/DxftX67) and [**Telegram**](https://t.me/AkashNW).\n\n## Deploying on Akash\n\nOnce you have set up your Keplr wallet and imported the address to Cloudmos you are ready to create your first deployment. Follow the instructions in Cloudmos to create a certificate, then click on _Create Deployment_.\n\nWhen prompted to _Choose Template_ select _Empty_ as we will copy-and-paste the deploy.yaml file from this repository (listed below). Choose _Empty_ for the template and paste the deploy.yaml file adjusting your wallet address and pool variables as desired.\n\n```\n---\nversion: \"2.0\"\n\nservices:\n  raptoreum:\n    image: cryptoandcoffee/cpu-akash-cpuminer-gr-avx2:2\n    expose:\n      - port: 4048\n        as: 80\n        proto: tcp\n        to:\n          - global: true\n    env:\n      - \"ADDRESS=RMB251ZucvCNyX1yoQqsSC2wwJ3s7fHx3b\"\n      - \"POOL=suprnova\" #You can enter custom pool here, otherwise suprnova nearest location will be used\n      - \"WORKER=akash\"\n      - \"TUNE=no-tune\"\n      - \"DONATION=0\"\nprofiles:\n  compute:\n    raptoreum:\n      resources:\n        cpu:\n          units: 1.0\n        memory:\n          size: 256Mi\n        storage:\n          size: 128Mi\n  placement:\n    akash:\n      pricing:\n        raptoreum:\n          denom: uakt\n          amount: 2\n\ndeployment:\n  raptoreum:\n    akash:\n      profile: raptoreum\n      count: 1\n```\n\n## Choosing a provider\n\nAkash is a marketplace of compute. Providers set their own prices for compute resources. We recommend you try different providers and check your logs after deployment to determine the hashrate.\n\n![](<../../../assets/chooseProvider (1).png>)\n\n## How to speed up mining?\n\n### Change the tuning option\n\n_TUNE=no-tune_ variable in deploy.yaml to _TUNE=full-tune_\n\nNo tune will start mining right away - with no performance tuning of the container. Without this expect a lower hashrate. Be warned, tuning can take at least 3 hours before mining begins - so do not expect to see hashrate on the pool immediately. You can check your logs in Cloudmos.\n\n### Increase the deployment size on Akash\n\nYou can deploy more CPUs to mine faster.\n\n```\ncpu:\n  units: 1.0 # Max cpu units is 10\n```\n\nOr increase the replica count from count: 1 to count: 2.\n\n```\ndeployment:\n  raptoreum:\n    akash:\n      profile: raptoreum\n      count: 1 # Multiplier for cpu:units\n```\n\n## Check your profitability\n\nAfter your deployment has finished tuning or is displaying results on the pool you can check your profitability by inputting your hashrate from the log file.\n\n[**Minerstat profitability calculator**](https://minerstat.com/coin/RTM)\n\n## What is the best pool? Where do I solo mine?\n\nWe recommend you check MiningPoolStats for the most up-to-date list of mining pools.\n\n[**Mining Pool Stats**](https://miningpoolstats.stream/raptoreum)","description":"How to Mine Raptoreum (RTM) on Akash Network","slug":"docs/guides/mine-raptoreum-on-akash"},{"title":"Multi-Tiered Deployment","body":"\n## Deploy a Multi-Tiered Application\n\nIn this guide, we will deploy a multi-tier web application on Akash. The example application will consist of two services: a front-end web service and a back-end database.\n\n### Before We Begin\n\nThis guide is to be considered an extension of the [Deploy an Application](https://github.com/akash-network/docs/blob/master/deploy/broken-reference/README.md) guide. Please ensure you have successfully completed all steps leading up to the \"Create the Deployment Configuration\" step in said guide, as they will not be discussed here.\n\n## Create the Deployment Configuration\n\nLet's create a deployment configuration that specifies multiple services in a single deployment.\n\n```bash\n---\nversion: \"2.0\"\n\nservices:\n  redis:\n    image: bitnami/redis:6.2\n    env:\n      - REDIS_AOF_ENABLED=no\n      - ALLOW_EMPTY_PASSWORD=yes\n    expose:\n      - port: 6379        \n        to:\n          - service: goosebin\n  goosebin:\n    image: hydrogen18/goosebin:latest\n    env:\n      - REDIS_HOST=redis\n      - REDIS_PORT=6379\n      - PASTE_SIZE_LIMIT=100000\n      - HTTP_PORT=8000\n    depends_on:\n      - redis\n    expose:\n      - port: 8000\n        as: 80\n        to:\n          - global: true        \n\nprofiles:\n  compute:\n    redis:\n      resources:\n        cpu:\n          units: 1\n        memory:\n          size: 128Mi\n        storage:\n          size: 16Mi\n    goosebin:\n      resources:\n        cpu:\n          units: 1\n        memory:\n          size: 64Mi\n        storage:\n          size: 16Mi\n  placement:\n    dc1:      \n      pricing:\n        redis: \n          denom: uakt\n          amount: 9999\n        goosebin: \n          denom: uakt\n          amount: 9999\n\ndeployment:\n  redis:\n    dc1:\n      profile: redis\n      count: 1\n  goosebin:\n    dc1:\n      profile: goosebin\n      count: 1\n```\n\nLet's break down the above SDL into its 3 primary categories: `services`, `profiles`, and `deployment`.\n\n#### Services\n\n```bash\nservices:\n  redis:\n    image: bitnami/redis:6.2\n    env:\n      - REDIS_AOF_ENABLED=no\n      - ALLOW_EMPTY_PASSWORD=yes\n    expose:\n      - port: 6379        \n        to:\n          - service: goosebin\n  goosebin:\n    image: hydrogen18/goosebin:latest\n    env:\n      - REDIS_HOST=redis\n      - REDIS_PORT=6379\n      - PASTE_SIZE_LIMIT=100000\n      - HTTP_PORT=8000\n    depends_on:\n      - redis\n    expose:\n      - port: 8000\n        as: 80\n        to:\n          - global: true\n```\n\nThe `services` entries contain maps of workloads to be run on the Akash deployment. This deployment has 2 service entries: `redis` and `goosebin` - the former being our backend and the latter being our frontend. Please note that while these service names are arbitrary, their usage must remain consistent across the whole .yml file.\n\nWe can see that `goosebin` is globally exposed and open to the public, while `redis` is internal to the deployment and is only shared with `goosebin`.\n\nWithin the `goosebin` entry, we set some environmental variables - these are declared internally within the `goosebin` image. Note that each application can choose whatever environment variables it wants to use on its own for configuration. The variable that ties our services together is `REDIS_HOST`. This is what `goosebin` looks at to determine which host to connect to for `redis`. In this case, we set `REDIS_HOST=redis` because we chose to name the service `redis` - had we named the service `database`, we would have set `REDIS_HOST=database`.\n\nThe `expose` section is similar to other HTTP examples - internally, the container for this example is listing on port 8000. The deployment will only be assigned a unique URI that can be visited in a web browser when `expose` has `global: true` and the port is set to 80.\n\n#### Profiles\n\n```bash\nprofiles:\n  compute:\n    redis:\n      resources:\n        cpu:\n          units: 1\n        memory:\n          size: 128Mi\n        storage:\n          size: 16Mi\n    goosebin:\n      resources:\n        cpu:\n          units: 1\n        memory:\n          size: 64Mi\n        storage:\n          size: 16Mi\n  placement:\n    dc1:      \n      pricing:\n        redis: \n          denom: uakt\n          amount: 100\n        goosebin: \n          denom: uakt\n          amount: 100\n```\n\nThe `profiles` entries contain named compute and placement profiles to be used in the deployment.\n\nSince we have 2 services to deploy, details for each of them must be specified here. This section is very similar to a standard deployment, so it won't be covered in detail here. An important item to note, however, is that the named compute/placement profiles here (`redis` and `goosebin`) must match the names we had specified in the `services` section. [Additional mappings](https://github.com/akash-network/docs/tree/a8e7a472b43ec742a03bc5063f6c5a82ca3ca2ea/sdl/README.md#profiles) can also be specified within `profiles` such as audited attributes and data center attributes.\n\n#### Deployments\n\n```bash\ndeployment:\n  redis:\n    dc1:\n      profile: redis\n      count: 1\n  goosebin:\n    dc1:\n      profile: goosebin\n      count: 1\n```\n\nThe `deployment` entries map the datacenter profiles to compute profiles to create a final desired configuration for the resources required for the services.\n\nSimilar to the `profiles` entries, we must specify deployment criteria for both of our services. This says that the 1 instance of the `redis` service and 1 instance of the `goosebin` service should be deployed to a datacenter matching the `dc1` datacenter profile. Each instance of the services will have the resources defined in its corresponding compute profile (`redis` or `goosebin`) available to it.\n\n#### Deployment\n\nNow that we have the SDL configured, let's deploy this application and see what happens. A more detailed guide on this process can be found in the [Deploy an Application](https://github.com/akash-network/docs/blob/master/deploy/broken-reference/README.md) guide.\n\n**Create the Deployment**\n\nCreate the deployment by running:\n\n```bash\nprovider-services tx deployment create goosebin.yml --from $AKASH_KEY_NAME --node $AKASH_NODE --chain-id $AKASH_CHAIN_ID --fees 5000uakt -y\n```\n\nOnce a provider is chosen, a lease is created, and the manifest is uploaded, we can view the status of our deployment by running:\n\n```bash\nprovider-services lease-status --node $AKASH_NODE --home ~/.akash --dseq $AKASH_DSEQ --from $AKSH_KEY_NAME --provider $AKASH_PROVIDER\n```\n\nYou should see a response similar to:\n\n```javascript\n{\n  \"services\": {\n    \"goosebin\": {\n      \"name\": \"goosebin\",\n      \"available\": 1,\n      \"total\": 1,\n      \"uris\": [\n        \"p3n56m0h7dant3iphkau8tdeds.ingress.sjc1p0.mainnet.akashian.io\"\n      ],\n      \"observed_generation\": 1,\n      \"replicas\": 1,\n      \"updated_replicas\": 1,\n      \"ready_replicas\": 1,\n      \"available_replicas\": 1\n    },\n    \"redis\": {\n      \"name\": \"redis\",\n      \"available\": 1,\n      \"total\": 1,\n      \"uris\": null,\n      \"observed_generation\": 1,\n      \"replicas\": 1,\n      \"updated_replicas\": 1,\n      \"ready_replicas\": 1,\n      \"available_replicas\": 1\n    }\n  },\n  \"forwarded_ports\": {}\n}\n```\n\nThe URI shown above will take you to the front-end service. We can verify the service is running and talking with the backend (`redis`) by running the following:\n\n```bash\nprovider-services lease-logs --node $AKASH_NODE --home $AKASH_HOME  --from $AKSH_KEY_NAME --dseq $AKASH_DSEQ  --provider $AKASH_PROVIDER\n```\n\nYou should see a response similar to:\n\n```bash\n[goosebin-5cf5678d5b-rfdqv] Starting HTTP server on \"0.0.0.0:8000\"\n[goosebin-5cf5678d5b-rfdqv] GET / rendered \"home\"\n[goosebin-5cf5678d5b-rfdqv] GET /create-paste rendered \"newPaste\"\n[goosebin-5cf5678d5b-rfdqv] POST /create-paste rendered \"createPaste\"\n[goosebin-5cf5678d5b-rfdqv] GET /paste/HD-i5mNSyiQn0Kop8y8IvQ rendered \"showPaste\"\n[goosebin-5cf5678d5b-rfdqv] GET /create-paste rendered \"newPaste\"\n[goosebin-5cf5678d5b-rfdqv] GET / rendered \"home\"\n[goosebin-5cf5678d5b-rfdqv] GET / rendered \"home\"\n[goosebin-5cf5678d5b-rfdqv] POST /create-paste rendered \"createPaste\"\n[redis-8569665588-zh6pr] redis 19:38:14.11 \n[redis-8569665588-zh6pr] redis 19:38:14.17 Welcome to the Bitnami redis container\n[redis-8569665588-zh6pr] redis 19:38:14.18 Subscribe to project updates by watching https://github.com/bitnami/bitnami-docker-redis\n[redis-8569665588-zh6pr] redis 19:38:14.19 Submit issues and feature requests at https://github.com/bitnami/bitnami-docker-redis/issues\n[redis-8569665588-zh6pr] redis 19:38:14.20 \n[redis-8569665588-zh6pr] redis 19:38:14.21 INFO  ==> ** Starting Redis setup **\n[redis-8569665588-zh6pr] redis 19:38:14.27 WARN  ==> You set the environment variable ALLOW_EMPTY_PASSWORD=yes. For safety reasons, do not use this flag in a production environment.\n[redis-8569665588-zh6pr] redis 19:38:14.29 INFO  ==> Initializing Redis\n[redis-8569665588-zh6pr] redis 19:38:14.42 INFO  ==> Setting Redis config file\n[redis-8569665588-zh6pr] redis 19:38:14.59 INFO  ==> ** Redis setup finished! **\n[redis-8569665588-zh6pr] \n[redis-8569665588-zh6pr] redis 19:38:14.62 INFO  ==> ** Starting Redis **\n[redis-8569665588-zh6pr] 1:C 06 Apr 2021 19:38:14.689 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo\n[redis-8569665588-zh6pr] 1:C 06 Apr 2021 19:38:14.689 # Redis version=6.2.1, bits=64, commit=00000000, modified=0, pid=1, just started\n[redis-8569665588-zh6pr] 1:C 06 Apr 2021 19:38:14.689 # Configuration loaded\n[redis-8569665588-zh6pr] 1:M 06 Apr 2021 19:38:14.690 * monotonic clock: POSIX clock_gettime\n[redis-8569665588-zh6pr] 1:M 06 Apr 2021 19:38:14.691 * Running mode=standalone, port=6379.\n```\n\nWe can see the `goosebin` frontend has made some requests to the `redis` backend, and the containers are healthy.","description":null,"slug":"docs/guides/multi-tiered-deployments"},{"title":"Polygon on Akash","body":"\nPolygon is both a protocol and a framework for building and connecting Ethereum-compatible blockchain networks. Aggregating scalable solutions on Ethereum and supporting a multi-chain Ethereum ecosystem.\n\nAkash is the world’s first open source decentralized cloud built on a censorship-resistant, permissionless, and self-sovereign network.\n\nIn this guide we will cover how to use these two technologies together when building our decentralized apps.\n\n## Getting Started Guides\n\nThese guides include architecture and technical documentation and a curated list of awesome apps for both Akash and Polygon.\n\nThroughout this guide we have included resources we believe most valuable to your journey.\n\n* _\\*\\*\\*\\*_[_**Akash Starter Kit**_](https://akashnet.notion.site/akashnet/Polygon-Akash-Starter-Kit-d4e817023556417ea8c9b679336d0d76)_\\*\\*\\*\\*_\n* _\\*\\*\\*\\*_[_**Polygon Starter Kit**_](https://polygontechnology.notion.site/Polygon-Starter-Kit-a289a505a0bb4e8b8189c1fc3b2223d0)_\\*\\*\\*\\*_\n\n## Polygon and Akash Architectures\n\nIf you want a deeper understanding of Polygon and Akash, see these architecture guides\n\n* _\\*\\*\\*\\*_[_**Polygon Architecture and Documentation**_](https://docs.polygon.technology)_\\*\\*\\*\\*_\n* [_**Akash Architecture and Documentation**_](https://docs.akash.network)_\\*\\*\\*\\*_\n\n## Akash Application Tools\n\nThis [_**guide**_](https://docs.akash.network/guides/deploy) _\\*\\*\\*\\*_ provides step by step instructions on how to deploy an app on Akash using a desktop tool named Cloudmos Deploy.\n\n## Technical Support Channels\n\nIn depth support for Polygon and Akash is available via the following channels.\n\n### Polygon on Akash\n\n* _\\*\\*\\*\\*_[_**Discord**_](https://discord.com/invite/xpUtZcWtyp)_\\*\\*\\*\\*_\n\n### Polygon General\n\n* _\\*\\*\\*\\*_[_**Discord**_](https://discord.com/invite/polygon)_\\*\\*\\*\\*_\n* _\\*\\*\\*\\*_[_**Telegram**_](https://t.me/joinchat/UMpbSrjAY\\_Ffx5CD)_\\*\\*\\*\\*_\n\n### Akash General\n\n* _\\*\\*\\*\\*_[_**Discord**_](https://discord.com/invite/akash)_\\*\\*\\*\\*_\n* _\\*\\*\\*\\*_[_**Support Forum**_](https://forum.akash.network)_\\*\\*\\*\\*_\n\n### **Faucets**\n\nThe following faucets let users collect a small amount of AKT (Akash) or MATIC (Polygon) currency for development purposes.\n\n* _\\*\\*\\*\\*_[_**Akash Developer Faucet**_](https://drip.akash.network/login)_\\*\\*\\*\\*_\n* _\\*\\*\\*\\*_[_**Polygon Developer Faucet**_](https://faucet.polygon.technology)_\\*\\*\\*\\*_","description":null,"slug":"docs/guides/polygon-on-akash"},{"title":"PostgreSQL restore/backup","body":"\n**Repository**: [ovrclk/akash-postgres-restore](https://github.com/ovrclk/akash-postgres-restore)\n\nAn auto-restoring Postgres server running on Akash, with backups taken on a configurable schedule. Backups are stored on decentralised storage using Filebase.\n\nUltimately this is a two container setup, one PostgreSQL server and one scheduler container to restore the database on boot, and run a cronjob to back it up.\n\n## Usage\n\n* Setup a [Filebase](https://filebase.com/) account and bucket (or any S3 compatible storage host).\n* Set the environment variables in the [deploy.yml](https://github.com/ovrclk/akash-postgres-restore/blob/master/deploy.yml) and deploy on Akash\n* Use the URL and port Akash gives you to connect to the Postgres server, with the credentials you provided in the environment variables. For example cluster.ewr1p0.mainnet.akashian.io:31234\n\n### Using with an app container\n\nAlternatively add your own app container to the deploy.yml and expose the Postgres 5432 port to your application only for a local server.\n\nFor example:\n\n```\nservices:\n  app: \n    image: myappimage:v1\n    depends_on: \n      - service: postgres\n  cron:\n    image: ghcr.io/ovrclk/akash-postgres-restore:v0.0.4\n    env:\n      - POSTGRES_PASSWORD=password\n      ...\n    depends_on:\n      - service: postgres\n  postgres:\n    image: postgres:12.6\n    env:\n      - POSTGRES_PASSWORD=password\n    expose:\n      - port: 5432\n        to:\n          - service: app\n          - service: cron\n```\n\n### Environment variables\n\n* `POSTGRES_USER=postgres` - your Postgres server username\n* `POSTGRES_PASSWORD=password` - your Postgres server password\n* `POSTGRES_HOST=postgres` - postgres server host, whatever you named it in deploy.yml\n* `POSTGRES_PORT=5432` - postgres port, will be 5432 unless you aliased it in deploy.yml\n* `POSTGRES_DATABASE=akash_postgres` - name of your database\n* `BACKUP_PATH=bucketname/path` - bucket and path for your deployments. Make sure directories exist first\n* `BACKUP_KEY=key` - your Filebase access key\n* `BACKUP_SECRET=secret` - your Filebase secret\n* `BACKUP_PASSPHRASE=secret` - a passphrase to encrypt your backups with\n* `BACKUP_HOST=https://s3.filebase.com` - the S3 backup host, this defaults to Filebase but can be any S3 compatible host\n* `BACKUP_SCHEDULE=*/15 * * * *` - the cron schedule for backups. Defaults to every 15 minutes\n* `BACKUP_RETAIN=7 days` - how many days to keep backups for\n\n## Development\n\nYou can run the application locally using Docker compose.\n\nCopy the `.env.sample` file to `.env` and populate\n\nRun `docker-compose up` to build and run the application","description":null,"slug":"docs/guides/postgres-sql-restore-or-backup"},{"title":"Ruby on Rails with Sia and Auth0","body":"\n\n\n**Repository**: [ovrclk/akash-on-rails](https://github.com/ovrclk/akash-on-rails)\\\n**Demo**: [pin.akash.host](https://pin.akash.host/)\n\nThis is an example Rails Pinterest clone hosted on Akash. There are a few extra features to make the most of decentralised hosting:\n\n* Database backup/restore to [Sia](https://sia.tech/) via [Filebase](https://filebase.com/).\n* User image uploads to [Sia](https://sia.tech/) via [Filebase](https://filebase.com/).\n* [Auth0](https://auth0.com/) user authentication.\n* [Cloudflare](https://www.cloudflare.com/) DNS and SSL.\n* Scheduled tasks using [Whenever](https://github.com/javan/whenever).\n\n## Architecture\n\n### App container\n\n* Runs the rails server and hosts the actual website.\n* Connects to the Postgres container for a persistent database.\n* Hosts files on [Filebase](https://filebase.com/) ([Sia](https://sia.tech/), [Skynet](https://siasky.net/), and [Storj](https://www.storj.io/) hosting currently).\n* Uses [Auth0](https://auth0.com/) for user login and registration.\n\n### Cron container\n\n* Auto-restores the Postgres database on boot, achieving persistent database through re-deploys.\n* Auto-backup of the database to [Filebase](https://filebase.com/) every 15 minutes.\n* Crontab is defined using [Whenever](https://github.com/javan/whenever) in [`schedule.rb`](https://github.com/ovrclk/akash-on-rails/blob/master/config/schedule.rb).\n* Runs the same docker image as the rails application, but running `cron` instead of the rails server.\n* A [standalone database backup/restore container](https://github.com/ovrclk/akash-postgres-restore) is also available.\n\n### Postgres container\n\n* Runs a standard Postgres server docker image.\n\n## Usage\n\nUltimately this repository is designed to provide a sensible example of hosting a rails application on Akash. There are a few ways to use it:\n\n### Run the application as-is on Akash with your own storage and [Auth0](https://auth0.com/) account\n\n* Setup a free [Cloudflare](https://www.cloudflare.com/) account and add your domain and set nameservers.\n* Setup a [Filebase](https://filebase.com/) account and bucket.\n  * Add a `backups` folder to your bucket.\n  * You will need your bucket name, client ID, and secret.\n* Sign up for an [Auth0](https://auth0.com/) account and set up an App.\n  * Callback URL: [https://yourdomain.com/auth/auth0/callback](https://yourdomain.com/auth/auth0/callback).\n  * Logout URL: [https://yourdomain.com](https://yourdomain.com/).\n  * You will need your [Auth0](https://auth0.com/) domain, client ID, and secret.\n* Using the example deploy.yml, populate the environment variables with the values from [Filebase](https://filebase.com/) and [Auth0](https://auth0.com/).\n* Deploy on Akash and get your app URL.\n* Point your domain to your app URL using a CNAME in [Cloudflare](https://www.cloudflare.com/).\n* Configure 'Full' SSL mode in [Cloudflare](https://www.cloudflare.com/).\n* Sign in to your website using [Auth0](https://auth0.com/). The first user created will be made an administrator.\n\n### Use the relevant files in your own project\n\n* [Dockerfile](https://github.com/ovrclk/akash-on-rails/blob/master/Dockerfile)\n  * Rails ready Dockerfile.\n  * Installs the AWS CLI tool to interact with [Filebase](https://filebase.com/).\n* [scripts/run-app.sh](https://github.com/ovrclk/akash-on-rails/blob/master/scripts/run-app.sh)\n  * Precompiles rails assets.\n  * Runs the rails server.\n* [scripts/run-scheduler.sh](https://github.com/ovrclk/akash-on-rails/blob/master/scripts/run-scheduler.sh)\n  * Creates and restores the database.\n  * Runs rake db:migrate and db:seed.\n  * Sets the crontab using [Whenever](https://github.com/javan/whenever) and runs the cron service.\n* [scripts/restore-postgres.sh](https://github.com/ovrclk/akash-on-rails/blob/master/scripts/restore-postgres.sh)\n  * Downloads latest backup from [Filebase](https://filebase.com/).\n  * Restore the DB if a backup was found.\n* [scripts/backup-postgres.sh](https://github.com/ovrclk/akash-on-rails/blob/master/scripts/backup-postgres.sh)\n  * Backs up the database to [Filebase](https://filebase.com/).\n  * Deletes backups older than KEEP\\_BACKUPS.\n* [config/schedule.rb](https://github.com/ovrclk/akash-on-rails/blob/master/config/schedule.rb)\n  * [Whenever](https://github.com/javan/whenever) cron schedule file to run scripts/backup-postgres.sh every 15 minutes.\n* [config/initializers/shrine.rb](https://github.com/ovrclk/akash-on-rails/blob/master/config/initializers/shrine.rb)\n  * Configures Shrine within the application to use [Filebase](https://filebase.com/) as an S3 host.\n* [deploy.yml](https://github.com/ovrclk/akash-on-rails/blob/master/deploy.yml)\n  * Akash deploy manifest.\n\n### Clone the repository and use it as a base for a new project\n\n* Clone the repository to your own Github account.\n* Rename any occurrence of AkashOnRails, akash-on-rails and, akash\\_on\\_rails to your own app name.\n* Change any app/models, app/controllers, app/views as required.\n\n## Development\n\nYou can run the application locally using Docker compose.\n\nCopy the `.env.sample` file to `.env` and populate.\n\nRun `docker-compose up` to build and run the application.","description":null,"slug":"docs/guides/ruby-on-rails-with-sia-and-auth0"},{"title":"TLS Termination of Akash Deployments","body":"\nCurrently only self-signed certificates are available from Akash Providers.\n\nIn this guide we detail a Cloudflare TLS termination strategy which will give Akash deployments valid public certificates.  Cloudflare will proxy traffic intended for the Akash deployment and allow end to end encrypted communication.\n\n**STEP 1** - [Prerequisites](#prerequisites)\n\n**STEP 2** - [Akash with TLS Example](#akash-with-tls-example)\n\n**STEP 3** - [Find IP Address of Deployment](#find-ip-address-of-deployment)\n\n**STEP 4** -[ Cloudflare Configuration](#cloudflare-configuration)\n\n**STEP 5** - [Verify HTTPS](#verify-https)\n\n**STEP 6** - [Troubleshooting](#troubleshooting)\n\n\n## Prerequisites\n\nCloudflare proxy of traffic is only possible if one of the following is in place:\n\n* Register your domain in Cloudflare\n* &#x20;Transfer control of domain from current DNS provider to Cloudflare’s control&#x20;\n* &#x20;Point domain records from current nameservers of the DNS provider to Cloudflare nameservers instead\n\n\n## Akash with TLS Example\n\nAn example Akash deployment with TLS step by step example.  We will use Ghost, a very simple web app, for the demo.\n\n### **Deploy Ghost**\n\nMake sure to specify the hostname you control, in this example it is “ghost.akash.pro”. Set the “https” when setting the “url” environment variable – this will get picked by the Ghost blogging platform so it knows to serve these HTTPS links.\n\nWhen you deploy with 80/tcp port exposed in Akash, the nginx-ingress-controller on the provider will automatically get 443/tcp exposed too. This makes Full TLS termination possible.\n\nIf you are not familiar with Akash deployments, visit the documentation for the desktop app [Cloudmos Deploy](https://docs.akash.network/guides/deploy) as an easy way to get started.\n\n```\n---\nversion: \"2.0\"\n\nservices:\n  ghost:\n    image: ghost:4.36.3-alpine\n    env:\n      - 'url=https://ghost.akash.pro'\n    expose:\n      - port: 2368\n        as: 80\n        accept:\n          - \"ghost.akash.pro\"\n        to:\n          - global: true\n\nprofiles:\n  compute:\n    ghost:\n      resources:\n        cpu:\n          units: 1.0\n        memory:\n          size: 512Mi\n        storage:\n          size: 512Mi\n  placement:\n    akash:\n      signedBy:\n        anyOf:\n          - \"akash1365yvmc4s7awdyj3n2sav7xfx76adc6dnmlx63\"\n      pricing:\n        ghost:\n          denom: uakt\n          amount: 100\n\ndeployment:\n  ghost:\n    akash:\n      profile: ghost\n      count: 1\n```\n\n\n## Find IP Address of Deployment\n\nWe will need to lookup the IP address of the deployment to later use in the Cloudflare configuration\n\n### Domain of the Deployment\n\n* Find the domain name of the deployment in the Cloudmos Deployment Detail page\n\n![](https://files.gitbook.com/v0/b/gitbook-x-prod.appspot.com/o/spaces%2F-LrNFlfuifzmQ\\_NMKu9C-887967055%2Fuploads%2F1OLAZX7ITvAbCClUClxb%2FcloudflareURL.png?alt=media\\&token=c3a3e6f0-5e71-49dc-8688-afe8a58d57a8)\n\n### IP Address of Deployment\n\n* From your terminal ping the domain name of the deployment and the IP address will be revealed\n* In the example shown the IP address is listed as `147.75.75.107`\n\n```\nscarruthers@Scotts-MacBook-Pro ghost % ping t2ns2f7105b7t38aukju1calp4.ingress.provider-2.prod.ewr1.akash.pub\n\nPING t2ns2f7105b7t38aukju1calp4.ingress.provider-2.prod.ewr1.akash.pub (147.75.75.107): 56 data bytes\n\n64 bytes from 147.75.75.107: icmp_seq=0 ttl=53 time=47.975 ms\n64 bytes from 147.75.75.107: icmp_seq=1 ttl=53 time=49.651 ms\n64 bytes from 147.75.75.107: icmp_seq=2 ttl=53 time=51.948 ms\n```\n\n## Cloudflare Configuration\n\n\n\n## Point Domain Name to Deployment IP\n\nThese configs are necessary in Cloudflare:\n\n* Point the domain name of the deployment to the IP address of the deployment captured in the last step\n* Set Proxy Status to `Proxied`\n\n![](https://files.gitbook.com/v0/b/gitbook-x-prod.appspot.com/o/spaces%2F-LrNFlfuifzmQ\\_NMKu9C-887967055%2Fuploads%2FPkRELRx4bWZqN65xAtdo%2FcloudflareDNS.png?alt=media\\&token=f0dd85fd-72f1-4247-baaa-43391005dc4b)\n\n### Cloudflare Recommendation\n\nCloudflare recommends orange-clouding the record so that any dig query against that record returns a Cloudflare IP address and your origin server IP address remains concealed from the public.&#x20;\n\nDNS proxied means it will be shown a Cloudflare IP if you look it up. Thus all attacks at that domain will DDoS Cloudflare and not you host directly. Non proxied means all traffic goes directly to your own IP without Cloudflare being a safety net in front.\n\n### **Ensure SSL/TLS Encryption Set to Full**\n\nIn most situations we want Full TLS mode specified in Cloudflare based on:\n\n* **Full TLS termination mode:** if the backend understands it is behind the Full TLS termination balancer (be that Cloudflare or anything else), it should then keep serving HTTPS\n* **Flexible TLS termination mode:** this is when the backend is only serving HTTP requests and does not understand that something is terminating the TLS in front, then it will only work if it is NOT trying to serve full scheme URI's (i.e. uri's containing \"http://\" in them). Otherwise you get a mixed content error (see the comment on Mixed Content error is below).\n\n> _**NOTE**_ - we advise using a **Full TLS termination mode** since Akash deployments exposed over the port 80/tcp HTTP (as: 80 in SDL manifest) are also automatically getting exposed over the 443/tcp HTTPS (TLS), though with the default self-signed certificates. This mode will ensure the traffic between Cloudflare and the Akash deployment gets encrypted.\n\n![](https://files.gitbook.com/v0/b/gitbook-x-prod.appspot.com/o/spaces%2F-LrNFlfuifzmQ\\_NMKu9C-887967055%2Fuploads%2FnbC4Bqsj8Eo4nTCMNgp1%2FcloudflareTLS.png?alt=media\\&token=af45f034-a99b-4125-af5e-7e8c9ecd357c)\n\n\n\n## Verify HTTPS\n\n* Test end to end HTTPS for your deployment\n\n![](https://files.gitbook.com/v0/b/gitbook-x-prod.appspot.com/o/spaces%2F-LrNFlfuifzmQ\\_NMKu9C-887967055%2Fuploads%2FWsUrM07CQNXwhQgUE2QT%2FcloudflareHttpsTest.png?alt=media\\&token=33350171-58a6-4731-8d73-4df77322f6c4)\n\n\n## Troubleshooting\n\n* In this section we review a couple of common problems encountered\n\n### **Mixed Content Errors**\n\nIn the example below you’ve noticed the “https” was set - “https://ghost.akash.pro”**.** That is to make sure the backend (Ghost) is serving the HTTPD links only, since our goal was to enable HTTPS via TLS termination over Cloudflare.\n\nIn situations when the backend server is not HTTPS aware, you will see the content would not load due to a mixed content (this is when the server is offering non-HTTPS links while being accessed over HTTPS).\n\nHere is an example of that, the deployment is set to “http://ghost.akash.pro” while “https://ghost.akash.pro” is opened in the browser.\n\n![](https://files.gitbook.com/v0/b/gitbook-x-prod.appspot.com/o/spaces%2F-LrNFlfuifzmQ\\_NMKu9C-887967055%2Fuploads%2F3zaPRHd62wRj1oQrewrN%2FcloudflareMixedMedia.png?alt=media\\&token=30738db8-7611-4480-9c71-076c6a839d32)","description":null,"slug":"docs/guides/tls-termination-of-akash-deployment"},{"title":"Unstoppable Web 2.0","body":"\nRepository repository is located [here](#decentralized-infrastructure).\n\nThis repository serves as a general guide and proof of concept for deploying a full-stack web application onto blockchain-based decentralized infrastructure.\n\nThe repository is structured as a monorepo - with infrastructure configuration, application frontend code, and application backend code all in one repository. This is done so that anyone can clone or fork this one repository and begin to experiment with deploying a decentralized web application.\n\n### Decentralized infrastructure\n\n| DNS                                | Frontend                     | Backend                        | <p>Redundant</p><p>Database</p>     |\n| ---------------------------------- | ---------------------------- | ------------------------------ | ----------------------------------- |\n|                                    |                              |                                |                                     |\n| [Handshake](https://handshake.org) | [Skynet](https://siasky.net) | [Akash](https://akash.network) | [Postgresql](http://postgresql.org) |\n\n### Web application\n\nThe demo application deployed in this guide is a note app - with create, read, delete functionality. It serves as a minimal proof of concept for deploying a multi-tier web application to decentralized infrastructure.\n\n| Frontend                     | Backend                                        | Database                                 |\n| ---------------------------- | ---------------------------------------------- | ---------------------------------------- |\n| [React](https://reactjs.org) | [Python FastAPI](https://fastapi.tiangolo.com) | [PostgreSQL](https://www.postgresql.org) |\n\nThe application structure is bootstrapped using this [cookiecutter](https://github.com/Buuntu/fastapi-react) template. The **docker** image is based on this [image](https://github.com/tiangolo/uvicorn-gunicorn-fastapi-docker).\n\nThe application code, technology choices, and configuration used in this repository should provide a general enough base to fork and adapt, or just reference, for future decentralized web application development.\n\n### Deployed Demo\n\nA deployed [live demo](https://0000ac8v9uf92otn28omnuqfan0qd05f3gjrsfoonjpjl3m3ir7qds8.siasky.net/#) was deployed using the steps in this guide.\n\n## Guide\n\n### Step 1 - Buy Handshake domain\n\nThere are a few simple options for registering a **Handshake** domain name:\n\n1.  Use a platform like [Namebase](https://www.namebase.io) or a tool like [Bob Wallet](https://github.com/kyokan/bob-wallet) to purchase a top-level Handshake domain.\n\n    Domain purchases using Handshake are achieved through an [auction process](https://www.namebase.io/blog/tutorial-3-basics-of-handshake-auction-and-bidding). It will take **10 days** for your auction to finalize. and a winning bid, before you can make use of the domain.\n2. Use [**gateway.io**](https://gateway.io) to purchase a domain under one of gateway's existing top-level Handshake domains, like **.c** or **.api**. Domains purchased in this way are **usable immediately**.\n\n### Step 2 - Push Docker image\n\n1. Make sure [Docker](https://www.docker.com) is installed.\n2. **CD** to the directory where your `Dockerfile` is. For this repository, it is in `application/backend/Dockerfile`.\n3. **Build** and **tag** docker image `docker build -t/: .`\n4. **Login** to dockerhub `docker login --username=`. You will be prompted for a password.\n5. **Push** your docker image to dockerhub `docker push`. Paste this yaml into the tool.\n\n### Step 3 - Deploy back-end to Akash\n\nIf you haven't already, take the time to read through the [Akash deployment documentation](https://docs.akash.network/guides/deploy) and familiarize yourself with the concepts and steps involved.\n\nYou can deploy to Akash using the [standard Akash CLI ](https://docs.akash.network/guides/cli)directly but for the purpose of this guide, I am using [Tom Beynon's Akash Deploy UI](https://github.com/tombeynon/akash-deploy) which is a great tool built on top of the standard CLI. Steps for deploying to Akash using this tool are below.\n\n1.  Start the **Akash Deploy UI** tool by running the following:\n\n    ```\n    docker run -v ~/.akash-ui:/root/akash -p 3000:3000 --rm -it tombeynon/akash-deploy\n    ```\n2. Visit [http://localhost:3000](https://github.com/coffeeroaster/unstoppable-web2.0/blob/main) to access the tool\n3. Use the tool to **create a new wallet**. Make sure to record your mnemonic phrase so that you can restore the wallet if needed in the future.\n4. **Fund this new wallet with at least 5 AKT** (5,000,000 UAKT) by transferring 5 AKT or more to the displayed wallet address.\n5. **Create a new certificate**. This certificate only needs to be created once and will be used for all future deployments.\n6. **Create a new deployment** using `akash/deploy-sample.yml` as a template. The Akash Stack Definition Language (SDL)\n7. Once you have created your deployment, you will start to receive bids. You can view these in the tool.\n8. Chose a bid and **Create a lease**, then **Send manifest**.\n9. At this point, the tool will show you your **Web URIs** which you can use to access your application.\n10. Update [application/frontend/.env.production](https://github.com/coffeeroaster/unstoppable-web2.0/blob/main/application/frontend/.env.production) with the **Web URIs**.\n\n### Step 4 - Deploy front-end to Skynet\n\nDeployment of the front-end to Skynet is **handled automatically** in this repository, using **GitHub Actions**. When any update to the `application/frontend/` directory is pushed to the _master_ branch, the Action workflow will be run automatically by GitHub. This [workflow](https://github.com/bcfus/unstoppable-stack/blob/master/.github/workflows/frontend.yml) tests, builds, and deploys the static files to **Skynet**.\n\nThis GitHub workflow is based on the excellent [write-up by Karol Wypchło ](https://blog.sia.tech/automated-deployments-on-skynet-28d2f32f6ca1)and uses his pre-built Skynet deploy Action.\n\nThe result of this GitHub Action is a **Skynet registry entry** that provides a constant point of reference for your DNS record. The **Skylink** itself changes with each deployment to Skynet and would require you to constantly update your DNS record if referencing it directly.\n\n### Step 5 - Configure Handshake domain\n\n**Once your GitHub action completes**, in the Action log in GitHub under the _Deploy to Skynet_ step, you will see a link to `https://siasky.net/skynet/registry` with parameters in the URL for `publickey` and `datakey`.\n\nYou now need to copy those param values and create your `skyns://` URL in the format `skyns:///`. The registry link will remain constant in the future, so this is a one-time manual step.\n\nIf using **Namebase**, you can now configure your Handshake domain to point to your Skynet hosted frontend by adding a TXT record under the _Blockchain DNS Records_ section on your domain manage page. The value of this record is the `skyns:///` URL that you just generated above.\n\nThis **initial DNS configuration can take up to \\~6 hours to be synced**. For all future code changes pushed to the _master_ branch, the _Deploy to Skynet_ Action will automatically update your Skynet Registry data and you should see the changes reflected very quickly.\n\n**Once the DNS change takes effect**, your site will be accessible at `https://.hns.siasky.net`. For example, the demo application for this guide is accessible [here](https://unstoppable-stack.hns.siasky.net).\n\n### Step 6 - Setup HTTPS URL for API\n\nAlthough you can now make requests directly to your Akash URL over HTTP successfully, if you attempt to make these requests from your frontend that is being served over HTTPS, your API call will be blocked due to **Mixed Content**.\n\nIt is likely possible to enabled an HTTPS connection just using Akash deployment configuration but I was unable to find any documentation on this functionality.\n\nThe approach used in this guide is to sign up a free [Cloudflare](https://cloudflare.com) account and set up their **flexible** SSL/TLS encryption mode on a secondary (non-Handshake) domain in order to provide an HTTPS entrypoint that proxies requests to the Akash API server.\n\nSupport for HTTPS with Akash is something that I expect to explore further.\n\n### Step 7 - Setup Database Redundancy\n\n### Backend Redundant Postgresql infrastructure\n\nThe backend runs on [Akash](https://akash.network). Take a moment to review the Akash [config](https://github.com/coffeeroaster/unstoppable-web2.0/blob/main/akash/deploy-sample.yml). The backend stores the database on Akash. Currently, Akash does not have a persistent storage solution. This solution provides the capability to perform regular backups and database restoration via [Skynet](https://siasky.net). This reference architecture provides the following capability:\n\n* Provide a redundant Postgresql configuration. **pg-0** is replicated to **pg-1**\n* `pg-0` - the postgres master starts up. When it first starts up, it will download an encrypted db backup from Skynet.\n* `pg-0` - decrypts the backup and loads into the database.\n* `pg-1` - the postgres standby starts up. and syncs with `pg-0`.\n* `pgpool` container starts up and connects to both `pg-0` and `pg-1` and automatically determines which one is master.\n* `web` container starts up and connects the the **db cluster** via pgpool.\n* If `pg-1` or `pg-0` goes down, **pgpool** will automatically re-route to the one that is active.\n* If the entire cluster goes down, the cluster can be re-deployed to Akash. It will then start up from the last backup.\n\n#### Implementation Notes\n\n* The postgres Docker image will execute any .sh, load and .sql, .sql.gz file in /docker-entrypoint-initdb.d\n* Note that when testing this locally with Docker there are no volumes. This is by design. Everything persistent should be uploaded to the Skynet.\n* The `pg-0` image initially loads [pg-0/dbout.sh](https://github.com/coffeeroaster/unstoppable-web2.0/blob/main/pg-0/dbout.sh) before the PG cluster is started. It reads in the following env vars: `BACKUP_SYKNET_URL` and `BACKUP_PASS`. The encrypted zip file is retrieved from Skynet and decrypts it with `BACKUP_PASS`. Finally, the DB backup is loaded into the PG cluster.\n* `BACKUP_SKYNET_URL` is used to retrieve the latest backup.\n\n#### Architecture\n\nBased off the [Bitnami pgpool project](https://github.com/bitnami/bitnami-docker-pgpool/), this approach sets up three containers. **pg-0** as a postgresql master, **pg-1** as a postgresql secondary ( running as a hot standby), and **pg-pool** will determine which postgres container to connect to.\n\n#### Testing Postgresql cluster in a test environment\n\n```\n# run the backend\n$docker-compose up -d\n# run the front end\n$ cd application/frontend/ && npm install && npm start\n# Connect to the browser at http://localhost:3000\n```\n\n#### Shut down **pg-0** container\n\n```\n$ docker-compose stop pg-0\n## verify that things are working as planned\n# Connect to the browser at http://localhost:3000\n# Should see all data still there. Try adding values\n```\n\n### Step 8 - Setup database backups\n\n#### Backup your data locally\n\nTo retrieve a live encrypted backup of the data, you can run the following command against the API:\n\n```\n# For testing locally with docker-compose\ncurl -v http://localhost/getsnapshot > dbout.zip\n# For testing against AKASH deployment\n# Obtain $AkashHost and $AkashPort from akash deployment (Consult Akash [documentation](https://docs.akash.network/guides/deploy) on how to obtain this)\ncurl -v http://$AkashHost/getsnapshot:$AkashPort > dbout.zip\n#\n# Backup against live demo\n#\ncurl https://unstoppablestack.coffeeroaster.me/getsnapshot > dbout.zip\n```\n\nMake sure to update the **`$BACKUP_PASS`** env variable in docker-compose.yml, akash/deploy-sample.yml. You will use the password to decrypt the zip file. You can use **`$BACKUP_PASS`** env var in [akash/deploy-sample.yml](https://github.com/coffeeroaster/unstoppable-web2.0/blob/main/akash/deploy-sample.yml) to decrypt and examine the backup.\n\n#### Upload local backup to Skynet\n\nUpload the backup to Skynet with [skynet-cli](https://github.com/vbstreetz/skynet-cli) and store URL as **BACKUP\\_SKYNET\\_URL**\n\n```\n$ npm install -g skynet-cli\n## send it to Skynet!\n$ skynet-cli dbout.zip\n## Take special note of the URL. This value will be used as **BACKUP_SKYNET_URL**\n```\n\n#### Tell Unstoppable Stack to load it up next time.\n\nUpdate [docker-compose.yml](https://github.com/coffeeroaster/unstoppable-web2.0/blob/main/docker-compose.yml) (for testing locally) and [akash/deploy-sample.yml](https://github.com/coffeeroaster/unstoppable-web2.0/blob/main/akash/deploy-sample.yml) environment variables section and update **BACKUP\\_SKYNET\\_URL** with the value from the previous step.\n\n**Shutdown and destroy your entire deployment**\n\n```\n# For test environment\n$docker-compose down -v\n# For Akash deployment\n(Consult Akash [documentation](https://docs.akash.network/guides/deploy)\n```\n\n**Start it back up (development env)**\n\n```\n$docker-compose up -d\n# run the front end\n$ cd application/frontend/ && npm start\n# Connect to the browser at http://localhost:3000\n```\n\nYou should now see the values from the database that you backed up.\n\n**Start it back up (production env)**\n\n* Update [akash/deploy-sample.yml](https://github.com/coffeeroaster/unstoppable-web2.0/blob/main/akash/deploy-sample.yml) with updated ENV variables (namely the **BACKUP\\_SKYNET\\_URL** )\n* Redeploy backend to Akash following `step 3` in this guide.\n* If using Handshake (Step 5) Use the same Front END URL.\n\n### Database replication next steps\n\n* Investigate [NuCypher](https://nucypher.com) for better key management\n* Look into the [SkyNet registry](https://siasky.net/docs/?javascript--node#setting-data-on-the-registry) for a simpler way to provide backup / restoration. Right now, each provision to Akash requires updating a new SkyLink URL. Using a Skylink Registry will allow the container to use a consistent `skyns URL` to retrieve the latest backup for ease of use.\n\n### Run demo application locally\n\nTo run the application locally, you can follow the steps below.\n\n1.  Stand up FastAPI and PostgreSQL **backend**\n\n    ```\n    cd backend\n    docker-compose up --build\n    ```\n2.  In a separate terminal, stand up the React **frontend**\n\n    ```\n    cd frontend\n    npm install\n    npm run start\n    ```\n\nYou can now **visit** [**http://localhost:3000**](http://localhost:3000) in the browser to **access the UI**. For testing the **API**, requests can be made to port 80, **Example**: `GET http://localhost:80/api/v1/notes`\n\n### Sources and resources\n\n**Akash**\n\n* [Akash deployment documentation](https://docs.akash.network/guides/deploy)\n* [SDL examples](https://github.com/akash-network/awesome-akash)\n* [In-depth walkthrough guide](https://medium.com/coinmonks/guide-to-deploying-applications-to-akash-decloud-b35dc97e5ca4)\n* [Akash Discord](https://discord.com/invite/DxftX67)\n\n**Skynet**\n\n* [Detailed Skynet deployment guide](https://blog.sia.tech/automated-deployments-on-skynet-28d2f32f6ca1)","description":null,"slug":"docs/guides/unstopabble-web"},{"title":"Akash Documentation","body":"\nimport Card from \"@/components/docs/homepage-cat-cards.astro\";\n\n\n\n\nimport wallets from './assets/essentials/wallets.svg'\nimport stack from './assets/essentials/stack.svg'\nimport questions from './assets/essentials/questions.svg'\n\n### Essentials\n\n\n      <div class=\"mt-5 grid grid-cols-1 gap-5 sm:grid-cols-2 2xl:grid-cols-3\">\n        <Card\n          title=\"Tokens & Wallets\"\n          description=\"Tap into the Akash Marketplace and deploy permissionlessly using one of the network’s open-source deployment tools.\"\n          icon={wallets}\n          link=\"/docs\"\n        />\n        <Card\n          title=\"Stack Definition Language (SDL)\"\n          description=\"Deploy a wide range of applications with one-click templates, including the leading AI models and web services.\"\n          icon={stack}\n          link=\"/docs\"\n        />\n        <Card\n          title=\"Common Questions\"\n          description=\"Monetize your cloud resources on the open-source Akash marketplace.\"\n          icon={questions}\n          link=\"/docs\"\n        />\n      </div>\n\n### Deployments\n    \n\n      <div class=\"mt-5 grid grid-cols-1 gap-5 sm:grid-cols-2 2xl:grid-cols-3\">\n        <Card\n          title=\"Cloudmos Deploy\"\n          description=\"Tap into the Akash Marketplace and deploy permissionlessly using one of the network’s open-source deployment tools.\"\n          icon={wallets}\n          link=\"/docs\"\n        />\n        <Card\n          title=\"CLI\"\n          description=\"Deploy a wide range of applications with one-click templates, including the leading AI models and web services.\"\n          icon={stack}\n          link=\"/docs\"\n        />\n        <Card\n          title=\"Stable Payment Deployments\"\n          description=\"Monetize your cloud resources on the open-source Akash marketplace.\"\n          icon={questions}\n          link=\"/docs\"\n        />\n      </div>\n\n\n### Providers\n\n  <div class=\"mt-5 grid grid-cols-1 gap-5 sm:grid-cols-2 2xl:grid-cols-3\">\n    <Card\n      title=\"Access high-performance GPUs\"\n      description=\"Tap into the Akash Marketplace and deploy permissionlessly using one of the network’s open-source deployment tools.\"\n      icon={wallets}\n      link=\"/docs\"\n    />\n    <Card\n      title=\"Deploy an application\"\n      description=\"Deploy a wide range of applications with one-click templates, including the leading AI models and web services.\"\n      icon={stack}\n      link=\"/docs\"\n    />\n    <Card\n      title=\"Praetor Provider Build App\"\n      description=\"Monetize your cloud resources on the open-source Akash marketplace.\"\n      icon={questions}\n      link=\"/docs\"\n    />\n  </div>","description":"Explore the world of Akash through our comprehensive documentation, offering tutorials and a rich repository of platform insights..","slug":"docs"},{"title":"Akash v0.30.0 Node Upgrade Guide","body":"\n> _**NOTE**_ - documentation for Mainnet9 upgrade is not yet complete.  Please check back prior to announced upgrade date/time for finalized details.\n\nDocumentation related to Akash Network upgrade to version `v0.30.0`:\n\n* [Node Upgrade Instructions](#akash-v0300-node-upgrade-guide)\n\n### Upgrade Details\n\n* Upgrade Height: `13880774`\n* [Upgrade Timer](https://www.mintscan.io/akash/block/13880774)\n\n\n\n\n## Akash v0.30.0 Node Upgrade Guide\n\n### Upgrade Details\n\n* **Upgrade name**: `v0.30.0`\n* **Binary version**: `v0.30.0`\n* [Upgrade countdown/block height](https://www.mintscan.io/akash/block/13880774)\n* [Binary Links](https://github.com/akash-network/node/releases/tag/v0.30.0)\n\n### Common Steps for All Upgrade Options\n\nIn the sections that follow both `Cosmovisor` and `non-Cosmovisor` upgrade paths are provided. Prior to detailing specifics steps for these upgrade paths, in this section we cover steps required regardless of upgrade path chosen.\n\n> _**NOTE -**_ The following steps are not required if the auto-download option is enabled for Cosmovisor.\n\nEither download the [Akash binary](https://github.com/akash-network/node/releases/tag/v0.30.0) or build it from source. We highly recommend using a pre-complied binary but provide instructions to build from source here in the rare event it would be necessary.\n\n### Option 1: Upgrade Using Cosmovisor\n\nThe following instructions assume the `akash` and `cosmovisor` binaries are already installed and cosmovisor is set up as a systemd service.\n\nThe section that follows will detail the install/configuration of Cosmovisor. If additional details are necessary, visit [Start a node with Cosmovisor](https://github.com/akash-network/docs/blob/anil/v3-instructions/guides/node/cosmovisor.md)for instructions on how to install and set up the binaries.\n\n> _**NOTE**_ - Cosmovisor `1.5.0` is required\n\n#### Configure Cosmovisor\n\n> _**Note**_: The following steps are not required if Cosmovisor v1.0 is already installed and configured to your preferred settings.\n\nTo install `cosmovisor` by running the following command:\n\n```\ngo install cosmossdk.io/tools/cosmovisor/cmd/cosmovisor@v1.5.0\n```\n\nCheck to ensure the installation was successful:\n\n```\nDAEMON_NAME=akash DAEMON_HOME=~/.akash cosmovisor version\n```\n\nUpdate `cosmovisor` systemd service file and make sure the environment variables are set to the appropriate values(the following example includes the recommended settings).\n\n* _**NOTE**_ - It is preferable if you start your service under a dedicated non-system user other than root.\n* _**NOTE**_ - `DAEMON_SHUTDOWN_GRACE` (optional, default none), if set, send interrupt to binary and wait the specified time to allow for cleanup/cache flush to disk before sending the kill signal. The value must be a duration (e.g. 1s).\n\n```\n[Unit]\nDescription=Akash with cosmovisor\nRequires=network-online.target\nAfter=network-online.target\n\n[Service]\nUser=root\nGroup=root\nExecStart=/root/go/bin/cosmovisor run start\n\nRestart=always\nRestartSec=10\nLimitNOFILE=4096\nEnvironment=\"DAEMON_NAME=akash\"\nEnvironment=\"DAEMON_HOME=/root/.akash\"\nEnvironment=\"DAEMON_RESTART_AFTER_UPGRADE=true\"\nEnvironment=\"DAEMON_ALLOW_DOWNLOAD_BINARIES=false\"\nEnvironment=\"DAEMON_LOG_BUFFER_SIZE=512\"\nEnvironment=\"UNSAFE_SKIP_BACKUP=true\"\nEnvironment=\"DAEMON_SHUTDOWN_GRACE=15s\"\n\n[Install]\nWantedBy=multi-user.target\n```\n\nCosmovisor can be configured to automatically download upgrade binaries. It is recommended that validators do not use the auto-download option and that the upgrade binary is compiled and placed manually.\n\nIf you would like to enable the auto-download option, update the following environment variable in the systemd configuration file:\n\n```\nEnvironment=\"DAEMON_ALLOW_DOWNLOAD_BINARIES=true\"\n```\n\nCosmovisor will automatically create a backup of the data directory at the time of the upgrade and before the migration.\n\nIf you would like to disable the auto-backup, update the following environment variable in the systemd configuration file:\n\n```\nEnvironment=\"UNSAFE_SKIP_BACKUP=true\"\n```\n\nMove the file to the systemd directory:\n\n```\nsudo mv cosmovisor.service /etc/systemd/system/akash.service\n```\n\nRestart `cosmovisor` to ensure the environment variables have been updated:\n\n```\nsystemctl daemon-reload\nsystemctl start akash\nsystemctl enable akash\n```\n\nCheck the status of the `cosmovisor` service:\n\n```\nsudo systemctl status cosmovisor\n```\n\nEnable `cosmovisor` to start automatically when the machine reboots:\n\n```\nsudo systemctl enable cosmovisor.service\n```\n\n#### Prepare Upgrade Binary\n\n> Skip this section if you have enabled DAEMON\\_ALLOW\\_DOWNLOAD\\_BINARIES cosmovisor parameter. It will automatically create the correct path and download the binary based on the plan info from the govt proposal.\n\nCreate the folder for the upgrade (v0.30.0) - cloned in this [step](v0.30.0-upgrade-docs.md#common-steps-for-all-upgrade-options) - and copy the akash binary into the folder.\n\nThis next step assumes that the akash binary was built from source and stored in the current (i.e., akash) directory:\n\n```\nmkdir -p $HOME/.akash/cosmovisor/upgrades/v0.30.0/bin\n\ncp ./.cache/bin $HOME/.akash/cosmovisor/upgrades/v0.30.0/bin\n```\n\nAt the proposed block height, `cosmovisor` will automatically stop the current binary (v0.28.X), set the upgrade binary as the new current binary (v0.30.0), and then restart the node.\\\\\n\n### Option 2: Upgrade Without Cosmovisor\n\nUsing Cosmovisor to perform the upgrade is not mandatory.\n\nNode operators also have the option to manually update the `akash` binary at the time of the upgrade. Doing it before the upgrade height will stop the node.\n\nWhen the chain halts at the proposed upgrade height, stop the current process running `akash`.\n\nEither download the [Akash binary](https://github.com/akash-network/node/releases/tag/v0.30.0) or build from source - completed in this [step](#common-steps-for-all-upgrade-options) - and ensure the `akash` binary has been updated:\n\n```\nakash version\n```\n\nUpdate configuration with\n\n```\nakash init\n```\n\nRestart the process running `akash`.\n\n### Appendix\n\n#### Build Binary From Sources\n\n##### Prerequisites\n\n> _**NOTE**_ - we highly recommend downloading a complied Akash binary over building the binary from source\n\nDirenv is required to compile sources. Check [here](https://direnv.net) for details on how to install and hook to your shell\n\nif using direnv first time (or cloning sources on to a new host) you should see following message after entering source dir:\n\n```shell\ndirenv: error .envrc is blocked. Run `direnv allow` to approve its content\n```\n\nif no such message, most like direnv is not hooked to the shell\n\n##### Build\n\n```shell\ngit clone --depth 1 --branch v0.30.0 https://github.com/akash-network/node\ncd node\ndirenv allow\nmake release\n```\n\nAfter build artefacts are located in `dist` directory","description":null,"slug":"docs/mainnet-9-upgrade/node-upgrade-guide"},{"title":"Authorized Spend","body":"\nAuthorized Spend allows users to authorize spend of a set number of tokens from a source wallet to a destination, funded wallet.  The authorized spend is restricted to Akash deployment activities and  the recipient of the tokens would not have access to those tokens for other operations. This allows large teams to work on deployments together without using large shared wallets, thereby reducing security concerns.\n\n* [Relevant Commands and Example Use](#relevant-commands-and-example-use)\n\n\n## Relevant Commands and Example Use\n\n### Authorize Another Wallet to Deploy Using Your Tokens\n\n#### **Description**\n\nAuthorize a “deploy wallet” to receive a specified amount of funds from a “funding wallet”\n\nThe command must be executed from a machine that has access to the funding wallet’s private key (I.e. access to private-key in local key-chain).\n\nNOTE - two wallets will be necessary to test Authorized Spend.\n\n#### **Syntax**\n\n* Replace wallet placeholders with actual addresses\n* Ensure that `uakt` is used as denomination for the `fund-amount` and as shown in `Example Use`.\n\n```\nprovider-services tx deployment authz grant <deploy-wallet> <fund-amount> --from <funding-wallet>\n```\n\n#### Example Use\n\n```\nprovider-services tx deployment authz grant akash17ck7uhkpjjj45fw9s9vpv7jn0m97958vjjxmf8 20000000uakt --from akash10x24jqyplwk37nynqy0pqaez5sx9fqrll59hl9\n```\n\n### View Authorization Created - Specific Deploy Wallet\n\n#### **Description**\n\nTo view details for a specific deploy wallet authorization from a specified funding wallet.\n\n#### **Syntax**\n\n```\nprovider-services query authz grants <funding-wallet> <deploy-wallet>\n```\n\n#### **Example Use**\n\n```\nprovider-services query authz grants akash10x24jqyplwk37nynqy0pqaez5sx9fqrll59hl9 akash17ck7uhkpjjj45fw9s9vpv7jn0m97958vjjxmf8\n```\n\n#### **Expected Output**\n\n```\nroot@ip-10-0-10-95:/home/ubuntu# akash query authz grants akash10x24jqyplwk37nynqy0pqaez5sx9fqrll59hl9 akash17ck7uhkpjjj45fw9s9vpv7jn0m97958vjjxmf8\n\ngrants:\n- authorization:\n    '@type': /akash.deployment.v1beta2.DepositDeploymentAuthorization\n    spend_limit:\n      amount: \"20000000\"\n      denom: uakt\n  expiration: \"2023-01-20T16:04:02Z\"\npagination:\n  next_key: null\n  total: \"0\"\n```\n\n### View Authorizations Created - All Deploy Wallets\n\n#### **Description**\n\nTo view ALL wallets authorized to spend from the funding wallet\n\n#### **Syntax**\n\n```\nprovider-services query authz granter-grants <funding-wallet-address>\n```\n\n#### **Example Use**\n\n```\nprovider-services query authz granter-grants akash10x24jqyplwk37nynqy0pqaez5sx9fqrll59hl9\n```\n\n### Change Amount of Authorized Funds\n\n#### **Description**\n\nTo change the amount of an authorized funds\n\n#### **Syntax**\n\n```\nprovider-services tx deployment authz grant <deploy-wallet> <fund-amount> --from <funding-wallet> --gas-prices=\"0.025uakt\" --gas=\"auto\" --gas-adjustment=1.5 -y\n```\n\n#### **Example Use**\n\n```\nprovider-services tx deployment authz grant akash17ck7uhkpjjj45fw9s9vpv7jn0m97958vjjxmf8 10000000uakt --from akash10x24jqyplwk37nynqy0pqaez5sx9fqrll59hl9 --gas-prices=\"0.025uakt\" --gas=\"auto\" --gas-adjustment=1.5 -y\n```\n\n### Create a Deployment from Authorized Funds\n\n#### **Description**\n\nUse the funds from the authorizers wallet to create a deployment. Please note that the deployment wallet needs some minimal, additional AKT to cover gas costs.\n\nNOTE - only the creation deployment step is covered in this section.  Please refer to our [Getting Started with Testnet](broken-reference) documentation for additional steps in creating a deployment.\n\n#### **Syntax**\n\n```\nprovider-services tx deployment create --depositor-account <funding-wallet> --from <deploy-wallet> deploy.yaml --gas-prices=\"0.025uakt\" --gas=\"auto\" --gas-adjustment=1.5 -y \n```\n\n#### **Example Use**\n\n```\nprovider-services tx deployment create --depositor-account akash10x24jqyplwk37nynqy0pqaez5sx9fqrll59hl9 --from akash17ck7uhkpjjj45fw9s9vpv7jn0m97958vjjxmf8 deploy.yaml --gas-prices=\"0.025uakt\" --gas=\"auto\" --gas-adjustment=1.5 -y\n```\n\n### **Deposit Additional Funds to Deployment**\n\n#### **Description**\n\nDeposit additional funds into the escrow account of a running deployment from the funding wallet\n\n#### **Syntax**\n\n```\nprovider-services tx deployment deposit <fund-amount> --dseq <deployment-id> --from <deploy-wallet> -–depositor-account <funding-wallet> --gas-prices=\"0.025uakt\" --gas=\"auto\" --gas-adjustment=1.5 -y\n```\n\n#### **Example Use**\n\n```\nprovider-services tx deployment deposit 10000000uakt --dseq 19012 --from akash17ck7uhkpjjj45fw9s9vpv7jn0m97958vjjxmf8 --depositor-account akash10x24jqyplwk37nynqy0pqaez5sx9fqrll59hl9 --gas-prices=\"0.025uakt\" --gas=\"auto\" --gas-adjustment=1.5 -y\n```\n\n### Revoke Access to a Deploy Wallet\n\n#### **Description**\n\nRevoke the authorization from a funding wallet\n\n#### **Syntax**\n\n```\nprovider-services tx deployment authz revoke <deploy-wallet> --from <funding-wallet>\n```\n\n#### **Example Use**\n\n```\nprovider-services tx deployment authz revoke akash17ck7uhkpjjj45fw9s9vpv7jn0m97958vjjxmf8 --from akash10x24jqyplwk37nynqy0pqaez5sx9fqrll59hl9\n```","description":null,"slug":"docs/network-features/authorized-spend"},{"title":"Deployment HTTP Options","body":"\n\nAkash deployment SDL services stanza definitions have been augmented to include “http\\_options” allowing granular specification of HTTP endpoint parameters.  Inclusion of the parameters in this section are optional but will afford detailed definitions of attributes such as body/payload max size where necessary.\n\n\nThe following “http\\_options” have been introduced in this version.  In subsequent sections of this guide the placement of “http\\_options” within the SDL services stanza will be detailed.\\\n\n\n> NOTE - the default HTTP option settings can be found [here](https://github.com/akash-network/node/blob/main/sdl/v2.go#L18..L37)\n\n* _**max\\_body\\_size**_ - sets the maximum size of an individual HTTP request body\n* _**read\\_timeout**_ - duration the proxy will wait for a response from the service&#x20;\n* _**send\\_timeout**_ - duration the proxy will wait for the service to accept a request\n* _**next\\_cases**_ - defines the cases where the proxy will try another replica in the service.  Reference the upcoming “Next Cases Attribute Usage” section for details pertaining to allowed values.\n* _**next\\_tries**_ - number of attempts the proxy will attempt another replica\n* _**next\\_timeout**_ - amount of time the proxy will wait before considering a timeout has occurred\n\n## **Example HTTP Options Usage**\n\n* Depiction displays the placement and structure of the http\\_options key within the greater services section and within a specific service’s expose key.\n* Service section of the greater SDL isolated for focus.\n\n![](https://lh4.googleusercontent.com/oXXBUSlWyFomOTKfA0z38maeEkdc-Y264KAukd0bnLByiQRDB6l3Qwa43jYmfk-Q4N6CXC7p5PPwqSobCOuVBKlaQUko9HTAJU1SJq\\_Yyv6AOgv2Z3dKOlQxkoHwJ-yyMv0eRy\\_e=s0)\n\n\n\n* Depiction displays the placement of http\\_options within the entire, greater SDL definition\n\n![](https://lh3.googleusercontent.com/cOrxEtOvXzyhPHbEpA\\_DI06km8v627RJZEmGlPFqE41k8N5I53DBGsEi3lXbxYewvjCUiN9fP9qItPC5E0zNOV8jkQYrl2sIREPnafu\\_k9zleNN1HKSYboFQR40U01o\\_P22limIC=s0)\n\n## **Next Cases Attribute Usage**\n\nThe “http\\_options” key of “next\\_cases” accepts an array of values which may contain one or more of the following values.  When included in the “next\\_cases” array value - the specified HTTP response code/message will provoke an attempt to service the request by one of the other container members/replicas of the deployment.  The “next\\_cases” attempt to service via an additional container will only provoke if the SDL defines a count of greater than one (1).  A deployment with a count of one (1) would have no other replicas to facilitate the additional service attempt.&#x20;\n\n* error”\n* “timeout”\n* “403”\n* “404”\n* “429”\n* “500”\n* “502”\n* “503”\n* “504”\n* “off”\n\n## **SDL Example Utilized**\n\nFull SDL code samples used throughout this guide\n\n```\n---\nversion: \"2.0\"\nservices:\n web:\n   image: pengbai/docker-supermario\n   expose:\n     - port: 8080\n       as: 80\n       to:\n         - global: true\n       http_options:\n         max_body_size: 3145728\n         read_timeout: 50000\n         send_timeout: 51000\n         next_cases: [\"error\", \"500\"]\n         next_tries: 2\n       accept:\n         - supermariotest.akash.network\nprofiles:\n compute:\n   web:\n     resources:\n       cpu:\n         units: 0.1\n       memory:\n         size: 512Mi\n       storage:\n         size: 512Mi\n placement:\n   westcoast:\n     signedBy:\n       anyOf:\n         - \"akash1365yvmc4s7awdyj3n2sav7xfx76adc6dnmlx63\"\n     pricing:\n       web:\n         denom: uakt\n         amount: 3000\ndeployment:\n web:\n   westcoast:\n     profile: web\n     count: 1\n```","description":null,"slug":"docs/network-features/deployment-http-options"},{"title":"Deployment Shell Access","body":"\n\nAbilities to manage deployed Akash containers have been accentuated greatly within this release.  Introduced deployment capabilities include:\n\n* Ability to execute commands within running Linux containers/Akash deployments.  Such an ability resembles “docker exec” command execution within a live container instance.\n* Ability to gain access to the CLI/shell of a running Linux container/Akash deployment.\n* Ability to remote copy files from running Linux containers/Akash deployments to a local file instance for inspection.\n\nIn the subsections which follow granular details of these introduced features will be explored with example executions and depictions.&#x20;\n\n## **Remote Shell Command Execution**\n\n_Execute command sets within a running Akash deployment_\n\n* Command template with variable bracketing as such \\<variable-name>\n* Notes of interest pertaining to command execution:\n  * The service-name variable must match the service value in the deployment’s SDL.  For example - in the depicted segment of an SDL file below - the service-name in remote shell execution would be “web”\n\n![](https://lh3.googleusercontent.com/BgF4dAJD-W3HKaLJM4xvmLk-BWxN7-OjD5QknE7kWV9K938u3MTZj0slv5VgFd8eC41QF0JmUtzcc4pCcu5PbG-HhgtDp7QCfIokY5AI1vlewgDo1E4QMKo4AXsUMMQOw7USXjSa=s0)\n\n```\nprovider-services lease-shell --from <key-name> --dseq <dseq-number> --provider=<provider-address> <service-name> <command-to-execute>\n```\n\n*   Example command fully populated\n\n    ```\n    provider-services lease-shell --from mykey --dseq 226186 --provider=akash1gx4aevud37w4d6kfd5szgp87lmkqvumaz57yww web cat /etc/passwd\n    ```\n* Example command fully populated using environment variables\n* Prior establishment of the AKASH\\_KEY\\_NAME and AKASH\\_PROVIDER environment variables would be necessary to allow this syntax\n\n```\nprovider-services lease-shell --from $AKASH_KEY_NAME --dseq 226186 --provider=$AKASH_PROVIDER web cat /etc/passwd\n```\n\n* Expected output example\n\n![](https://lh4.googleusercontent.com/ME0D00NtelEkGHbiFQYO66gBbrPGs3IvyeNADitplLF2AE6h4JK-iaNCGEQ2C5qd2636lYvdRJRAXTnfFwGdYcJSKOe5TVtF\\_sb3jDvbtfaQOFeyod8m3d146FB9Ga6eTJ49Cvu4=s0)\n\n\n\n## **Access the Deployment Shell (CLI)**\n\n_Gain access to an active Akash deployment’s CLI/shell_\n\n* Command template with variable bracketing as such \\<variable-name>\n* Command notes of interest:\n  * The service-name variable must match the service value in the deployment’s SDL.  For example - in the depicted segment of an SDL file below - the service-name in remote shell execution would be “web”\n  * Note the “tty” switch dictating desire for shell/CLI access\n\n![](https://lh3.googleusercontent.com/BgF4dAJD-W3HKaLJM4xvmLk-BWxN7-OjD5QknE7kWV9K938u3MTZj0slv5VgFd8eC41QF0JmUtzcc4pCcu5PbG-HhgtDp7QCfIokY5AI1vlewgDo1E4QMKo4AXsUMMQOw7USXjSa=s0)\n\n```\nprovider-services lease-shell --from <key-name> --dseq <dseq-number> --tty --provider=<provider-address> <service-name> /bin/sh --node $AKASH_NODE\n```\n\n* Example command fully populated\n* Note - the container instance must have a /bin/sh shell for the command to work in this exact syntax.  If this were an Alpine container base image /bin/sh would need to become /bin/ash and this serves as an example of possible edit to the command syntax based on container type.\n* Prior establishment of the AKASH\\_KEY\\_NAME and AKASH\\_PROVIDER environment variables would be necessary to allow this syntax\n\n```\nprovider-services lease-shell --from $AKASH_KEY_NAME --dseq 226186 --tty --provider=$AKASH_PROVIDER web /bin/sh --node $AKASH_NODE\n```\n\n\n\n* Expected output example\n* Note - Linux commands “pwd” and “ls” are included and as executed within the deployment to validate Akash container shell access\n\n![](https://lh6.googleusercontent.com/6Bd4MCrhU71vIM5OzREMlV8DdxaSEO2T80PNzFJVO91mVrkDYzdBIZ45V10Crcazvpi6afl3ojocnUu\\_8bnPgxHflJ6WJuZFvZsZpfcf19wna1xs1akzCEnNzghJLJP\\_xYsVOB2F=s0)\n\n## **Copy File from Akash Container/Deployment**\n\n_Copy a file from an active Akash deployment to a local file instance for inspection_\n\n* Command template with variable bracketing as such \\<variable-name>\n* Command notes of interest:\n  * The service-name variable must match the service value in the deployment’s SDL.  For example - in the depicted segment of an SDL file below - the service-name in remote shell execution would be “web”\n\n![](https://lh3.googleusercontent.com/BgF4dAJD-W3HKaLJM4xvmLk-BWxN7-OjD5QknE7kWV9K938u3MTZj0slv5VgFd8eC41QF0JmUtzcc4pCcu5PbG-HhgtDp7QCfIokY5AI1vlewgDo1E4QMKo4AXsUMMQOw7USXjSa=s0)\n\n```\nprovider-services lease-shell --from <key-name> --dseq <dseq-number> --provider=<provider-address> <service-name> <command-to-execute> > <local-file-name>\n```\n\n* Example command fully populated\n\n```\nprovider-services lease-shell --from mykey --dseq 226186 --provider=akash1gx4aevud37w4d6kfd5szgp87lmkqvumaz57yww web cat /etc/passwd > local_copy_of_passwd\n```\n\n* Example command fully populated using environment variables\n* Prior establishment of the AKASH\\_KEY\\_NAME and AKASH\\_PROVIDER environment variables would be necessary to allow this syntax\n\n```\nprovider-services lease-shell --from $AKASH_KEY_NAME --dseq 226186 --provider=$AKASH_PROVIDER web cat /etc/passwd > local_copy_of_passwd\n```\n\n* Expected output example\n* Note - Linux command “ls” and “cat” are included in the depiction to validate successful file copy from remote Akash container/deployment to local file\n\n![](https://lh4.googleusercontent.com/cJd-e86o-vYhhPNsfLsOjxEXDM7Sb8d-AMkpjj5W8VJ9E0ynO-6RN\\_nzHQIinb00vPgm8xfj0njYBw5-\\_CqBuajPqE-sKcxrqkaehfF5Gf9RXiVJf27khnNxbm3lmYWtqTLN2Rfy=s0)\n\n## **SDL Example Utilized**\n\nFull SDL code samples used throughout this guide\n\n```\n---\nversion: \"2.0\"\nservices:\n web:\n   image: pengbai/docker-supermario\n   expose:\n     - port: 8080\n       as: 80\n       to:\n         - global: true\n       http_options:\n         max_body_size: 3145728\n         read_timeout: 50000\n         send_timeout: 51000\n         next_cases: [\"error\", \"500\"]\n         next_tries: 2\n       accept:\n         - supermariotest.akash.network\nprofiles:\n compute:\n   web:\n     resources:\n       cpu:\n         units: 0.1\n       memory:\n         size: 512Mi\n       storage:\n         size: 512Mi\n placement:\n   westcoast:\n     signedBy:\n       anyOf:\n         - \"akash1365yvmc4s7awdyj3n2sav7xfx76adc6dnmlx63\"\n     pricing:\n       web:\n         denom: uakt\n         amount: 3000\ndeployment:\n web:\n   westcoast:\n     profile: web\n     count: 1\n```","description":null,"slug":"docs/network-features/deployment-shell-access"},{"title":"Fractional uAKT","body":"\n\nFractional uAKT is the removal of the implicit minimum cost of deployment. In the past, a deployment could not be cheaper than one uAKT per block. Meaning, extremely light workloads like a crypto wallet, or perhaps a personal blog could end up being more expensive than necessary. The limitation also would have more severe consequences as token price increases. A lightweight deployment could increase from $1 to $5 per month if the token were to double or triple in price. With fractional uAKT, prices can be adjusted better so resource consumption can be accurate to the cost.\n\nIn this guide we will use the Cloudmos Deploy tool to launch deployments using fractional uAKT. If this is your first time using Cloudmos Deploy, use this [guide](https://github.com/akash-network/docs/blob/master/features/fractional-uakt/broken-reference/README.md) to install the app and get started.\n\n[Relevant SDL Declaration and Example Use](#relevant-sdl-declaration-and-example-use)\n\n\n## Relevant SDL Declaration and Example Use\n\nFor the purpose of demonstrating the use of fractional uAKT we will utilize the popular Hello World web application and SDL that can be found in the [Awesome Akash repository](https://github.com/akash-network/awesome-akash). The example SDL file will be modified to take advantage of the new fractional uAKT option.\n\nThe [Cloudmos Deploy](https://docs.akash.network/guides/deploy) application will be used to launch the deployment.\n\n### **Example Fractional uAKT Use in Cloudmos Deploy**\n\nWithin Cloudmos Deploy:\n\n* [ ] Create a new deployment\n* [ ] Select Hello-world from the available templates\n\n![](https://files.gitbook.com/v0/b/gitbook-x-prod.appspot.com/o/spaces%2F-LrNFlfuifzmQ\\_NMKu9C-887967055%2Fuploads%2FsFfTMYjuy3sh5mH9NyXi%2FfractionalCreateDeployment.png?alt=media\\&token=0b9bf90d-5a35-4bf6-8fec-33d010913337)\n\n![](https://files.gitbook.com/v0/b/gitbook-x-prod.appspot.com/o/spaces%2F-LrNFlfuifzmQ\\_NMKu9C-887967055%2Fuploads%2F4iI0PjLQlYtrotySEIS6%2FfractionalHelloWorld.png?alt=media\\&token=24b93d25-d5aa-4e8b-aebd-cf14ce7892f7)\n\n### Edit the Template with a Fractional uAKT Amount\n\n* Update the template to use a fractional uAKT amount of 100.1 as seen in the capture\n* Additionally ensure that any signBy and attributes sections are removed to mimic the screenshot below\n* The necessary template is also shown in text below the screenshot\n* Proceed thru the remaining steps to complete the deployment\n\n![](https://files.gitbook.com/v0/b/gitbook-x-prod.appspot.com/o/spaces%2F-LrNFlfuifzmQ\\_NMKu9C-887967055%2Fuploads%2FxB48MXLbSpBdYfnfUHC7%2FtestnetFractionalUpdated.png?alt=media\\&token=0eb1cfd9-e8a0-4e87-8939-424ad2ab1ee0)\n\n* The SDL should be as follows post update\n\n```\n---\nversion: \"2.0\"\n\nservices:\n  web:\n    image: tombeynon/akash-hello-world:release-v0.1.1\n    expose:\n      - port: 80\n        as: 80\n        to:\n          - global: true\n\nprofiles:\n  compute:\n    web:\n      resources:\n        cpu:\n          units: 0.5\n        memory:\n          size: 512Mi\n        storage:\n          size: 512Mi\n  placement:\n    dcloud:\n      attributes:\n        host: akash\n      pricing:\n        web:\n          denom: uakt\n          amount: 100.1\n\ndeployment:\n  web:\n    dcloud:\n      profile: web\n      count: 1\n```\n\n### Verification of Fractional uAKT Use\n\nThe verification of fractional uAKT functionality comes in two forms:\n\n1. The deployment progress and will eventually complete with a fractional uAKT per block price of 100.1\n2. The bids received from providers come in fractional uAKT amounts\n\nContinue with these steps:\n\n* Select a fractional uAKT bid from a provider and then select “ACCEPT BID”\n* Proceed the remaining steps of the deployment process\n* When the deployment completes - we have successfully tested fractional uAKT\n\n![](https://files.gitbook.com/v0/b/gitbook-x-prod.appspot.com/o/spaces%2F-LrNFlfuifzmQ\\_NMKu9C-887967055%2Fuploads%2F2uSiK0ZeItEQeuPGDAm3%2FfractionalBid.png?alt=media\\&token=3a44ce7b-6dd7-429e-add8-ad16ce12602b)","description":null,"slug":"docs/network-features/fractional-uakt"},{"title":"Hostname Migration","body":"\n\nDeployment of an updated workload encounters a challenge when the DNS hostname remains active on the existing deployment.  An example scenario is presented to ensure understanding of the challenge - followed by a review of the mechanism introduced in this release to migrate a hostname to the new deployment.\n\n## **Overview of Hostname Migration Need**\n\nIn this scenario two identical deployments will be launched mimicking an initial instance/deployment and a secondary updated deployment.\n\nThe following SDL is utilized in this example.  Note the presence of the highlighted accept key dictating the DNS name of the deployment (supermariotest.akash.network).\n\n![](https://lh6.googleusercontent.com/aXCyB9D5ScqRmD6ZfBQbI56wZkpQdlxj38AKc3wKB-\\_tmJR2DXbsPNo1S7nQgqByR5ejykf\\_MgxO57HUghtFqeKLZVYfBtKT-rb8gIcLiVPKQ9updFikKNRm-aSZeQ31vXyT-MXP=s0)\n\n* Two instances of the deployment are provoked (referred to as “OriginalDeployment” and “UpdatedDeployment” in this guide for clarity)\n* The OriginalDeployment lease status displays URIs for both the Akash full domain name (ending in akashian.io) and the SDL specified domain name of “supermariotest.akash.network”\n* Note - the DSEQ for this deployment is - 241027.  The DSEQ reference point will be of importance for later validations.\n\n![](https://lh6.googleusercontent.com/xaohN1p97gj1v\\_kyGOX5-4CtMxf25bZp9GGom9s8JH-KFwZ\\_zSE6O\\_o0yBiNavdaeMtUS15bfsuZEJCo0Qe7PBtebU8CPuAVcQXavfWa\\_N1zMAU2EcpQHDRUUhT-RcFS8jb9QEuT=s0)\n\n* The UpdatedDeployment lease status displays a single URI - the Akash full domain name (ending in akashian.io) - and the SDL specified domain name of “supermariotest.akash.network” is not present\n* The hostname of “supermariotest.akash.network” was not migrated when the new deployment generated despite the declaration of the hostname in the SDL’s accept key-value pair\n* If the initial deployment (OriginalDeployment) had been destroyed prior to the creation of the upgraded instance (UpdatedDeployment) the hostname would have been free and the new deployment would have assumed this domain name per the SDL specification.\n* But often to ensure continuity of services the desirable strategy would be to activate the new instance prior to removing a pre-existing instance and only migrate production traffic to a readied upgraded instance.\n* The hostname migration commands introduced - and discussed subsequently - allows such graceful migration.\n* Note - the DSEQ for the UpdatedDeployment is - 241064.  The DSEQ reference point will be of importance for later validations.\n\n![](https://lh5.googleusercontent.com/lofd32uDsrfnWKWeUwxtxkgj9B\\_z0hSwJevZJ5YHusIlXPLm-TXYoFmxiM6XCeWiG0IEOm7OD0qnWBpfN47uTcnIT8bxQGsEJ0VXgETiZLrqXVJ5qOhjst9QOcOVhX6cfVG-DeW9=s0)\n\n## **Hostname Migration Command**\n\nThe example hostname migration will continue with the two deployment scenarios provoked in the prior sub-section.\n\n* Hostname migration command syntax with variable bracketing as such \\<variable-name>\n\n```\nprovider-services migrate-hostnames --from <key-name> --dseq <dseq-number> --provider=<provider-address> <hostname>\n```\n\n* Example, populated hostname migration command\n* In referencing the deployment DSEQ the hostname is migrated to the UpdatedDeployment instance with a DSEQ of 241064 and simultaneously removing the hostname from OriginalDeployment (DSEQ 241027)\n\n```\nprovider-services migrate-hostnames --from mykey --dseq 241064 --provider=akash1gx4aevud37w4d6kfd5szgp87lmkqvumaz57yww supermariotest.akash.network\n```\n\n## **Validation of Hostname Migration**\n\n#### **Hostname/URI Configuration Validations**\n\nValidations of hostname migration are provided in the sample OriginalDeployment and UpdatedDeployment scenario.\\\n\n\n* The lease-status command is utilized in the section to display active URIs/hostnames associated with specific deployments\n* The syntax for the lease-status command is as follows\n\n```\nprovider-services lease-status --node $AKASH_NODE --home ~/.akash --dseq $AKASH_DSEQ --from $AKASH_KEY_NAME --provider $AKASH_PROVIDER\n```\n\n\n\nIn the depiction which follows the state of the OriginalDeployment instance proves:\n\n* Prior to the migrate-hostnames command invoke the hostname specified in the SDL is married to the instance\n* The migrate-hostnames command is invoked specifying the DSEQ of the upgraded deployment instance\n* Post invoke of migrate-hostnames the SDL specified hostname is no longer married to the original instance\n\n![](https://lh3.googleusercontent.com/NjH7BDfxfVsmlG7ct9jmf2yPBJmGGKyt4jwJ4\\_N9d2Zpqv1Xlr9MaXCjNlI9jGWduoDwieu2324H91qxI7Kuwau8yNGLOFYYyHtnQhXqTol8ApStJkOD3l6ZtUqa07n1o\\_8IgtoY=s0)\n\nIn the depiction which follows the state of the UpgradedDeployment instance proves:\n\n* Post migrate-hostnames command invoke the hostname has been migrated to the upgraded deployment instance\n* If desired and in an event in which the upgrade deployment was not servicing traffic correctly - the hostname could be migrated back to OriginalDeployment using the identical migrate-hostname command set but with the OriginalDeployment DSEQ specified\n\n![](https://lh5.googleusercontent.com/hfSpUr7G2msOv-IseXC-thWYLtNNeRU46ZwmwPdMq94duq\\_DzpVjQlN-tkBOyNvFau788DzsdS2HfDmWTBnjLFKdg8gudVv6aID9YOFjdMA4uyGp0iePaPYV4C0S0z80f\\_G3Pz0F=s0)\n\n## **Connectivity Test Validations**\n\n\n\nAs discussed prior - in a production environment it may be advantageous to leave the pre-existing deployment (I.e. OriginalDeployment in this guide’s scenario) in place post migration for possible fallback scenarios.  But to validate connectivity has migrated successfully - the legacy deployment will be removed in our example scenario to prove connectivity to the upgraded deployment post hostname migration.\n\n* Prior to hostname migration a CuRL test was conducted to the pre-existing deployment (OriginalDeployment).  At the time of this test the upgraded deployment did not exist.  A CuRL header rewrite is utilized to successfully test connectivity to the hostname (supermariotest.akash.network).\n\n```\ncurl --header 'Host: supermariotest.akash.network' 2gkpg7qhghetb7tu1ku9aspbl8.wwwp1.h18.akashian.io\n```\n\n![](https://lh5.googleusercontent.com/UNii0\\_mjtEGvVNxuh-u3-h41kAKuCWJWtIMJtM3URMwxfsVcYHN8JhevX7XiZ3HqLnNMrIIviYiAUzbURD2uM71xwiJ6s6pkbZNNd54od\\_xSps4cTRnRd1eh6We1F7XoWSVFpXRo=s0)\n\n* Post deployment  of the upgraded instance and hostname migration - the pre-existing deployment is removed, connectivity to the hostname via CuRL is conducted anew, and the successful HTML response validates the hostname was migrated properly.\n\n\n\n```\ncurl -v --header 'Host: supermariotest.akash.network' t3r1sn9c5991beqe90hfu6facg.wwwp1.h18.akashian.io\n```\n\n![](https://lh5.googleusercontent.com/WplSAKrIfP\\_NK6sMP7XoJMKRdwtKyA0Q6g2rhdOl1SgMh02KV0l3w1aE0KGwNH\\_uhW9\\_x-9YFJRUwwTynGJ\\_gJ31L3sDHtkYEoNYNXpwB5u72yko9stYE8n\\_Es-PzK1wJDlRnkaN=s0)\n\n## **Requirements and Limitations of Migrate Hostname**\n\n#### **DNS CNAME Update**\n\n* When planning a hostname migration ensure that proper public DNS record updates are planned and executed to limit interruption in services.\n* In the example scenario utilized and to highlight necessary public DNS record adjustments:\n  * The organization owning the akash.network DNS record would have a DNS canonical name (CNAME) record for supermariotest.akash.network pointing to the Akash deployment destination of 2gkpg7qhghetb7tu1ku9aspbl8.wwwp1.h18.akashian.io (OriginalDeployment)\n  * The organization would need to update the CNAME record once the hostname migration was completed to the destination of t3r1sn9c5991beqe90hfu6facg.wwwp1.h18.akashian.io (UpgradedDeployment)\n  * An interruption in service may be provoked as the public DNS records are published and distributed\n\n#### **Requirements**\n\n* A domain name migration may be migrated to any deployment associated with the same wallet.&#x20;\n* The hostname must be declared in the SDL’s accept key-value pair of the deployment to allow use of the migrate-hostnames command.\n\n#### **Limitations**\n\n* A domain name used by another wallet may not be utilized.  This is a pre-existing limitation - there may not be conflicts in domain names across wallets -- and re-emphasizing to ensure clarity that this limitation remains.\n\n## **SDL Example Utilized**\n\nFull SDL code samples used throughout this guide\n\n```\n---\nversion: \"2.0\"\nservices:\n web:\n   image: pengbai/docker-supermario\n   expose:\n     - port: 8080\n       as: 80\n       to:\n         - global: true\n       http_options:\n         max_body_size: 3145728\n         read_timeout: 50000\n         send_timeout: 51000\n         next_cases: [\"error\", \"500\"]\n         next_tries: 2\n       accept:\n         - supermariotest.akash.network\nprofiles:\n compute:\n   web:\n     resources:\n       cpu:\n         units: 0.1\n       memory:\n         size: 512Mi\n       storage:\n         size: 512Mi\n placement:\n   westcoast:\n     signedBy:\n       anyOf:\n         - \"akash1365yvmc4s7awdyj3n2sav7xfx76adc6dnmlx63\"\n     pricing:\n       web:\n         denom: uakt\n         amount: 3000\ndeployment:\n web:\n   westcoast:\n     profile: web\n     count: 1\n```","description":null,"slug":"docs/network-features/hostname-migration"},{"title":"IP Leases","body":"\n\nThe IP Lease feature for Akash deployments allows a workload to obtain a static, reserved public IP address.\n\nOur guide to including an IP Lease in your deployment includes these sequential sections.\n\n- [IP Leases Features and Limitations](#ip-leases-features-and-limitations)\n  - [IP Leases Features](#ip-leases-features)\n  - [IP Leases Limitations](#ip-leases-limitations)\n- [IP Leases within SDL](#ip-leases-within-sdl)\n  - [Endpoints Stanza](#endpoints-stanza)\n  - [Services Stanza](#services-stanza)\n  - [Attributes](#attributes)\n- [IP Leases Port Use](#ip-leases-port-use)\n- [IP Leases Verification](#ip-leases-verification)\n    - [Example/Expected Output](#exampleexpected-output)\n- [IP Leases Migration](#ip-leases-migration)\n    - [Migration Steps](#migration-steps)\n      - [Migrate Command Template](#migrate-command-template)\n      - [Migrate Command Example](#migrate-command-example)\n  - [Migration Example](#migration-example)\n- [Full SDL Example with IP Leases](#full-sdl-example-with-ip-leases)\n\n\n## IP Leases Features and Limitations\n\n### IP Leases Features\n\n* Option for Tenants to request publicly routable IP addresses for the services they deploy\n* IP Lease can be ordered as part of a deployment\n* Opens new deployment opportunities dependent: on&#x20;\n  * Static public IP address\n  * Static port mappings\n* Allows use of all ports (1-65535 range)\n\n### IP Leases Limitations\n\n* If a deployment is updated the IP lease is retained.  However if the lease is closed the IP address is not retained and the reservation of the IP address is lost.\n* IP Leases is only valid for inbound communications.  Any communication initiated from within the deployment outbound will utilize a shared IP address from the provider.  However and based on the nature of TCP communications, responses to inbound initiated traffic will use the IP Leases address.\n* An IP lease may be migrated to another deployment as detailed in this [section](ip-leases-migration.md). Using a migration it would be possible to transfer the IP lease to a new deployment and preserve the reserved address.\n* If a deployment is closed and moved to another provider there is no means to maintain or migrate the leased IP address\n* Not all providers offer the IP Leases and is currently limited to providers using [MetalLB. ](https://metallb.universe.tf/installation/clouds/)However some providers might use [alternative, custom load balancers](https://metallb.universe.tf/installation/clouds/#alternatives) such as `keepalive-vip`.\n\n\n\n## IP Leases within SDL\n\nTo illustrate the use of an IP Leases the simple Hello World SDL is used. Please refer to the full SDL used in this document for further clarification on the place of reviewed stanzas.\n\n### Endpoints Stanza\n\nIP Leases can be included in any SDL via the addition of the endpoints section. The endpoints section must be created at the top level of the SDL.\n\nIn this example we name the endpoint `myendpoint` but could be any name of your choosing.\n\nThe kind is defined as `ip` which is the only valid option at this time. This adds a requirement that the SDL leases exactly one IPv4 address from a provider.\n\n_**NOTE**_ - the endpoint name must be unique across your deployments on a single provider. Using the example - if we tried to use `myendpoint` in another deployment and on the same provider - no IP lease would be created on that second deployment.\n\n```\nendpoints:\n myendpoint:\n   kind: ip\n```\n\n### Services Stanza\n\nIn the services stanza we create the association with the endpoint detailed previously.\n\nIn this example we have a service running a docker container with port `3000` globally exposed. Additionally the `myendpoint` endpoint is defined via the `ip` key .\n\n_**NOTE**_ - if an endpoint is declared in an SDL, it must be used at least once. Specifying an endpoint name that is not declared results in an error during deployment.\n\n```\nservices:\n web:\n   image: akashlytics/hello-akash-world:0.2.0\n   expose:\n     - port: 3000\n       as: 80\n       to:\n         - global: true\n           ip: myendpoint\n```\n\n### Attributes\n\n> _**NOTE**_ - it is no longer necessary to specify the `ip-lease` attribute covered in this section as the bid engine now filters out non-IP lease capable providers when an IP endpoint is required by the deployment.  **This is now an optional and not required step**.\n\nInclude the attribute key-value pair of `ip-lease: true` in the SDL.  This ensures that only provider advertising this attribute will bid on the workload.\n\n```\n     attributes:\n        ip-lease: true\n```\n\n## IP Leases Port Use\n\nThe IPv4 standard defines ports 1-65535 as valid port numbers.&#x20;\n\nYou may use all the unique ports of a single leased IPv4 address amongst any combination of services.&#x20;\n\nAttempting to use the `same port` on `two different` services with the same endpoint is an `error`.\n\n\n## IP Leases Verification\n\nAssigned IP Leases info can be displayed from the Akash CLI via the `lease-status` command.\n\n_**NOTE**_ _**-**_ ensure the `AKASH_DSEQ` and `AKASH_PROVIDER` environment variables are defined prior to issuing this command. Additional info on environment variables set up for the Akash CLI is available [here](broken-reference).\n\n```\nprovider-services lease-status --from $AKASH_ACCOUNT_ADDRESS\n```\n\n#### Example/Expected Output\n\n**NOTE** - the IP Leases info displayed in the `ips` section.\n\n```\n{\n  \"services\": {\n    \"web\": {\n      \"name\": \"web\",\n      \"available\": 1,\n      \"total\": 1,\n      \"uris\": [\n        \"bnd9dtb1rddsl6gfcn3q0j771g.www.nocixp1.iptestnet.akashian.io\"\n      ],\n      \"observed_generation\": 1,\n      \"replicas\": 1,\n      \"updated_replicas\": 1,\n      \"ready_replicas\": 1,\n      \"available_replicas\": 1\n    }\n  },\n  \"forwarded_ports\": null,\n  \"ips\": {\n    \"web\": [\n      {\n        \"Port\": 3000,\n        \"ExternalPort\": 80,\n        \"Protocol\": \"TCP\",\n        \"IP\": \"198.204.231.229\"\n      }\n    ]\n  }\n}\n```\n\n## IP Leases Migration\n\nThe migration steps in this section are useful when you need to make changes to an existing deployment - which require deploying the app anew - and there is a desire to limit down time as we transition to the new, updated deployment.\n\nWhen starting the new deployment - on the same  provider as the pre-existing deployment and with the same IP Leases endpoint name - the new deployment will have no IP address allocated initially.  Used the Migration Steps below to transition the IP lease from the pre-existing to new/updated deployment.\n\n#### Migration Steps\n\nYou can migrate an IP address between two active deployments on the same provider. This is done by using the `akash provider migrate-endpoints` command.&#x20;\n\nTo migrate an endpoint between two deployments, the endpoint must be declared with identical names in both deployments.\n\n##### Migrate Command Template\n\n* The DSEQ specified should be that of the deployment the IP address should be migrated to\n\n```\nprovider-services migrate-endpoints --from <key-name> --dseq <dseq-number> --provider=<provider-address> <endpoint-name>\n```\n\n##### Migrate Command Example\n\n```\nprovider-services migrate-endpoints --from mykey --dseq 250172 --provider=akash16l4nf3z6xttgk673q536p873axmy8c7aggre3g myendpoint\n```\n\n### Migration Example\n\nTo experiment with the IP Leases migration functionality follow these steps.  This example is provided simply to clarify the steps and functionality of the migration process that could be used for your own applications.\n\n* STEP 1 - create a deployment using the [full SDL example](#full-sdl-example-with-ip-leases) in this guide\n* STEP 2 - create a second deployment using the [full SDL example](#full-sdl-example-with-ip-leases) in this guide.  No changes to the SDL are necessary.\n* STEP 3 - observe that the IP lease remains on the first deployment made\n* STEP 4 - with the goal of migrating the IP lease to the new/second deployment - execute the migration steps detailed in this [section](#ip-leases-migration.md#migration-steps)\n* STEP 5 - following the successful IP Leases migration - observe that the IP lease is now active on the second/new deployment\n* STEP 6 - close the first deployment completing the example of migrating an IP lease to new/updated deployment with little down time during the transition&#x20;\n\n\n## Full SDL Example with IP Leases\n\nSections of the following SDL was used in prior sections of this guide. Providing the full SDL for context and placement clarity.\n\n```\n---\nversion: \"2.0\"\n \nendpoints:\n myendpoint:\n   kind: ip\n \nservices:\n web:\n   image: akashlytics/hello-akash-world:0.2.0\n   expose:\n     - port: 3000\n       as: 80\n       to:\n         - global: true\n           ip: \"myendpoint\"\n \nprofiles:\n compute:\n   web:\n     resources:\n       cpu:\n         units: 0.5\n       memory:\n         size: 512Mi\n       storage:\n         size: 512Mi\n \n placement:\n   dcloud:\n     pricing:\n       web:\n         denom: uakt\n         amount: 1000\n \ndeployment:\n web:\n   dcloud:\n     profile: web\n     count: 1\n```","description":null,"slug":"docs/network-features/ip-leases"},{"title":"Persistent Storage","body":"\n\nAkash persistent storage allows deployment data to persist through the lifetime of a lease.  The provider creates a volume on disk that is mounted into the deployment.  This functionality closely mimics typical container persistent storage.\n\n* [Persistent Storage Limitations](#persistent-storage-limitations)\n* [Implementation Overview](#implementation-overview)\n* [Persistent Storage SDL Deepdive](#persistent-storage-sdl-deepdive)\n* [Troubleshooting](#troubleshooting)\n* [Complete Persistent Storage Manifest/SDL Example](#complete-persistent-storage-manifest-sdl-example)\n\n\n## Persistent Storage Limitations\n\n### Persistence of Storage\n\nPlease note that storage only persists during the lease. The storage is lost when:\n\n* The deployment is migrated to a different provider.\n* The deployment’s lease is closed.  Even when relaunched onto the same provider, storage will not persist across leases.\n* Shared volumes are not currently supported.  If a SDL defines a single profile with a persistent storage definition - and that profile is then used by multiple services - individual, unique volumes will be created per service.\n\n### Deployment Specifications\n\n* Note that currently only a single persistent volume is allowed/supported per service definition in the Akash SDL.  It is not possible to mount multiple persistent volumes in a service.\n\n### Additional Details\n\nWhen planning to use persistent storage in a deployment, take into account the network (between the storage nodes) as a factor which will cause the latency, causing slower disk throughput / IOPS. This might not be suitable for heavy IOPS applications such as a Solana validator.\n\n## Implementation Overview\n\n### Configuration Examples\n\nFor the purposes of our review this [SDL Example ](#complete-persistent-storage-manifest-sdl-example)will be used.\n\n### Backward Compatibility\n\nThe introduction of persistent storage does not affect your pre-existing SDLs.  All manifests authored previously will work as is with no change necessary.\n\n### Troubleshooting Tips\n\nIf any errors occur in your deployments of persistent storage workloads, please review the [troubleshooting ](#troubleshooting)section for possible tips.  We will continue to build on this section if there are any frequently encountered issues.\n\n\n## Persistent Storage SDL Deepdive\n\nOur review highlights the persistent storage parameters within the larger SDL example.\n\n### Resources Section <a href=\"#resources-section\" id=\"resources-section\"></a>\n\n#### Overview <a href=\"#overview\" id=\"overview\"></a>\n\nWithin the profiles > compute > \\<profile-name> resources section of the SDL storage profiles are defined. Our review begins with an overview of the section and this is followed by a deep dive into the available parameters.\n\n_**NOTE**_ - a maximum amount of two (2) volumes per profile may be defined. The storage profile could consist of:\n\n* A mandatory local container volume which is created with only a size key in our example\n* An optional persistent storage volume which is created with the persistent: true attribute in our example\n\n#### Name <a href=\"#name\" id=\"name\"></a>\n\nEach storage profile has a new and optional field name. The name is used by services to link service specific storage parameters to the storage. It can be omitted for single value use case and default value is set to default.\n\n#### Attributes <a href=\"#attributes\" id=\"attributes\"></a>\n\nA storage volume may have the following attributes.\n\n_**persistent**_ - determines if the volume requires persistence or not. The default value is set to false.\n\n_**class**_ - storage class for persistent volumes. Default value is set to default. NOTE - It is invalid to set storage class for non-persistent volumes. Storage volume class types are expanded upon in the subsequent section.\n\n#### Storage Class Types\n\nThe class allows selection of a storage type.  Only providers capable of delivering the storage type will bid on the lease.\n\n| Class Name | Throughput/Approx matching device |\n| ---------- | --------------------------------- |\n| beta1      | hdd                               |\n| beta2      | ssd                               |\n| beta3      | NVMe                              |\n\n```\nprofiles:\n  compute:\n    grafana-profile:\n      resources:\n        cpu:\n          units: 1\n        memory:\n          size: 1Gi\n        storage:\n          - size: 512Mi\n          - name: data\n            size: 1Gi\n            attributes:\n              persistent: true\n              class: beta2\n```\n\n### Services Section\n\n#### Overview\n\nWithin the services > \\<service-name> section a new params section is introduced and is meant to define service specific settings.  Currently only storage related settings are available in params.  Our review begins with an overview of the section and this is followed by a deep dive into the use of storage params.\n\n```\nservices:\n  postgres:\n    image: postgres\n    params:\n      storage:\n        data:\n          mount: /var/lib/postgres\n```\n\n#### Params\n\nNote that params is an optional section under the greater services section.  Additionally note that non-persistent storage should not be defined in the params section.  In this example profile section, two storage profiles are created.  The no name ephemeral storage is not mentioned in the services > params definition.  However the persistent storage profile, named data, is defined within services > params.\n\n```\nprofiles:\n  compute:\n    grafana-profile:\n      resources:\n        cpu:\n          units: 1\n        memory:\n          size: 1Gi\n        storage:\n          - size: 512Mi\n          - name: data\n            size: 1Gi\n            attributes:\n              persistent: true\n              class: beta3\n```\n\n#### Storage\n\nThe persistent volume is mounted within the container’s local /var/lib/postgres directory.\n\n```\n     params:\n      storage:\n        default:\n          mount: /var/lib/postgres\n```\n\n### Alternative Uses of Params Storage\n\n#### Default Name Use\n\nIn this example the params > storage section is defined for a storage profile using the default (no name explicitly defined) profile\n\n##### _**Services Section**_\n\n```\nservices:\n  postgres:\n    image: postgres/postgres\n    params:\n      storage:\n        data:\n          mount: /var/lib/postgres\n```\n\n##### **Profiles Section**\n\n```\nprofiles:\n  compute:\n    grafana-profile:\n      resources:\n        cpu:\n          units: 1\n        memory:\n          size: 1Gi\n        storage:\n          - size: 512Mi\n          - size: 1Gi\n            attributes:\n              persistent: true\n              class: beta3\n```\n\n## Troubleshooting\n\n### Possible Deployment Issues and Recommendations\n\n##### Provider Slow or Over Utilized disks\n\n_Issue_  - Slow/over utilized disks OR networking issue impacting distributed storage system (Ceph)&#x20;\n\n_Solution_ -  always use providers with beta3 class fast storage and change to a new provider if you experience issues\n\n##### Persistent Storage for Deployment Full\n\n_Issue_ - persistent storage allocated to the deployment reaches capacity\n\n_Solution_ - either use fast ephemeral storage so the pod will automatically restart once it gets full or allocate more disk space when for the persistent storage.  Continue to watch and clean the disk or redeploy the pod once persistent storage gets full.\n\n### Hostname Conflict - May Cause Manifest Send Errors\n\nIf the hostname defined in the accept field is already in use within the Akash provider, a conflict occurs if another deployment attempts to launch with the same hostname.  This could occur within our testnet environment if multiple people are attempting to use the same SDL and deploy to the same provider.  Consider changing the accept field to a unique hostname (I.e. \\<myname>.locahost) if you receive an error in send of the manifest to the provider.\n\n```\n grafana:\n    image: grafana/grafana\n    expose:\n      - port: 3000\n        as: 80\n        to:\n          - global: true\n        accept:\n          - webdistest.localhost\n```\n\n\n## Complete Persistent Storage Manifest/SDL Example\n\n```\nversion: \"2.0\"\nservices:\n  postgres:\n    image: postgres\n    params:\n      storage:\n        data:\n          mount: /var/lib/postgres\n  grafana:\n    image: grafana/grafana\n    expose:\n      - port: 3000\n        as: 80\n        to:\n          - global: true\n        accept:\n          - webdistest.localhost\n    params:\n      storage:\n        data:\n          mount: /var/lib/grafana\nprofiles:\n  compute:\n    grafana-profile:\n      resources:\n        cpu:\n          units: 1\n        memory:\n          size: 1Gi\n        storage:\n          - size: 512Mi\n          - name: data\n            size: 1Gi\n            attributes:\n              persistent: true\n              class: beta2\n    postgres-profile:\n      resources:\n        cpu:\n          units: 1\n        memory:\n          size: 1Gi\n        storage:\n          - size: 512Mi\n          - name: data\n            size: 1Gi\n            attributes:\n              persistent: true\n              class: beta2\n  placement:\n    westcoast:\n      pricing:\n        grafana-profile:\n          denom: uakt\n          amount: 1000\n        postgres-profile:\n          denom: uakt\n          amount: 7000\ndeployment:\n  grafana:\n    westcoast:\n      profile: grafana-profile\n      count: 1\n  postgres:\n    westcoast:\n      profile: postgres-profile\n      count: 1\n```","description":null,"slug":"docs/network-features/persistent-storage"},{"title":" Akash Mainnet8 Upgrade","body":"\n\n\n> _**NOTE**_ - documentation for Mainnet8 upgrade is not yet complete.  Please check back prior to announced upgrade date/time for finalized details.\n\nDocumentation related to Akash Network upgrade to version `0.28.0`:\n\n* [Node Upgrade Instructions](#akash-v0280-node-upgrade-guide)\n* [Provider Upgrade Instructions](#provider-up)\n\n### Upgrade Details\n\n* Upgrade Height: `13759618`\n* [Upgrade Timer](https://www.mintscan.io/akash/block/13759618)\n\n\n\n## Akash v0.28.0 Node Upgrade Guide\n\n### Upgrade Details\n\n- **Upgrade name**: v0.28.0\n- **Binary version**: v0.28.2\n- [Upgrade countdown/block height](https://www.mintscan.io/akash/block/13759618)\n- [Binary Links](https://github.com/akash-network/node/releases/tag/v0.28.2)\n\n### Common Steps for All Upgrade Options\n\nIn the sections that follow both `Cosmovisor` and `non-Cosmovisor` upgrade paths are provided.\nPrior to detailing specifics steps for these upgrade paths, in this section we cover steps required regardless of upgrade path chosen.\n\n> _**NOTE -**_ The following steps are not required if the auto-download option is enabled for Cosmovisor.\n\nEither download the [Akash binary](#upgrade-details) or build it from source.\nWe highly recommend using a pre-complied binary but provide instructions to build from source [here](#build-binary-from-source) in the rare event it would be necessary.\n\n### Option 1: Upgrade Using Cosmovisor\n\nThe following instructions assume the `akash` and `cosmovisor` binaries are already installed and cosmovisor is set up as a systemd service.\n\nThe section that follows will detail the install/configuration of Cosmovisor.\nIf additional details are necessary, visit [Start a node with Cosmovisor](https://github.com/akash-network/docs/blob/anil/v3-instructions/guides/node/cosmovisor.md)for instructions on how to install and set up the binaries.\n\n> _**NOTE**_ - Cosmovisor 1.0 is required\n\n#### Configure Cosmovisor\n\n> _**Note**_: The following steps are not required if Cosmovisor v1.0 is already installed and configured to your preferred settings.\n\nTo install `cosmovisor` by running the following command:\n\n```\ngo install cosmossdk.io/tools/cosmovisor/cmd/cosmovisor@v1.5.0\n```\n\nCheck to ensure the installation was successful:\n\n```\nDAEMON_NAME=akash DAEMON_HOME=~/.akash cosmovisor version\n```\n\nUpdate `cosmovisor` systemd service file and make sure the environment variables are set to the appropriate values(the following example includes the recommended settings).\n\n- _**NOTE**_ - It is preferable if you start your service under a dedicated non-system user other than root.\n- _**NOTE**_ - `DAEMON_SHUTDOWN_GRACE` (optional, default none), if set, send interrupt to binary and wait the specified time to allow for cleanup/cache flush to disk before sending the kill signal. The value must be a duration (e.g. 1s).\n\n```\n[Unit]\nDescription=Akash with cosmovisor\nRequires=network-online.target\nAfter=network-online.target\n\n[Service]\nUser=root\nGroup=root\nExecStart=/root/go/bin/cosmovisor run start\n\nRestart=always\nRestartSec=10\nLimitNOFILE=4096\nEnvironment=\"DAEMON_NAME=akash\"\nEnvironment=\"DAEMON_HOME=/root/.akash\"\nEnvironment=\"DAEMON_RESTART_AFTER_UPGRADE=true\"\nEnvironment=\"DAEMON_ALLOW_DOWNLOAD_BINARIES=false\"\nEnvironment=\"DAEMON_LOG_BUFFER_SIZE=512\"\nEnvironment=\"UNSAFE_SKIP_BACKUP=true\"\nEnvironment=\"DAEMON_SHUTDOWN_GRACE=15s\"\n\n[Install]\nWantedBy=multi-user.target\n```\n\nCosmovisor can be configured to automatically download upgrade binaries.\nIt is recommended that validators do not use the auto-download option and that the upgrade binary is compiled and placed manually.\n\nIf you would like to enable the auto-download option, update the following environment variable in the systemd configuration file:\n\n```\nEnvironment=\"DAEMON_ALLOW_DOWNLOAD_BINARIES=true\"\n```\n\nCosmovisor will automatically create a backup of the data directory at the time of the upgrade and before the migration.\n\nIf you would like to disable the auto-backup, update the following environment variable in the systemd configuration file:\n\n```\nEnvironment=\"UNSAFE_SKIP_BACKUP=true\"\n```\n\nMove the file to the systemd directory:\n\n```\nsudo mv cosmovisor.service /etc/systemd/system/akash.service\n```\n\nRestart `cosmovisor` to ensure the environment variables have been updated:\n\n```\nsystemctl daemon-reload\nsystemctl start akash\nsystemctl enable akash\n```\n\nCheck the status of the `cosmovisor` service:\n\n```\nsudo systemctl status cosmovisor\n```\n\nEnable `cosmovisor` to start automatically when the machine reboots:\n\n```\nsudo systemctl enable cosmovisor.service\n```\n\n#### Prepare Upgrade Binary\n\n> Skip this section if you have enabled DAEMON_ALLOW_DOWNLOAD_BINARIES cosmovisor parameter. It will automatically create the correct path and download the binary based on the plan info from the govt proposal.\n\nCreate the folder for the upgrade (v0.28.0) - cloned in this [step](#common-steps-for-all-upgrade-options) - and copy the akash binary into the folder.\n\nThis next step assumes that the akash binary was built from source and stored in the current (i.e., akash) directory:\n\n```\nmkdir -p $HOME/.akash/cosmovisor/upgrades/v0.28.0/bin\n\ncp ./.cache/bin $HOME/.akash/cosmovisor/upgrades/v0.28.0/bin\n```\n\nAt the proposed block height, `cosmovisor` will automatically stop the current binary (v0.26.X), set the upgrade binary as the new current binary (v0.28.0), and then restart the node.\\\\\n\n### Option 2: Upgrade Without Cosmovisor\n\nUsing Cosmovisor to perform the upgrade is not mandatory.\n\nNode operators also have the option to manually update the `akash` binary at the time of the upgrade. Doing it before the upgrade height will stop the node.\n\nWhen the chain halts at the proposed upgrade height, stop the current process running `akash`.\n\nEither download the [Akash binary](#upgrade-details) or build from source - completed in this [step](#common-steps-for-all-upgrade-options) - and ensure the `akash` binary has been updated:\n\n```\nakash version\n```\n\nUpdate configuration with\n\n```\nakash init\n```\n\nRestart the process running `akash`.\n\n### Appendix\n\n#### Build Binary From Sources\n\n##### Prerequisites\n> _**NOTE**_ - we highly recommend downloading a complied Akash binary over building the binary from source\n\nDirenv is required to compile sources. Check [here](https://direnv.net) for details on how to install and hook to your shell\n\nif using direnv first time (or cloning sources on to a new host) you should see following message after entering source dir:\n```shell\ndirenv: error .envrc is blocked. Run `direnv allow` to approve its content\n```\nif no such message, most like direnv is not hooked to the shell\n\n\n##### Build\n\n```shell\ngit clone --depth 1 --branch v0.28.2 https://github.com/akash-network/node\ncd node\ndirenv allow\nmake release\n```\n\nAfter build artefacts are located in `dist` directory\n\n## Mainnet8 Provider Upgrade Procedure\n\n### Overview\n\nThis is a comprehensive guide that covers the steps necessary to upgrade from Mainnet5 to Mainnet6 of Akash Network and Akash Provider components in a Kubernetes cluster.\n\n### IMPORTANT\n\n#### This procedure does not apply to Praetor providers!\n\nPraetor providers should wait for further upgrade instructions from Praetor team!\n\n### Provider Components to be Upgraded\n\n* `provider-services` is the main binary of the `akash-provider`, `akash-hostname-operator`, `akash-inventory-operator`, and `akash-ip-operator` helm charts\n* `akash` is the main binary of the `akash-node` helm chart\n\n#### Mainnet6 Versions (Pre-Upgrade)\n\n* `provider-services`: `0.4.7`\n* `node`: `0.26.0`\n\n#### Mainnet8 Versions (Post-Upgrade)\n\n* `provider-services`: `0.4.8`\n* `node`: `0.28.2`\n\n### Upgrade Procedure\n\nIMPORTANT! Seek help if you encounter an issue at any step or have doubts! Please seek the support in the `#providers` Akash Network Devs Discord room [here](https://discord.akash.network/).\n\n#### STEP 1 - Scale down to 0 replicas the akash-provider\n\nThis step is crucial to prevent unexpected behavior during the upgrade.\n\n> _**NOTE**_: The Akash Deployments will continue to run as usual while `akash-provider` service is stopped. The only impact is that users won't be able to perform `lease-<shell|events|logs>` against their deployments nor deploy/update or terminate them.\n\n```\nkubectl -n akash-services scale statefulsets akash-provider --replicas=0\n```\n\n#### STEP 2 - Upgrade the Helm Charts\n\n> Follow these steps to upgrade various Helm charts. Make sure you've backed up your existing Helm chart configurations.\n\nHelm charts to be upgraded are: `akash-node` (aka RPC node), `akash-provider`, `akash-hostname-operator`, `akash-inventory-operator`, and `akash-ip-operator`.\n\n##### 2.1. Upgrade the Repo\n\n```\nhelm repo update akash\n```\n\nVerify you can see the correct chart/app versions:\n\n```\n# helm search repo akash\nNAME                          \tCHART VERSION\tAPP VERSION\tDESCRIPTION                                       \nakash/akash-hostname-operator \t8.0.0        \t0.4.8      \tAn operator to map Ingress objects to Akash dep...\nakash/akash-inventory-operator\t8.0.0        \t0.4.8      \tAn operator required for persistent storage (op...\nakash/akash-ip-operator       \t8.0.0        \t0.4.8      \tAn operator required for ip marketplace (optional)\nakash/akash-node              \t8.0.0        \t0.28.2     \tInstalls an Akash RPC node (required)\nakash/provider                \t8.0.0        \t0.4.8      \tInstalls an Akash provider (required)\n```\n\n##### 2.2. akash-node Chart\n\nTake the current `akash-node` chart values:\n\n```\nhelm -n akash-services get values akash-node | grep -v '^USER-SUPPLIED VALUES' > akash-node-values.yml\n```\n\nUpgrade your `akash-node` chart:\n\n```\nhelm upgrade akash-node akash/akash-node -n akash-services -f akash-node-values.yml\n```\n\n##### 2.3 akash-provider Chart\n\nTake the current `akash-provider` chart values:\n\n```\nhelm -n akash-services get values akash-provider | grep -v '^USER-SUPPLIED VALUES' > provider.yaml\n```\n\n```\nhelm upgrade akash-provider akash/provider -n akash-services -f provider.yaml\n```\n\n> _**IMPORTANT**_: Make sure your provider is using the latest bid price script! Here is the guide that tells you how you can set it for your akash-provider chart. [https://docs.akash.network/providers/build-a-cloud-provider/akash-cloud-provider-build-with-helm-charts/step-6-provider-bid-customization](https://docs.akash.network/providers/build-a-cloud-provider/akash-cloud-provider-build-with-helm-charts/step-6-provider-bid-customization)\n\n##### 2.4 akash-hostname-operator Chart\n\n```\nhelm upgrade akash-hostname-operator akash/akash-hostname-operator -n akash-services\n```\n\n##### 2.5 akash-inventory-operator Chart\n\n> Skip this section if your provider does not provide persistent storage.\n\n> _**Note**_: This is not a typo, we are installing the inventory-operator without the akash- prefix.\n\n```\nhelm upgrade inventory-operator akash/akash-inventory-operator -n akash-services\n```\n\n##### 2.6 akash-ip-operator Chart\n\n> Skip this section if your provider does not provide IP leasing.\n\nTake the current akash-ip-operator chart values:\n\n```\nhelm -n akash-services get values akash-ip-operator | grep -v '^USER-SUPPLIED VALUES' > akash-ip-operator-values.yml\n```\n\n```\nhelm upgrade akash-ip-operator akash/akash-ip-operator -n akash-services -f akash-ip-operator-values.yml\n```\n\n#### STEP 3 - Verify the Charts Have Been Upgraded\n\nPerform these checks to ensure the upgrade was successful.\n\n\\\nRun this command to check the pods and their versions within the `akash-services` namespace:\n\n```\nkubectl -n akash-services get pods -o custom-columns='NAME:.metadata.name,IMAGE:.spec.containers[*].image'\n```\n\nThe charts upgrade went well, if you are seeing these images and versions:\n\n* provider and operator image is: `ghcr.io/akash-network/provider:0.4.8`\n* node image is: `ghcr.io/akash-network/node:0.28.2`\n\nExample Result:\n\n```\n# kubectl -n akash-services get pods -o custom-columns='NAME:.metadata.name,IMAGE:.spec.containers[*].image'\nNAME                                        IMAGE\nakash-hostname-operator-86d4596d6c-pwbt8    ghcr.io/akash-network/provider:0.4.8\nakash-inventory-operator-69464fbdff-dxkk5   ghcr.io/akash-network/provider:0.4.8\nakash-ip-operator-6f6ddc47f8-498kj          ghcr.io/akash-network/provider:0.4.8\nakash-node-1-0                              ghcr.io/akash-network/node:0.28.2\nakash-provider-0                            ghcr.io/akash-network/provider:0.4.8\n```\n\n### Update your provider-services client\n\nMake sure to update your local `provider-services` binary (aka the Akash client) if you had it installed.\n\nVerify the version and the location using these two commands:\n\n```\nprovider-services version\ntype -p provider-services\n```\n\nIf you get anything below `0.4.8`, then get the right binary from [this page](https://github.com/akash-network/provider/releases/tag/v0.4.8).","description":null,"slug":"docs/other-resources/akash-mainnet8-upgrade"},{"title":"Authentication","body":"\n\n\nFor tenant it is important to send manifest to the right provider as well as for provider to ensure only owners can access their deployments. Thus each account must [create](#manage-certificates) certificate prior deploying workload or starting the provider.\n\n**Note** In this guide `--from` is referring to the key `main` which has been previously created with `akash key add`. Consider changing to the name of yours.\n\n```yaml\n- name: main\n  type: local\n  address: akash1gp3scyd8aye3z8szf3mpqzgsg4csyplcqehxus\n  pubkey: akashpub1addwnpepq0np6xltudgnau39046qtty3k46gzd482884hqcfxvzpyf2ttnr8ue3hc55\n  mnemonic: \"\"\n  threshold: 0\n  pubkeys: []\n```\n\n## Manage certificates\n\nBy default certificate is valid for 365 days from the moment of issuing\n\n### Create\n\n#### Client (aka tenant) certificate\n\n```\nprovider-services tx cert generate client --from $AKASH_KEY_NAME\n\nprovider-services tx cert publish client --from $AKASH_KEY_NAME\n```\n\n#### Provider certificate\n\nIt is important for provider to list same domain(s) as hostURI in provider attributes For example if `HostURI: https://example.com` the `example.com` must be listed as one of the domains in the certificate\n\n```\n#provider-services tx cert create server [list of domains provider is serving on] --from=main\nprovider-services tx cert create server example.com example1.com --from=main\n```\n\nLocally certificates and it's respective private key are stored in single file in akash home directory. The name of the file is stated as `<address>.pem`. For example certificate created with key `main` the file will be named as `akash1gp3scyd8aye3z8szf3mpqzgsg4csyplcqehxus.pem`\n\nIf file already exists user will be prompted to check if certificate already present on chain:\n\n* certificate is **not** on chain: user is prompted whether to commit or to leave as is\n* certificate is on chain: user prompted to revoke it or leave as is\n\nTo create certificate without being prompted use `--rie` flag (revoke if exists)\n\n#### Custom expiration dates\n\nUse following flags to set custom period of validity\n\n* `naf`: valid not after. value either number of days with `d` suffix `364d` or RFC3339 formatted timestamp\n* `nbf`: valid not before. value must be RFC3339 formatted timestamp\n\n**Note** flags above are valid for both [client](#client-aka-tenant-certificate) and [server](#provider-certificate) certificates\n\n**example1**\n\ncertificate valid for 180days after issuing\n\n```\nprovider-services tx cert generate client --from=main --naf=180d\n```\n\n**example2**\n\ncertificate valid for 180days after date of start\n\n```\nprovider-services tx cert generate client --from=main --naf=\"2022-03-19T18:35:03-04:00\" --naf=180d\n```\n\n**example3**\n\ncertificate valid for 365days after date of start\n\n```\nprovider-services tx cert generate client --from=main --naf=\"2022-03-19T18:35:03-04:00\"\n```\n\n### Revoke\n\n```\nprovider-services tx cert revoke --from=main\n```\n\n```\nprovider-services tx cert revoke --from=main --serial=<serial #>\n```\n\n## Query\n\nTo query certificates for particular account\n\n```\nprovider-services query cert list --owner=\"$(akash keys show main -a)\"\n```\n\nTo filter by state\n\n```\nprovider-services query cert list --owner=\"$(akash keys show main -a)\" --state=valid\nprovider-services query cert list --owner=\"$(akash keys show main -a)\" --state=revoked\n```","description":null,"slug":"docs/other-resources/authentication"},{"title":"Containers","body":"\n\n\nThe **Akash Container Platform** is a deployment platform for hosting and managing [containers](broken-reference) where users can run _**any**_ Cloud-Native application. Akash is built with a set of cloud management services including [Kubernetes](https://kubernetes.io) to orchestrate and manage containers.\n\n### Containers\n\nA **container** is a standard unit of software that packages up code and all its dependencies so the application runs quickly and reliably from one computing environment to another. A **container** **image** is a lightweight, standalone, executable package of software that includes everything needed to run an application: code, runtime, system tools, system libraries, and settings.\n\n**Container images** become **containers** at runtime. Available for both Linux and Windows-based applications, containerized software will always run the same, regardless of the infrastructure. Containers isolate software from its environment and ensure that it works uniformly despite differences for instance between development and staging.","description":null,"slug":"docs/other-resources/containers"},{"title":"Experimental","body":"\n","description":null,"slug":"docs/other-resources/experimental"},{"title":"Marketplace","body":"\n\nContents:\n\n* [Overview](#overview)\n* [Payments](#payments)\n* [On-Chain Parameters](#on-chain-parameters)\n* [Transactions](#transactions)\n* [Models](#models)\n\n## How does the Marketplace work?\n\nThe Akash Marketplace revolves around [Deployments](#deployment), which fully describe the resources that a tenant is requesting from the network. [Deployments](#deployment) contain [Groups](#group), which is a grouping of resources that are meant to be leased together from a single provider.\n\nDeploying applications onto [Akash](https://github.com/akash-network/node) involves two types of users:\n\n1. The **Tenant**: the entity that deploys the application.\n2. The **Provider**: the entity that hosts the application.\n\n### What is a Reverse Auction?\n\nAkash uses a reverse auction. Tenants set the price and terms of their deployment, and the Cloud providers bid on the deployments.\n\nIn a very simple reverse auction:\n\n1. A tenant creates orders.\n2. Providers bid on orders.\n3. Tenants choose winning bids and create leases.\n\nA typical application deployment on Akash will follow this flow:\n\n1. The tenant describes their desired deployment in \\[SDL], called a [deployment](#deployment).\n2. The tenant submits that definition to the blockchain.\n3. Their submission generates an [order](#order) on the marketplace.\n4. Providers that would like to fulfill that order [bid](#bid) on it.\n5. After some period of time, a winning [bid](#bid) for the [order](#order) is chosen, and a [lease](#lease) is created.\n6. Once a [lease](#lease) has been created, the tenant submits a [manifest](/docs/docs/getting-started/stack-definition-language/) to the provider.\n7. The provider executes workloads as instructed by the [manifest](/docs/docs/getting-started/stack-definition-language/).\n8. The workload is running - if it is a web application it can be visited\n9. The provider or tenant eventually closes the [lease](#lease), shutting down the workload.\n\nThe general workflow is:\n\n1. A tenant creates orders.\n2. Providers bid on orders.\n3. Tenants choose winning bids and create leases.\n\n### Lifecycle of a Deployment\n\nThe lifecycle of a typical application deployment is as follows:\n\n1. The tenant describes their desired deployment in \\[SDL], called a [deployment](#deployment).\n2. The tenant submits that definition to the blockchain.\n3. Their submission generates an [order](#order) on the marketplace.\n4. Providers that would like to fulfill that order [bid](#bid) on it.\n5. After some period of time, a winning [bid](#bid) for the [order](#order) is chosen, and a [lease](#lease) is created.\n6. Once a [lease](#lease) has been created, the tenant submits a [manifest](/docs/docs/getting-started/stack-definition-language/) to the provider.\n7. The provider executes workloads as instructed by the [manifest](/docs/docs/getting-started/stack-definition-language/).\n8. The workload is running - if it is a web application it can be visited\n9. The provider or tenant eventually closes the [lease](#lease), shutting down the workload.\n\n## Payments\n\nLeases are paid from deployment owner (tenant) to the provider through a deposit & withdraw mechanism.\n\nTenants are required to submit a deposit when creating a deployment. Leases will be paid passively from the balance of this deposit. At any time, a lease provider may withdraw the balance owed to them from this deposit.\n\nIf the available funds in the deposit ever reaches zero, a provider may close the lease. A tenant can add funds to their deposit at any time. When a deployment is closed, the unspent portion of the balance will be returned to the tenant.\n\n### Escrow Accounts\n\n[Escrow accounts](/docs/docs/other-resources/payments/) are a mechanism that allow for time-based payments from one account to another without block-by-block micropayments. They also support holding funds for an account until an arbitrary event occurs.\n\nEscrow accounts are necessary in akash for two primary reasons:\n\n1. Leases in Akash are priced in blocks - every new block, a payment from the tenant (deployment owner) to the provider (lease holder) is due. Performance and security considerations prohibit the naive approach of transferring tokens on every block.\n2. Bidding on an order should not be free (for various reasons, including performance and security). Akash requires a deposit for every bid. The deposit is returned to the bidder when the bid is closed.\n\n## Bid Deposits\n\nBidding on an order requires a deposit to be made. The deposit will be returned to the provider account when the [bid](#bid) transitions to state `CLOSED`.\n\nBid deposits are implemented with an escrow account module. See [here](/docs/docs/other-resources/payments/) for more information.\n\n## Audited Attributes\n\nAudited attributes allow users deploying applications to be more selective about which providers can run their apps. Anyone on the Akash Network can assign these attributes to Providers via an on-chain transaction.\n\n## On-Chain Parameters\n\n| Name                     | Initial Value | Description                         |\n| ------------------------ | ------------- | ----------------------------------- |\n| `deployment_min_deposit` | `5akt`        | Minimum deposit to make deployments |\n| `bid_min_deposit`        | `5akt`        | Deposit amount required to bid      |\n\n## Transactions\n\n### `DeploymentCreate`\n\nCreates a [deployment](#deployment), and open [groups](#group) and [orders](#order) for it.\n\n#### Parameters\n\n| Name            | Description                                                    |\n| --------------- | -------------------------------------------------------------- |\n| `DeploymentID`  | ID of Deployment.                                              |\n| `DepositAmount` | Deposit amount. Must be greater than `deployment_min_deposit`. |\n| `Version`       | Hash of the manifest that is sent to the providers.            |\n| `Groups`        | A list of [group](#group) descriptons.           |\n\n### `DeploymentDeposit`\n\nAdd funds to a deployment's balance.\n\n#### Parameters\n\n| Name            | Description                                                   |\n| --------------- | ------------------------------------------------------------- |\n| `DeploymentID`  | ID of Deployment.                                             |\n| `DepositAmount` | Deposit amount. Must be greater than `deployment_min_deposit` |\n\n### `GroupClose`\n\nCloses a [group](#group) and any [orders](#order) for it. Sent by the tenant.\n\n#### Parameters\n\n| Name | Description  |\n| ---- | ------------ |\n| `ID` | ID of Group. |\n\n### `GroupPause`\n\nPuts a `PAUSED` state, and closes any and [orders](#order) for it. Sent by the tenant.\n\n#### Parameters\n\n| Name | Description  |\n| ---- | ------------ |\n| `ID` | ID of Group. |\n\n### `GroupStart`\n\nTransitions a [group](#group) from state `PAUSED` to state `OPEN`. Sent by the tenant.\n\n#### Parameters\n\n| Name | Description  |\n| ---- | ------------ |\n| `ID` | ID of Group. |\n\n### `BidCreate`\n\nSent by a provider to bid on an open [order](#order). The required deposit will be returned when the bid transitions to state `CLOSED`.\n\n#### Parameters\n\n| name      | description                                 |\n| --------- | ------------------------------------------- |\n| `OrderID` | ID of Order                                 |\n| `TTL`     | Number of blocks this bid is valid for      |\n| `Deposit` | Deposit amount. `bid_min_deposit` if empty. |\n\n### `BidClose`\n\nSent by provider to close a bid or a lease from an existing bid.\n\nWhen closing a lease, the bid's group will be put in state `PAUSED`.\n\n#### Parameters\n\n| name    | description |\n| ------- | ----------- |\n| `BidID` | ID of Bid   |\n\n#### State Transitions\n\n| Object | Previous State | New State |\n| ------ | -------------- | --------- |\n| Bid    | `ACTIVE`       | `CLOSED`  |\n| Lease  | `ACTIVE`       | `CLOSED`  |\n| Order  | `ACTIVE`       | `CLOSED`  |\n| Group  | `OPEN`         | `PAUSED`  |\n\n### `LeaseCreate`\n\nSent by tenant to create a lease.\n\n1. Creates a `Lease` from the given [bid](#bid).\n2. Sets all non-winning [bids](#bid) to state `CLOSED` (deposit returned).\n\n#### Parameters\n\n| name    | description                                      |\n| ------- | ------------------------------------------------ |\n| `BidID` | [Bid](#bid) to create a lease from |\n\n### `MarketWithdraw`\n\nThis withdraws balances earned by providing for leases and deposits of bids that have expired.\n\n#### Parameters\n\n| name    | description                        |\n| ------- | ---------------------------------- |\n| `Owner` | Provider ID to withdraw funds for. |\n\n## Models\n\n### Deployment\n\n| Name       | Description                                                                         |\n| ---------- | ----------------------------------------------------------------------------------- |\n| `ID.Owner` | account addres of tenant                                                            |\n| `ID.DSeq`  | Arbitrary sequence number that identifies the deployment. Defaults to block height. |\n| `State`    | State of the deployment.                                                            |\n| `Version`  | Hash of the manifest that is sent to the providers.                                 |\n\n#### State\n\n| Name     | Description                      |\n| -------- | -------------------------------- |\n| `OPEN`   | Orders may be created.           |\n| `CLOSED` | All groups are closed. Terminal. |\n\n### Group\n\n| Name              | Description                                                         |\n| ----------------- | ------------------------------------------------------------------- |\n| `ID.DeploymentID` | [Deployment](#deployment) ID of group.                |\n| `ID.GSeq`         | Arbitrary sequence number. Internally incremented, starting at `1`. |\n| `State`           | State of the group.                                                 |\n\n#### State\n\n| Name     | Description                               |\n| -------- | ----------------------------------------- |\n| `OPEN`   | Has an open or active order.              |\n| `PAUSED` | Bid closed by provider. May be restarted. |\n| `CLOSED` | No open or active orders. Terminal.       |\n\n### Order\n\n| Name         | Description                                                         |\n| ------------ | ------------------------------------------------------------------- |\n| `ID.GroupID` | [Group](#group) ID of group.                          |\n| `ID.OSeq`    | Arbitrary sequence number. Internally incremented, starting at `1`. |\n| `State`      | State of the order.                                                 |\n\n#### State\n\n| Name     | Description                                        |\n| -------- | -------------------------------------------------- |\n| `OPEN`   | Accepting bids.                                    |\n| `ACTIVE` | Open lease has been created.                       |\n| `CLOSED` | No active leases and not accepting bids. Terminal. |\n\n### Bid\n\n| Name          | Description                                                |\n| ------------- | ---------------------------------------------------------- |\n| `ID.OrderID`  | [Group](#group) ID of group.                 |\n| `ID.Provider` | Account address of provider.                               |\n| `State`       | State of the bid.                                          |\n| `EndsOn`      | Height at which the bid ends if it is not already matched. |\n| `Price`       | Bid price - amount to be paid on every block.              |\n\n#### State\n\n| Name     | Description                              |\n| -------- | ---------------------------------------- |\n| `OPEN`   | Awaiting matching.                       |\n| `ACTIVE` | Bid for an active lease (winner).        |\n| `CLOSED` | No active leases for this bid. Terminal. |\n\n### Lease\n\n| Name    | Description                                                 |\n| ------- | ----------------------------------------------------------- |\n| `ID`    | The same as the [bid](#bid) ID for the lease. |\n| `State` | State of the bid.                                           |\n\n#### State\n\n| Name     | Description                                              |\n| -------- | -------------------------------------------------------- |\n| `ACTIVE` | Active lease - tenant is paying provider on every block. |\n| `CLOSED` | No payments being made. Terminal.                        |","description":null,"slug":"docs/other-resources/marketplace"},{"title":"Payments","body":"\n\n\nLeases are paid from deployment owner (tenant) to the provider through a [deposit](escrow.md#bid-deposits) & withdraw mechanism.\n\n### Bid Deposits\n\nTenants are required to submit a deposit when creating a deployment. Leases will be paid passively from the balance of this deposit. At any time, a lease provider may withdraw the balance owed to them from this deposit.\n\nIf the available funds in the deposit ever reaches zero, a provider may close the lease. A tenant can add funds to their deposit at any time. When a deployment is closed, the unspent portion of the balance will be returned to the tenant.\n\nBidding on an order requires a deposit to be made. The deposit will be returned to the provider account when the [bid](marketplace.md#bid) transitions to state `CLOSED`.\n\nBid deposits are implemented with an escrow account module. See [here](escrow.md) for more information.\n\n## Escrow Accounts\n\nEscrow accounts are a mechanism that allow for time-based payments from one account to another without block-by-block micropayments. They also support holding funds for an account until an arbitrary event occurs.\n\nEscrow accounts are necessary in akash for two primary reasons:\n\n1.  Leases in Akash are priced in blocks - every new block, a payment\n\n    from the tenant (deployment owner) to the provider (lease holder)\n\n    is due. Performance and security considerations prohibit the\n\n    naive approach of transferring tokens on every block.\n2. Bidding on an order should not be free (for various reasons, including performance and security). Akash requires a deposit for every bid. The deposit is returned to the bidder when the bid is closed.\n\nEscrow [accounts](escrow.md#account) are created with an arbitrary ID, an owner, and a balance. The balance is immediately transferred from the owner account to the escrow module [account](escrow.md#account). [Accounts](escrow.md#account) may have their balance increased by being deposited to after creation.\n\n[Payments](escrow.md#payment) represent transfers from the escrow account to another account. They are (currently) block-based - some amount is meant to be transferred for every block. The amount owed to the payment from the escrow [account](escrow.md#account) is subtracted from the escrow [account](escrow.md#account) balance through a settlement process.\n\n[Payments](escrow.md#payment) may be withdrawn from, which transfers any undisbursed balance from the module account to the payment owner's account.\n\nWhen an [account](escrow.md#account) or a [payment](escrow.md#payment) is closed, any remaining balance will be transferred to their respective owner accounts.\n\nIf at any time the amount owed to [payments](escrow.md#payment) is greater than the remaining balance of the [account](escrow.md#account), the account and all payments are closed with state `OVERDRAWN`.\n\nMany actions invoke the settlement process and may cause the account to become overdrawn.\n\n**How are gas fees calculated on Akash?**\n\nAkash uses the basic Cosmos gas calculations for all fees. Cosmos documentation on gas can be found [here](https://docs.cosmos.network/master/basics/gas-fees.html).\n\n## Account Settlement\n\nAccount settlement is the process of updating internal accounting of the balances of [payments](escrow.md#payment) for an [account](escrow.md#account) to the current height.\n\nMany actions trigger the account settlement process - it ensures an up-to-date ledger when acting on the [account](escrow.md#account).\n\nAccount settlement goes as follows:\n\n1. Determine `blockRate` - the amount owed for every block.\n2. Determine `heightDelta` - the number of blocks since last settlement.\n3. Determine `numFullBlocks` - the number of blocks that can be paid for in full.\n4. Transfer amount for `numFullBlocks` to [payments](escrow.md#payment).\n5. If `numFullBlocks` is less than `heightDelta` ([account](escrow.md#account) overdrawn), then\n   1. Distribute remaining balance among [payments](escrow.md#payment), weighted by each [payment](escrow.md#payment)'s `rate`\n   2. Distribute any remaining balance from above as evenly as possible\n   3. Set [account](escrow.md#account) and [payments](escrow.md#payment) to state `OVERDRAWN`.\n\n## Models\n\n### Account\n\n| Field         | Description                                 |\n| ------------- | ------------------------------------------- |\n| `ID`          | Unique ID of account.                       |\n| `Owner`       | Account address of owner.                   |\n| `State`       | Account state.                              |\n| `Balance`     | Amount deposited from owner account.        |\n| `Transferred` | Amount disbursed from account via payments. |\n| `SettledAt`   | Last block that payments were settled.      |\n\n#### Account State\n\n| Name        |\n| ----------- |\n| `OPEN`      |\n| `CLOSED`    |\n| `OVERDRAWN` |\n\n### Payment\n\n| Field       | Description                               |\n| ----------- | ----------------------------------------- |\n| `AccountID` | Escrow [`Account`](escrow.md#account) ID. |\n| `PaymentID` | Unique (over `AccountID`) ID of payment.  |\n| `Owner`     | Account address of owner.                 |\n| `State`     | Payment state.                            |\n| `Rate`      | Tokens per block to transfer.             |\n| `Balance`   | Balance currently reserved for owner.     |\n| `Withdrawn` | Amount already withdrawn by owner.        |\n\n#### Payment State\n\n| Name        |\n| ----------- |\n| `OPEN`      |\n| `CLOSED`    |\n| `OVERDRAWN` |\n\n## Methods\n\n### `AccountCreate`\n\nCreate an escrow account. Funds are deposited from the owner account to the escrow module account.\n\n#### Arguments\n\n| Field     | Description                          |\n| --------- | ------------------------------------ |\n| `ID`      | Unique ID of account.                |\n| `Owner`   | Account address of owner.            |\n| `Deposit` | Amount deposited from owner account. |\n\n### `AccountDeposit`\n\nAdd funds to an escrow account. Funds are transferred from the owner account to the escrow module account.\n\n#### Arguments\n\n| Field    | Description                          |\n| -------- | ------------------------------------ |\n| `ID`     | Unique ID of account.                |\n| `Amount` | Amount deposited from owner account. |\n\n### `AccountSettle`\n\nRe-calculate remaining account and payment balances.\n\n#### Arguments\n\n| Field | Description           |\n| ----- | --------------------- |\n| `ID`  | Unique ID of account. |\n\n### `AccountClose`\n\nClose account - settle and close payments, return remaining account balance to owner account.\n\n#### Arguments\n\n| Field | Description           |\n| ----- | --------------------- |\n| `ID`  | Unique ID of account. |\n\n### `PaymentCreate`\n\nCreate a new payment. The account will first be settled; this method will fail if the account cannot be settled.\n\n#### Arguments\n\n| Field       | Description                               |\n| ----------- | ----------------------------------------- |\n| `AccountID` | Escrow [`Account`](escrow.md#account) ID. |\n| `PaymentID` | Unique (over `AccountID`) ID of payment.  |\n| `Owner`     | Account address of owner.                 |\n| `Rate`      | Tokens per block to transfer.             |\n\n#### Invariants\n\n* Account is in state `OPEN` after being settled.\n* `ID` is unique.\n* `Owner` exists.\n* `Rate` is non-zero and account has funds for one block.\n\n### `PaymentWithdraw`\n\nWithdraw funds from a payment balance. Account will first be settled.\n\n#### Arguments\n\n| Field       | Description                               |\n| ----------- | ----------------------------------------- |\n| `AccountID` | Escrow [`Account`](escrow.md#account) ID. |\n| `PaymentID` | Unique (over `AccountID`) ID of payment.  |\n\n### `PaymentClose`\n\nClose a payment. Account will first be settled.\n\n#### Arguments\n\n| Field       | Description                               |\n| ----------- | ----------------------------------------- |\n| `AccountID` | Escrow [`Account`](escrow.md#account) ID. |\n| `PaymentID` | Unique (over `AccountID`) ID of payment.  |\n\n## Hooks\n\nHooks are callbacks that are registered by users of the escrow module that are to be called on specific events.\n\n### `OnAccountClosed`\n\nWhenever an account is closed `OnAccountClosed(Account)` will be called.\n\n### `OnPaymentClosed`\n\nWhenever a payment is closed, `OnAccountClosed(Account)` will be called.","description":null,"slug":"docs/other-resources/payments"},{"title":"Security","body":"\n\n### How does **Security** work on Akash?\n\nWith Akash, you decide **who** you want to trust.\n\n### How does Akash authenticate users?\n\nIt is important for the tenant to send their manifest to the correct provider, and for the provider to ensure only valid owners can access their deployments. This authentication is implemented with [mTLS](mtls.md) and involves each account creating a certificate prior to deploying a workload or starting a provider.\n\nDefault certificate lifespan is 365 days from the moment of issuance. This can be customized to be valid up to a certain date, or not valid until a certain date.\n\n### **How do I limit my trust to Audited Providers?**\n\nFollow the getting started guide, and you will see the [instructions for audited attributes](https://docs.akash.network/guides/deploy#audited-attributes) suggest using only servers **\"signed by\"** Akash Network. If you deploy today, you will see bids by Equinix servers that audited and signed by Akash Network. By doing this you are trusting [Equinix’s Security Standards and Compliance](https://www.equinix.com/data-centers/design/standards-compliance) and you are trusting Overclock Labs as the auditor to only sign servers that meet those standards.\n\n### **What are Audited Attributes?**\n\nAkash has a feature designed to allow you to _control_ your trust settings called **Audited Attributes**. Audited attributes allow users deploying applications to be more selective about which providers can run their apps. Anyone on the Akash Network can assign these attributes to Providers via an on-chain transaction.\n\nAkash's Stack Definition Language (SDL) allows you to define attributes such as the type of provider, region, CPU, Memory, Storage, and which auditors you want to trust. When you deploy on Akash, you can configure any attribute that restricts bids to only providers that meet your criteria.\n\nAuditors on the Akash Network review cloud providers and digitally sign the provider on-chain with their certificate. If you only accept bids from audited providers this means you are trusting the Auditor/Provider not just a Provider.\n\nOn the `akashnet-2` network, to ensure tenants have smooth and reliable service from their provider, it is recommended to use the following audited attributes in their deployment: \\_\\_\n\n```bash\n    attributes:\n        host: akash\n      signedBy:\n       anyOf:\n        - \"akash1365yvmc4s7awdyj3n2sav7xfx76adc6dnmlx63\"\n```\n\n\\--or--\n\n```bash\n       attributes:\n        datacenter: equinix-metal-ewr1\n      signedBy:\n       anyOf:\n        - \"akash1365yvmc4s7awdyj3n2sav7xfx76adc6dnmlx63\"\n```\n\nPlease note that all of the following can be substituted in the `datacenter` field above and should be chosen based on your needs:\n\n| Datacenter           | Location                  |\n| -------------------- | ------------------------- |\n| `equinix-metal-ewr1` | New Jersey, United States |\n| `equinix-metal-sjc1` | California, United States |\n\n## Create a Certificate\n\nBefore you can create a deployment, a [certificate](#mtls) must first be created. While an account may have several certificates associated, when using the Akash CLI a single cert may be used across many/all deployments. If using both the Cloudmos Deploy and Akash CLI, a single account may be used across those platforms with a separate certificate create and used per platform.  To do this, run:\n\n```\nakash tx cert create client --chain-id $AKASH_CHAIN_ID --keyring-backend $AKASH_KEYRING_BACKEND --from $AKASH_KEY_NAME --node $AKASH_NODE --fees 5000uakt\n```\n\nYou should see a response similar to:\n\n```javascript\n{\n  \"body\": {\n    \"messages\": [\n      {\n        \"@type\": \"/akash.cert.v1beta1.MsgCreateCertificate\",\n        \"owner\": \"akash1vns5ka3x69ekm3ecp8my8d5zfu8j23p5qew0w3\",\n        \"cert\": \"LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUJ3RENDQVdXZ0F3SUJBZ0lJRm1pcUJWcWZDVmt3Q2dZSUtvWkl6ajBFQXdJd1NqRTFNRE1HQTFVRUF4TXMKWVd0aGMyZ3hkbTV6Tld0aE0zZzJPV1ZyYlRObFkzQTRiWGs0WkRWNlpuVTRhakl6Y0RWeFpYY3dkek14RVRBUApCZ1ZuZ1FVQ0JoTUdkakF1TUM0eE1CNFhEVEl4TURNd01qSXpNak15TmxvWERUSXlNRE13TWpJek1qTXlObG93ClNqRTFNRE1HQTFVRUF4TXNZV3RoYzJneGRtNXpOV3RoTTNnMk9XVnJiVE5sWTNBNGJYazRaRFY2Wm5VNGFqSXoKY0RWeFpYY3dkek14RVRBUEJnVm5nUVVDQmhNR2RqQXVNQzR4TUZrd0V3WUhLb1pJemowQ0FRWUlLb1pJemowRApBUWNEUWdBRUtaSTlmWGVPVzRCYXRwcU1mb1VTekx2b01lWGlpbEZTMnJhZlhKdUNObUlMVjJMaWhIZW5JdjJTCjV5Uzh1Zkh5QmNMSUI5aFE1VE81THRHSUpPdzIvYU0xTURNd0RnWURWUjBQQVFIL0JBUURBZ1F3TUJNR0ExVWQKSlFRTU1Bb0dDQ3NHQVFVRkJ3TUNNQXdHQTFVZEV3RUIvd1FDTUFBd0NnWUlLb1pJemowRUF3SURTUUF3UmdJaApBSjJzQ3ZodGNzWkRXUkQ2MU03ZkVCRUk5eEt5Z0UzRkd3K2tIYVhZYXl0TUFpRUE4cUZtb3FEc1Z0ZzhPSHc1Ck5iOEljd0hiNHVkc0RpTzRxaWhoL0owNWZKaz0KLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo=\",\n        \"pubkey\": \"LS0tLS1CRUdJTiBFQyBQVUJMSUMgS0VZLS0tLS0KTUZrd0V3WUhLb1pJemowQ0FRWUlLb1pJemowREFRY0RRZ0FFS1pJOWZYZU9XNEJhdHBxTWZvVVN6THZvTWVYaQppbEZTMnJhZlhKdUNObUlMVjJMaWhIZW5JdjJTNXlTOHVmSHlCY0xJQjloUTVUTzVMdEdJSk93Mi9RPT0KLS0tLS1FTkQgRUMgUFVCTElDIEtFWS0tLS0tCg==\"\n      }\n    ],\n    \"memo\": \"\",\n    \"timeout_height\": \"0\",\n    \"extension_options\": [],\n    \"non_critical_extension_options\": []\n  },\n  \"auth_info\": {\n    \"signer_infos\": [],\n    \"fee\": {\n      \"amount\": [],\n      \"gas_limit\": \"200000\",\n      \"payer\": \"\",\n      \"granter\": \"\"\n    }\n  },\n  \"signatures\": []\n}\n```","description":null,"slug":"docs/other-resources/security"},{"title":"Akash Audited Attributes","body":"\n\nAudited Attributes allow a user to filter which providers can bid on their project. In the use of a Stack Definition Language (SDL) file Audited Attributes enable:\n\n* Deployers to declare mandatory provider attributes, meaning leases will only be generated on platforms that fulfill the developers needs.&#x20;\n* Deployers can declare the address of an independent auditor who has validated the provider attributes.\n\nIn this guide we will show detailed examples of Audited Attribute usage and best practises.\n\nDeployer usage of Audited Attributes within SDL files is the focus of this document. The mechanics involved in becoming an auditor and auditing provider resources will be covered in separate, provider centric documentation.\n\n## Sample SDL Deployment\n\nWe will use the following sample SDL deployment file throughout this guide to explore the use of attributes and auditors.\n\nThe SDL example will provision two light-weight, redundant web servers using the Lunie Light container image.\n\n```\nversion: \"2.0\"\nservices:\n web:\n   image: ovrclk/lunie-light\n   expose:\n     - port: 3000\n       as: 80\n       to:\n         - global: true\nprofiles:\n compute:\n   web:\n     resources:\n       cpu:\n         units: 0.1\n       memory:\n         size: 512Mi\n       storage:\n         size: 512Mi\n placement:\n   ewr1-provider0:\n     attributes:\n       host: akash\n       datacenter: equinix-metal-ewr1\n     signedBy:\n       anyOf:\n         - akash1365yvmc4s7awdyj3n2sav7xfx76adc6dnmlx63\n     pricing:\n       web:\n         denom: uakt\n         amount: 100\n   sjc1-provider0:\n     attributes:\n       host: akash\n       datacenter: equinix-metal-sjc1\n     signedBy:\n       anyOf:\n         - akash1365yvmc4s7awdyj3n2sav7xfx76adc6dnmlx63\n     pricing:\n       web:\n         denom: uakt\n         amount: 100\ndeployment:\n web:\n   ewr1-provider0:\n     profile: web\n     count: 1\n   sjc1-provider0:\n     profile: web\n     count: 1\n```\n\n## SDL Attributes\n\n### Attribute Location Within the SDL\n\nUsing the example SDL provided previously, let’s take a look at the profiles section.\n\n```\nprofiles:\n compute:\n   web:\n     resources:\n       cpu:\n         units: 0.1\n       memory:\n         size: 512Mi\n       storage:\n         size: 512Mi\n placement:\n   ewr1-provider0:\n     attributes:\n       host: akash\n       datacenter: equinix-metal-ewr1\n     signedBy:\n       anyOf:\n         - akash1365yvmc4s7awdyj3n2sav7xfx76adc6dnmlx63\n     pricing:\n       web:\n         denom: uakt\n         amount: 100\n```\n\nFrom the isolated SDL section above observe the following from the top down:\n\n1. Attributes are declared within the “profiles” section. The “profiles” section defines compute and placement requirements.&#x20;\n2. The “attributes” key is placed within the “placement” section.&#x20;\n3. Within the example SDL two unique “attributes” are defined:\n   * host: akash&#x20;\n   * datacenter: equinix-metal-ewr1\n\n### Attribute Syntax\n\nReferring to the “profiles” SDL section we saw previously - note the following details of “attributes” syntax:\n\n* Attributes are declared key-value pairs with the key and value both strings.&#x20;\n* The attribute key-value pairs could be any values but must match the attributes declared by desired providers.&#x20;\n* Providers analyze the submitted SDL files and will only bid on projects for which they satisfy the attribute list.\n\nAn example attribute would be a location key-value pair which could be used to ensure the deployment is hosted in a specific physical region such as locale: west.\n\n### Attribute Impact\n\nWhen a user creates a deployment that includes attributes they are ensuring that only providers which have matching attributes are eligible to host their workloads.  Referencing the example SDL file and the isolated placement section - the only eligible providers would be those which have declared:\n\n* The host attribute of value akash&#x20;\n* The datacenter attribute of value equinix-metal-ewr1\n\n### Attribute Additional Notes\n\n_**No Attribute/Attributes Defined**_\n\nAttributes are an optional parameter in the Akash SDL. A valid manifest may include no attribute section/key.\n\n_**Attribute Governance**_\n\nNo governance is available to ensure that the attributes declared by the provider are actual or valid. In theory any provider may declare they satisfy the “host: akash” condition and send bids to any user requiring that attribute. Verification of the attributes to ensure a provider satisfies the claim of the attribute is the role of an auditor and is the subject of the subsequent section.\n\n## Attribute Auditors\n\nProvider auditors assume the role of governance within SDL attributes use.  An Auditors job is to ensure a provider has accurate attributes placed on them and can  fulfill such requirements.\n\n### Standard Attributes\n\nA comprehensive, open-source framework is in development which will further decentralize the auditing of providers.  Pending the release of this framework, an interim process has been developed to confirm providers claiming Akash recognized attributes.  The process will allow providers to bid on deployments launched via the SDLs within the Awesome Akash repository.  Interested providers may initiate the interim signing process by making contact in Discord with Andy (member of the Akash Developer Insider team) at the following handle: @andy01.\n\n_**Attributes of Community Providers**_\n\nCommunity providers vetted by Andy should advertise the following attributes before they can be signed by Akash address `akash1365yvmc4s7awdyj3n2sav7xfx76adc6dnmlx63`.  Leases should use these attributes to allow audited community providers to bid.\n\n* host: akash\n* tier: community\n\n_**Attributes of Akash Providers**_\n\nThe Akash provider, compute resources owned by Akash, will advertise the following attributes.  Use of these attributes ensures only Akash provider instances will bid on the lease.\n\n* host: akash\n* tier: premium\n* organization: ovrclk.com\n\n_**Awesome Akash Examples**_\n\nThe example applications in the Awesome Akash repo have the following attributes included.  Based on these attributes, both Akash and community providers are able to bid on associated leases.\n\n* host: akash\n\n_**Lease signedBy Fields**_\n\nTo use audited providers only, ensure SDLs have the Akash signedBy address of: akash1365yvmc4s7awdyj3n2sav7xfx76adc6dnmlx63.\n\n### Auditor Location Within the SDL\n\nUsing the example SDL provided previously let’s take a look at the Auditor review.\n\n```\nprofiles:\n compute:\n   web:\n     resources:\n       cpu:\n         units: 0.1\n       memory:\n         size: 512Mi\n       storage:\n         size: 512Mi\n placement:\n   ewr1-provider0:\n     attributes:\n       host: akash\n       datacenter: equinix-metal-ewr1\n     signedBy:\n       anyOf:\n         - akash1365yvmc4s7awdyj3n2sav7xfx76adc6dnmlx63\n     pricing:\n       web:\n         denom: uakt\n         amount: 100\n```\n\nFrom the isolated SDL section above we can see the following:\n\n1. Required auditors are declared within the “profiles” section. The “profiles” section defines compute and placement requirements.&#x20;\n2. Within a specific placement profile (“ewr1-provider0” in the example SDL) there is a signedBy key.\n\n### Declared Auditor Syntax\n\nReferring to the “profiles” SDL section looked at previously - note the following details of the “signedBy” syntax:\n\n* Required auditors may be specified within the signedBy section via either of the following options:\n  * [ ] anyOf\n    * The “anyOf” key states that if multiple auditors are declared in the YAML list - confirmation is satisfied if ONE auditor in the list is able to confirm necessary attributes.\n  * [ ] allOf\n    * The “allOf” key states that if multiple auditors are declared in the YAML list - confirmation is satisfied only if ALL auditors in the list are able to confirm necessary attributes.\n\nIn the modified SDL manifest depicted below the second placement (“sjc1-provider0”) has been edited to demonstrate the use and syntax of the “allOf” specification within the signedBy section.\n\n```\nprofiles:\n compute:\n   web:\n     resources:\n       cpu:\n         units: 0.1\n       memory:\n         size: 512Mi\n       storage:\n         size: 512Mi\n placement:\n   ewr1-provider0:\n     attributes:\n       host: akash\n       datacenter: equinix-metal-ewr1\n     signedBy:\n       anyOf:\n         - akash1365yvmc4s7awdyj3n2sav7xfx76adc6dnmlx63\n     pricing:\n       web:\n         denom: uakt\n         amount: 100\n   sjc1-provider0:\n     attributes:\n       host: akash\n       datacenter: equinix-metal-sjc1\n     signedBy:\n       allOf:\n         - akash1365yvmc4s7awdyj3n2sav7xfx76adc6dnmlx63\n         - akash1ss3ty253h6yun0a0fly8s0prcx34x4q2qewpkk\n     pricing:\n       web:\n         denom: uakt\n         amount: 100\n```\n\n### Auditor Impact\n\nProvider auditors play the role of governance within SDL attribute use.\n\nUsing the sample SDL section below should help clarify the concept and use of the auditor role_._\n\n```\nprofiles:\n compute:\n   web:\n     resources:\n       cpu:\n         units: 0.1\n       memory:\n         size: 512Mi\n       storage:\n         size: 512Mi\n placement:\n   ewr1-provider0:\n     attributes:\n       host: akash\n       datacenter: equinix-metal-ewr1\n     signedBy:\n       anyOf:\n         - akash1365yvmc4s7awdyj3n2sav7xfx76adc6dnmlx63\n     pricing:\n       web:\n         denom: uakt\n         amount: 100\n```\n\n_**Auditor Role**_\n\nProvider auditors play the role of governance within SDL attribute use.\n\nUsing the sample SDL section below should help clarify the concept and use of the auditor role_._\n\n```\nprofiles:\n compute:\n   web:\n     resources:\n       cpu:\n         units: 0.1\n       memory:\n         size: 512Mi\n       storage:\n         size: 512Mi\n placement:\n   ewr1-provider0:\n     attributes:\n       host: akash\n       datacenter: equinix-metal-ewr1\n     signedBy:\n       anyOf:\n         - akash1365yvmc4s7awdyj3n2sav7xfx76adc6dnmlx63\n     pricing:\n       web:\n         denom: uakt\n         amount: 100\n```\n\n_**Auditor Role**_\n\nIn traditional data centers, third-party auditors often validate physical security and environmental mandates. In the Akash landscape third-party validators play a similar role in ensuring a provider is capable of the attributes they ascribe to.\n\nIn this role the auditor will perform the following function:\n\n* Audit the attributes claimed by the provider and validate the claims/capabilities are legit.&#x20;\n* With introduction of the auditor and use of the signedBy key the danger of a provider declaring an attribute incorrectly or maliciously is removed.\n\n_**Auditor/Signed By Key Functionality**_\n\n* Should the manifest declare both attributes and a signedBy key - only a provider that has been validated by the declared auditor/auditors to have such attributes would become eligible to host the compute resource.&#x20;\n* As noted in the Declared Auditor Syntax section:\n  * [ ] Inclusion of the signedBy key with a nested allOf key mandates validation by all listed auditors.&#x20;\n  * [ ] Inclusion of the signedBy key with a nested anyOf key is satisfied with the validation by one listed auditor.\n\n_**Auditor Trust Relationship**_\n\n* The auditor concept assumes a trust relationship with the defined auditor. For example, in the sample SDL the auditor account declared in the signedBy field (address = “akash1365yvmc4s7awdyj3n2sav7xfx76adc6dnmlx63”) is owned by Akash. Through the use of the signedBy field and the declaration of the Akash auditor address, only Akash audited providers and those validated to possess declared attributes would be eligible to host manifest resources.\n* In a circumstance in which the auditor is a third party (not Akash),no process exists to ensure that the auditor conducted accurate provider attribute validation and thus only trusted auditors should be utilized.\n* An Akash signBy address may be used when assurance that Akash has validated a provider’s attributes is desired.\n\n### Auditor Additional Notes\n\n_**No Auditor/Signed By Specification**_\n\nA SDL is considered valid with no signedBy section. Both the attributes and signedBy sections are optional.\n\nWhen a SDL includes attributes but no signedBy key-value pair only providers declaring such attributes will select the deployment for bid but there would be no third party validation that the provider truly meets such specifications.\n\nIf a deployer desired access to every provider currently on the Akash network both the attributes and signedBy sections could be removed from the SDL. **NOTE**: Akash cannot be responsible for deployments on providers that are not audited specifically by an Akash auditor.\n\n_**Multiple Placements with Differing Attributes/Auditors**_\n\nMultiple placements may exist in a SDL and each placement may have unique attributes/signedBy specifications.\n\nIn an example scenario, a SDL containing multiple micro-services (i.e. web frontend and database backend), it would be possible to dictate that the frontend exist in one locale and the backend exist in another locale via differing values in a locale attribute key-value pair.","description":null,"slug":"docs/providers/akash-audites-atributes"},{"title":"Build a Cloud Provider","body":"\n\nBecome an Akash provider and make profit on your spare compute made available for tenant lease.\n\nOur curated guide to building your decentralized provider follows these sequential steps.\n\n* **STEP 1** - [Build the Kubernetes Cluster for your Provider](#kubernetes-cluster-for-akash-providers)\n* **STEP 2** - [Build your Cloud Provider via Helm Charts](#akash-cloud-provider-build-with-helm-charts)\n* **STEP 3** - [Complete a CheckUp of your New Provider](#akash-provider-checkup)\n* **STEP 4** - [Enable GPU Resources on your Cloud Provider](#step-4)\n* **STEP 5** - [Enable Persistent Storage on your Cloud Provider](helm-based-provider-persistent-storage-enablement/)\n* **STEP 6** - [Provider Maintenance](../akash-provider-troubleshooting/provider-maintenance.md)\n* **STEP 7** - [Provider Troubleshooting](../akash-provider-troubleshooting/)\n* **STEP 8** - [What's Next (Provider Care and Maintenance)](../akash-provider-troubleshooting/maintaining-and-rotating-kubernetes-etcd-certificates-a-how-to-guide.md)\n\n#### OPTIONAL STEPS\n\n> _**NOTE**_ - the following features are not requirements of an Akash Provider but the enablement of such features may increase demand and tenant lease activity.  Consider adding such functionality to your Akash Provider either in initial build or at a later stage.\n\n* [TLS Certs for Akash Providers](tls-certs-for-akash-provider-optional-step/)\n* [IP Leases](ip-leases-provider-enablement-optional-step/)\n* [Persistent Storage](helm-based-provider-persistent-storage-enablement/persistent-storage-requirements.md)\n* [Akash Provider GPU Resource Enablement](../../other-resources/experimental/build-a-cloud-provider/gpu-resource-enablement-optional-step/)\n\n#### VERIFICATIONS AND TROUBLESHOOTING\n\n> _**NOTE**_ - utilize these guides post Akash Provider build to ensure the provider is fully operational and during troubleshooting/maintenance needs.\n\n* [Akash Provider Checkup](akash-provider-checkup/)\n* [Akash Provider Maintenance/FAQ/Troubleshooting](../akash-provider-troubleshooting/)\n\n## Helm Chart Repository for Akash Providers\n\nThe steps outlined above will guide the user through the creation of an Akash Provider and necessary components.  The table below clarifies required and optional Helm Charts in the Provider build process.\n\nThe Akash Helm Chart repository can be accessed [here](https://github.com/akash-network/helm-charts).\n\n<table><thead><tr><th>Chart Name</th><th>Description</th><th data-hidden></th></tr></thead><tbody><tr><td>akash-provider</td><td>Installs an Akash provider (required)</td><td></td></tr><tr><td>akash-hostname-operator</td><td>An operator to map Ingress objects to Akash deployments (required)</td><td></td></tr><tr><td>akash-node</td><td>Installs an Akash RPC node (required)</td><td></td></tr><tr><td>akash-inventory-operator</td><td>An operator required for persistent storage (optional)</td><td></td></tr><tr><td>akash-ip-operator</td><td>An operator required for ip marketplace (optional)</td><td></td></tr></tbody></table>\n\n\n# Kubernetes Cluster for Akash Providers\n\n## **Overview**\n\nAkash leases are deployed as Kubernetes pods on provider clusters.  This guide details the build of the provider’s Kubernetes control plane and worker nodes.\n\nThe setup of a Kubernetes cluster is the responsibility of the provider. This guide provides best practices and recommendations for setting up a Kubernetes cluster. This document is not a comprehensive guide and assumes pre-existing Kubernetes knowledge.\n\nThe Kubernetes Cluster created is then ready for the Akash Provider build detailed [here](../akash-cloud-provider-build-with-helm-charts/).\n\n## Prerequisites\n\nThe Kubernetes instructions in this guide are intended for audiences that have the following skills sets and knowledge.\n\n* **Server Administration Skills** - necessary for setting up servers/network making up the Kubernetes cluster\n* **Kubernetes Experience** - a base level of Kubernetes administration is highly recommended\n\n> Please consider using the [Praetor](../../community-solutions/praetor.md) application to build an Akash Provider for small and medium sized environments which require little customization.\n\n## Guide Sections\n\n* [Clone the Kubespray Project](step-1-clone-the-kubespray-project.md)\n* [Install Ansible](step-2-install-ansible.md)\n* [Ansible Access to Kubernetes Cluster](step-3-ansible-access-to-kubernetes-cluster.md)\n* [Ansible Inventory](step-4-ansible-inventory.md)\n* [Additional Verifications](step-5-enable-gvisor.md)\n* [DNS Configuration](step-6-dns-configuration.md)\n* [Provider Ephemeral Storage Config](step-6-provider-ephemeral-storage-config.md)\n* [Create Kubernetes Cluster](step-6-create-kubernetes-cluster.md)\n* [Confirm Kubernetes Cluster](step-7-confirm-kubernetes-cluster.md)\n* [Custom Kernel Parameters](step-10-custom-kernel-parameters.md)\n* [Review Firewall Policies](step-9-review-firewall-policies.md)\n\n# STEP 1 - Clone the Kubespray Project\n\n## Cluster Recommendations\n\nWe recommend using the Kubespray project to deploy a cluster. Kubespray uses Ansible to make the deployment of a Kubernetes cluster easy.\n\nThe recommended minimum number of hosts is four for a production Provider Kubernetes cluster. This is meant to allow:\n\n* Three hosts serving as a redundant control plane (aka master)/etcd instances\n* One host to serve as Kubernetes worker node to host provider leases.\n\n### Additional Cluster Sizing Considerations\n\n* While a production Kubernetes cluster would typically require three redundant control plane nodes, in circumstances in which the control plane node is easily recoverable the use of a single control instance for Akash providers should suffice.\n\n* The number of control plane nodes in the cluster should always be an odd number to allow the cluster to reach consensus.\n\n* We recommend running a single worker node per physical server as CPU is typically the largest resource bottleneck. The use of a single worker node allows larger workloads to be deployed on your provider.\n\n* If you intended to build a provider with persistent storage please refer to host storage requirements detailed [here](../helm-based-provider-persistent-storage-enablement/persistent-storage-requirements.md).\n\n## Kubernetes Cluster Software/Hardware Requirements and Recommendations\n\n### Software Recommendation\n\nAkash Providers have been tested on **Ubuntu 22.04** with the default Linux kernel. Your experience may vary should install be attempted using a different Linux distro/kernel.\n\n### Kubernetes Control Plane Node Requirements\n\n* Minimum Specs\n  * 2 CPU\n  * 4 GB RAM\n  * 30 GB disk\n* Recommended Specs\n  * 4 CPU\n  * 8 GB RAM\n  * 40 GB disk\n\n### Kubernetes Worker Node Requirements\n\n* Minimum Specs\n  * 4 CPU\n  * 8 GB RAM\n  * 100 GB disk\n* Recommendations\n  * The more resources the better depending on your goal of maximum number of concurrent deployments.\n  * Especially important to note that worker node needs to have as much CPU as possible, because if it's got, say 8 CPU and, 100 GB RAM, and 2 TB disk -> the cpu would likely be a bottleneck. Since people tend to deploy at least 1 CPU per deployment, the server could only host 8 deployments maximum and likely about 6 deployments as other \\~2 CPU will be reserved by the Kubernetes system components.\n\n## **etcd Hardware Recommendations**\n\n* Use this [guide](https://etcd.io/docs/v3.5/op-guide/hardware) to ensure Kubernetes control plane nodes meet the recommendations for hosting a `etcd` database.\n\n## **Kubespray Clone**\n\nInstall Kubespray on a machine that has connectivity to the hosts that will serve as the Kubernetes cluster. Kubespray should not be installed on the Kubernetes hosts themselves but rather on a machine that has connectivity to the Kubernetes hosts.\n\n### Kubespray Host Recommendation\n\nWe recommend installing Kubespray on Ubuntu 22.04. Versions prior it Ubuntu 20.X may experience issues with recent Ansible versions specified in later steps.\n\n### Clone the Kubespray Project\n\nObtain Kubespray and navigate into the created local directory:\n\n```\ncd ~\n\ngit clone -b v2.23.1 --depth=1 https://github.com/kubernetes-sigs/kubespray.git\n\ncd kubespray\n\n# make sure to run the following sed command in order to address https://github.com/kubernetes-sigs/kubespray/issues/10688 \"The conditional check 'groups.get('kube_control_plane')' failed. The error was: Conditional is marked as unsafe, and cannot be evaluated.\" issue\n\nsed -i \"/- name: Stop if either kube_control_plane or kube_node group is empty/,/with_items:/s/that: \\\"groups.get('{{ item }}')\\\"/that: \\\"groups.get( item )\\\"/\" roles/kubernetes/preinstall/tasks/0040-verify-settings.yml\n```\n\n## Cluster Updates\n\nTo update the Kubernetes cluster in the future, review the[ latest Kubespray documentation](https://github.com/kubernetes-sigs/kubespray/blob/master/docs/upgrades.md) to take advantage of recent bug fixes and enhancements.\n\n\n# STEP 2 - Install Ansible\n\n\n\n> _**NOTE**_ - the commands in this section and in all remaining sections of this guide assume that the `root` user is used.  For ease we suggest using the `root` user for the Kubernetes and Akash Provider install.  If a non-root user is used instead, minor command adjustments may be necessary such as using `sudo` command prefixes and updating the home directory in command syntaxes.\n\nWhen you launch Kubespray it will use an Ansible playbook to deploy a Kubernetes cluster.  In this step we will install Ansible.\n\nDepending on your operating system it may be necessary to install OS patches, pip3, and virtualenv.  Example steps for a Ubuntu OS are detailed below.\n\n```\napt-get update ; apt-get install -y python3-pip virtualenv\n```\n\nWithin the kubespray directory use the following commands for the purpose of:\n\n* Opening a Python virtual environment for the Ansible install\n* Installing Ansible and other necessary packages specified in the requirements.txt file\n* Please remember to `cd kubespray` AND `source venv/bin/activate` - as detailed in the code block below - each time you want to use the  `ansible-playbook` commands in upcoming sections. &#x20;\n\n```\ncd ~/kubespray\n\nvirtualenv --python=python3 venv\n\nsource venv/bin/activate\n\npip3 install -r requirements.txt\n```\n\n# STEP 3 - Ansible Access to Kubernetes Cluster\n\nAnsible will configure the Kubernetes hosts via SSH.  The user Ansible connects with must be root or have the capability of escalating privileges to root.\n\nCommands in this step provide an example of SSH configuration and access to Kubernetes hosts and testing those connections.\n\n## Section Overview\n\nThe command sets provided in this section may be copied and pasted into your terminal without edit unless otherwise noted.\n\n## **Create SSH Keys on Ansible Host**\n\n* Accept the defaults to create a public-private key pair\n\n```\nssh-keygen -t rsa -C $(hostname) -f \"$HOME/.ssh/id_rsa\" -P \"\" ; cat ~/.ssh/id_rsa.pub\n```\n\n### **Confirm SSH Keys**\n\n* The keys will be stored in the user’s home directory\n* Use these commands to verify keys\n\n```\ncd ~/.ssh ; ls\n```\n\n#### **Example files created**\n\n```\nauthorized_keys  id_rsa  id_rsa.pub\n```\n\n## **Copy Public Key to the Kubernetes Hosts**\n\n### **Template**\n\n* Replace the username and IP address variables in the template with your own settings.  Refer to the Example for further clarification.\n\n```\nssh-copy-id -i ~/.ssh/id_rsa.pub <username>@<ip-address>\n```\n\n### **Example**\n\n* Conduct this step for every Kubernetes control plane and worker node in the cluster\n\n```\nssh-copy-id -i ~/.ssh/id_rsa.pub root@10.88.94.5\n```\n\n## **Confirm SSH to the Kubernetes Hosts**\n\n* Ansible should be able to access all Kubernetes hosts with no password\n\n### **Template**\n\n* Replace the username and IP address variables in the template with your own settings.  Refer to the Example for further clarification.\n\n```\nssh -i ~/.ssh/id_rsa <username>@<ip-address>\n```\n\n### **Example**\n\n* Conduct this access test for every Kubernetes control plane and worker node in the cluster\n\n```\nssh -i ~/.ssh/id_rsa root@10.88.94.5\n```\n\n# STEP 4 - Ansible Inventory\n\nAnsible will use an inventory file to determine the hosts Kubernetes should be installed on.\n\n## **Inventory File**\n\n* Use the following commands on the Ansible host and in the “kubespray” directory\n* Replace the IP addresses in the declare command with the addresses of your Kubernetes hosts (master/control-plane and worker nodes)\n* Running these commands will create a hosts.yaml file within the kubespray/inventory/akash directory\n* NOTE - ensure that you are still within the Python virtual environment when running these commands.  Your cursor should have a “(venv)” prefix.  If needed - re-enter the virtual environment by issuing:\n  * `source venv/bin/activate`\n\n```\ncd ~/kubespray\n\ncp -rfp inventory/sample inventory/akash\n\n#REPLACE IP ADDRESSES BELOW WITH YOUR KUBERNETES CLUSTER IP ADDRESSES\ndeclare -a IPS=(10.0.10.136 10.0.10.239 10.0.10.253 10.0.10.9)\n\nCONFIG_FILE=inventory/akash/hosts.yaml python3 contrib/inventory_builder/inventory.py ${IPS[@]}\n```\n\n### **Expected Result (Example)**\n\n```\n(venv) root@ip-10-0-10-145:/home/ubuntu/kubespray# CONFIG_FILE=inventory/akash/hosts.yaml python3 contrib/inventory_builder/inventory.py ${IPS[@]}\n\nDEBUG: Adding group all\nDEBUG: Adding group kube_control_plane\nDEBUG: Adding group kube_node\nDEBUG: Adding group etcd\nDEBUG: Adding group k8s_cluster\nDEBUG: Adding group calico_rr\nDEBUG: adding host node1 to group all\nDEBUG: adding host node2 to group all\nDEBUG: adding host node3 to group all\nDEBUG: adding host node4 to group all\nDEBUG: adding host node1 to group etcd\nDEBUG: adding host node2 to group etcd\nDEBUG: adding host node3 to group etcd\nDEBUG: adding host node1 to group kube_control_plane\nDEBUG: adding host node2 to group kube_control_plane\nDEBUG: adding host node3 to group kube_control_plane\n\nDEBUG: adding host node1 to group kube_node\nDEBUG: adding host node2 to group kube_node\nDEBUG: adding host node3 to group kube_node\nDEBUG: adding host node4 to group kube_node\n```\n\n### **Verification of Generated File**\n\n* Open the hosts.yaml file in VI (Visual Editor) or nano\n* Update the kube\\_control\\_plane category if needed with full list of hosts that should be master nodes\n* Ensure you have either 1 or 3 Kubernetes control plane nodes under `kube_control_plane`. If 2 are listed, change that to 1 or 3, depending on whether you want Kubernetes be Highly Available.\n* Ensure you have only control plane nodes listed under `etcd`.  If you would like to review additional best practices for etcd, please review this [guide](https://rafay.co/the-kubernetes-current/etcd-kubernetes-what-you-should-know/).\n* For additional details regarding `hosts.yaml` best practices and example configurations, review this [guide](additional-k8s-resources/kubespray-hosts.yaml-examples.md).\n\n```\nvi ~/kubespray/inventory/akash/hosts.yaml\n```\n\n#### **Example hosts.yaml File**\n\n* Additional hosts.yaml examples, based on different Kubernetes cluster topologies, may be found [here](additional-k8s-resources/kubespray-hosts.yaml-examples.md)\n\n```\nall:\n  hosts:\n    node1:\n      ansible_host: 10.0.10.136\n      ip: 10.0.10.136\n      access_ip: 10.0.10.136\n    node2:\n      ansible_host: 10.0.10.239\n      ip: 10.0.10.239\n      access_ip: 10.0.10.239\n    node3:\n      ansible_host: 10.0.10.253\n      ip: 10.0.10.253\n      access_ip: 10.0.10.253\n    node4:\n      ansible_host: 10.0.10.9\n      ip: 10.0.10.9\n      access_ip: 10.0.10.9\n  children:\n    kube_control_plane:\n      hosts:\n        node1:\n        node2:\n        node3:\n    kube_node:\n      hosts:\n        node1:\n        node2:\n        node3:\n        node4:\n    etcd:\n      hosts:\n        node1:\n        node2:\n        node3:\n    k8s_cluster:\n      children:\n        kube_control_plane:\n        kube_node:\n    calico_rr:\n      hosts: {}\n```\n\n## Manual Edits/Insertions of the hosts.yaml Inventory File\n\n* Open the hosts.yaml file in VI (Visual Editor) or nano\n\n```\nvi ~/kubespray/inventory/akash/hosts.yaml\n```\n\n* Within the YAML file’s “all” stanza and prior to the “hosts” sub-stanza level  - insert the following vars stanza\n\n```\nvars:\n  ansible_user: root\n```\n\n* The hosts.yaml file should look like this once finished\n\n```\nall:\n  vars:\n    ansible_user: root\n  hosts:\n    node1:\n      ansible_host: 10.0.10.136\n      ip: 10.0.10.136\n      access_ip: 10.0.10.136\n    node2:\n      ansible_host: 10.0.10.239\n      ip: 10.0.10.239\n      access_ip: 10.0.10.239\n    node3:\n      ansible_host: 10.0.10.253\n      ip: 10.0.10.253\n      access_ip: 10.0.10.253\n    node4:\n      ansible_host: 10.0.10.9\n      ip: 10.0.10.9\n      access_ip: 10.0.10.9\n  children:\n    kube_control_plane:\n      hosts:\n        node1:\n        node2:\n        node3:\n    kube_node:\n      hosts:\n        node1:\n        node2:\n        node3:\n        node4:\n    etcd:\n      hosts:\n        node1:\n        node2:\n        node3:\n    k8s_cluster:\n      children:\n        kube_control_plane:\n        kube_node:\n    calico_rr:\n      hosts: {}\n```\n\n## Additional Kubespray Documentation\n\nUse these resources for a more through understanding of Kubespray and for troubleshooting purposes\n\n* [Adding/replacing a node](https://github.com/kubernetes-sigs/kubespray/blob/9dfade5641a43c/docs/nodes.md)\n* [Upgrading Kubernetes in Kubespray](https://github.com/kubernetes-sigs/kubespray/blob/e9c89132485989/docs/upgrades.md)\n\n# STEP 5 - Additional Verifications/Config\n\nIn this section we will enable gVisor which provides basic container security.\n\n## Containerd Edit/Verification\n\n* Change into the directory of the config file\n\n```\ncd ~/kubespray/inventory/akash/group_vars/k8s_cluster\n```\n\n* Using VI or nano edit the k8s-cluster.yml file:\n\n```\nvi k8s-cluster.yml\n```\n\n* Add/update the container\\_manager key if necessary to containerd\n\n```\ncontainer_manager: containerd\n```\n\n## **gVisor Issue - No system-cgroup v2 Support**\n\n> Skip if you are not using gVisor\n\nIf you are using a newer systemd version,  your container will get stuck in ContainerCreating state on your provider with gVisor enabled. Please reference [this document](../gvisor-issue-no-system-cgroup-v2-support.md) for details regarding this issue and the recommended workaround.\n\n\n# STEP 6 - DNS Configuration\n\n## Upstream DNS Servers\n\nAdd  `upstream_dns_servers` in your Ansible inventory\n\n> _**NOTE**_ - the steps in this section should be conducted on the Kubespray host\n\n```\ncd ~/kubespray\n```\n\n### Verify Current Upstream DNS Server Config\n\n```\ngrep -A2 upstream_dns_servers inventory/akash/group_vars/all/all.yml\n```\n\n_**Expected/Example Output**_\n\n* Note that in the default configuration of a new Kubespray host the Upstream DNS Server settings are commented out via the `#` prefix.\n\n```\n#upstream_dns_servers:\n  #- 8.8.8.8\n  #- 8.8.4.4\n```\n\n### Update Upstream DNS Server Config\n\n```\nvim inventory/akash/group_vars/all/all.yml\n```\n\n* Uncomment the `upstream_dns_servers` and the public DNS server line entries.\n* When complete the associated lines should appears as:\n\n```\n## Upstream dns servers\nupstream_dns_servers:\n  - 8.8.8.8\n  - 8.8.4.4\n```\n\n---\ndescription: Configure provider to offer sufficient ephemeral storage\n---\n\n# STEP 7 - Provider Ephemeral Storage Config (OPTIONAL)\n\n## Overview\n\nEnsure that the provider is configured to offer more ephemeral storage than is available at the OS root partition.\n\nObjective of this guide - move /var/lib/kubelet (nodefs) and /var/lib/containerd (imagefs) onto the RAID0 NVME disk mounted over the /data directory.\n\n* _**nodefs**_: The node's main filesystem, used for local disk volumes, emptyDir, log storage, and more. For example - nodefs contains /var/lib/kubelet/.\n* _**imagefs**_: An optional filesystem that container runtimes use to store container images and container writable layers.\n\n## Ephemeral and Persistent Storage Considerations\n\nNotes to consider when planning your provider storage allocations:\n\n* Ephemeral storage is faster (in terms of IOPS) and persistent storage can be slower (in terms of IOPS).  This is due to network latency associated with persistent storage and as the storage nodes (or the pods to storage nodes) are connected over the network.\n* Some types of deployments - such as Chia workloads - do not need persistent storage and need just ephemeral storage\n\n## Observations\n\nStopping `kubelet` alone does not clear the `/var/lib/kubelet` open file handles locked by pods using it. Hence, `kubelet` should be disabled, node restarted.\n\n`kubectl drain` (& `kubectl uncordon` after) is not sufficient as `Ceph OSD` can't be evicted due to PDB (Pod's Disruption Budget).\n\nAssociated error when attempting to stop/start `kubelet`:\n\n```\nerror when evicting pods/\"rook-ceph-osd-60-5fb688f86b-9hzt2\" -n \"rook-ceph\" (will retry after 5s): Cannot evict pod as it would violate the pod's disruption budget.\n```\n\n```\n$ kubectl -n rook-ceph describe pdb\nName:             rook-ceph-osd\nNamespace:        rook-ceph\nMax unavailable:  1\nSelector:         app=rook-ceph-osd\nStatus:\n    Allowed disruptions:  0\n    Current:              119\n    Desired:              119\n    Total:                120\nEvents:                   <none>\n```\n\n## Recommended Steps\n\n### STEP 1 - Stop and disable `Kubelet` & `containerd`\n\n```\nsystemctl stop kubelet\nsystemctl disable kubelet\n\nsystemctl stop containerd\nsystemctl disable containerd\n```\n\n### STEP 2 - Reboot the node\n\n* Have to reboot the node so it will release the `/var/lib/kubelet` and `/var/lib/containerd`\n\n_**Verify**_\n\n```\nroot@k8s-node-1:~# lsof -Pn 2>/dev/null |grep -E '/var/lib/kubelet|/var/lib/containerd'\n# should be no response here, this will indicate /var/lib/kubelet and /var/lib/containerd are not used.\n```\n\n### STEP 3 - Find 2 free NVME disks\n\n```\nroot@k8s-node-0:~# lsblk\n...\nnvme0n1                                                                                               259:0    0   1.5T  0 disk \nnvme1n1                                                                                               259:1    0   1.5T  0 disk \n```\n\n### STEP 4 - Create RAID0 over 2 NVME\n\n```\nroot@k8s-node-0:~# mdadm --create /dev/md0 --level=raid0 --metadata=1.2 --raid-devices=2 /dev/nvme0n1 /dev/nvme1n1\nmdadm: array /dev/md0 started.\n```\n\n_**Verify:**_\n\n```\nroot@k8s-node-0:~# cat /proc/mdstat\nPersonalities : [linear] [multipath] [raid0] [raid1] [raid6] [raid5] [raid4] [raid10] \nmd0 : active raid0 nvme1n1[1] nvme0n1[0]\n      3125626880 blocks super 1.2 512k chunks\n      \nunused devices: <none>\n```\n\n### STEP 5 - Format /dev/md0\n\n```\nroot@k8s-node-0:~# mkfs.ext4 /dev/md0\n```\n\n### STEP 6 - Move old kubelet data\n\n```\nmv /var/lib/kubelet /var/lib/kubelet-backup\n```\n\n### STEP 7 - Update fstab with the new /dev/md0\n\n#### **Backup fstab**\n\n```\ncp -p /etc/fstab /etc/fstab.1\n```\n\n#### Update fstab\n\n> Remove ,discard after defaults if you are NOT using SSD/NVME disks!\n\n```\ncat >> /etc/fstab << EOF\nUUID=\"$(blkid /dev/md0 -s UUID -o value)\"  /data        ext4   defaults,discard  0 0\nEOF\n```\n\n#### Verify\n\n```\ndiff -Nur /etc/fstab.1 /etc/fstab\n```\n\n### STEP 8- Mount /dev/md0 as /data\n\n```\nmkdir /data\nmount /data\n```\n\n#### Verify\n\n```\nroot@k8s-node-0:~# df -Ph /data\nFilesystem      Size  Used Avail Use% Mounted on\n/dev/md0        2.9T   89M  2.8T   1% /data\n```\n\n### STEP 9 - Generate mdadm.conf so it gets detected on boot\n\n```\nroot@k8s-node-0:~# /usr/share/mdadm/mkconf > /etc/mdadm/mdadm.conf\n```\n\n#### Verify\n\n```\nroot@k8s-node-0:~# cat /etc/mdadm/mdadm.conf | grep -v ^\\#\n\n\nHOMEHOST <system>\n\nMAILADDR root\n\nARRAY /dev/md/0  metadata=1.2 UUID=a96501a3:955faf1e:06f8087d:503e8c36 name=k8s-node-0.mainnet-1.ca:0\n```\n\n### STEP 10 - Regenerate initramfs so the new mdadm.conf gets there\n\n```\nroot@k8s-node-0:~# update-initramfs -c -k all\n```\n\n### STEP 11 - Move kubelet data onto RAID0\n\n```\nmv /var/lib/kubelet-backup /data/kubelet\n```\n\n#### Verify\n\n```\nroot@k8s-node-0:~# df -Ph /data\nFilesystem      Size  Used Avail Use% Mounted on\n/dev/md0        2.9T   43G  2.7T   2% /data\n```\n\n### STEP 12 - Move the containerd to the new path\n\n```\nmv /var/lib/containerd /data/containerd\n```\n\n### STEP 13 - kubespray the cluster with the following config\n\n* This configuration entry should be made in the file of `group_vars/k8s_cluster/k8s-cluster.yml` on the Kubespray host\n\n```\ncontainerd_storage_dir: \"/data/containerd\"\nkubelet_custom_flags:\n  \"--root-dir=/data/kubelet\"\n```\n\n### STEP 14 - Start and enable containerd\n\n```\nsystemctl start containerd\nsystemctl enable containerd\n```\n\n### STEP 15 - Start and enable kubelet\n\n```\nsystemctl start kubelet\nsystemctl enable kubelet\n```\n\n#### Verify\n\n```\njournalctl -u kubelet -f\n```\n\n### STEP 16 - Wait until all pods are Running\n\n```\nkubectl get pods -A -o wide | grep <your node>\n```\n\n## Make applications aware of the new nodefs location\n\n> If you aren't using `rook-ceph` / `velero`, then skip this section\n\n### Ceph's rook-ceph-operator\n\n```\ncsi:\n  kubeletDirPath: /data/kubelet\n```\n\n### Velero restic\n\n> velero helm-chart config\n\n```\nrestic:\n  podVolumePath: /data/kubelet/pods\n```\n\n# STEP 8 - Create Kubernetes Cluster\n\n## Create Cluster\n\nWith inventory in place we are ready to build the Kubernetes cluster via Ansible.\n\n* Note - the cluster creation may take several minutes to complete\n* If the Kubespray process fails or is interpreted, run the Ansible playbook again and it will complete any incomplete steps on the subsequent run\n\n> _**NOTE**_ - if you intend to enable GPU resources on your provider - consider completing this [step](../../../other-resources/experimental/build-a-cloud-provider/gpu-resource-enablement-optional-step/gpu-provider-configuration.md) now to avoid having to run Kubespray on multiple occasions.  Only the `NVIDIA Runtime Configuration` section of the `GPU Resource Enablement` guide should be completed at this time and then return to this guide/step.\n\n```\ncd ~/kubespray\n\nansible-playbook -i inventory/akash/hosts.yaml -b -v --private-key=~/.ssh/id_rsa cluster.yml\n```\n\n# STEP 9 - Confirm Kubernetes Cluster\n\nA couple of quick Kubernetes cluster checks are in order before moving into next steps.\n\n## SSH into Kubernetes Master Node\n\n* The verifications in this section must be completed on a master node with kubectl access to the cluster.\n\n## Confirm Kubernetes Nodes\n\n```\nkubectl get nodes\n```\n\n### **Example output from a healthy Kubernetes cluster**\n\n```\nroot@node1:/home/ubuntu# kubectl get nodes\n\nNAME    STATUS   ROLES                  AGE     VERSION\nnode1   Ready    control-plane,master   5m48s   v1.22.5\nnode2   Ready    control-plane,master   5m22s   v1.22.5\nnode3   Ready    control-plane,master   5m12s   v1.22.5\nnode4   Ready    <none>                 4m7s    v1.22.5\n```\n\n## **Confirm Kubernetes Pods**\n\n```\nkubectl get pods -n kube-system\n```\n\n### Example output of the pods that are the brains of the cluster\n\n```\nroot@node1:/home/ubuntu# kubectl get pods -n kube-system\n\nNAME                                      READY   STATUS    RESTARTS        AGE\ncalico-kube-controllers-5788f6558-mzm64   1/1     Running   1 (4m53s ago)   4m54s\ncalico-node-2g4pr                         1/1     Running   0               5m29s\ncalico-node-6hrj4                         1/1     Running   0               5m29s\ncalico-node-9dqc4                         1/1     Running   0               5m29s\ncalico-node-zt8ls                         1/1     Running   0               5m29s\ncoredns-8474476ff8-9sgm5                  1/1     Running   0               4m32s\ncoredns-8474476ff8-x67xd                  1/1     Running   0               4m27s\ndns-autoscaler-5ffdc7f89d-lnpmm           1/1     Running   0               4m28s\nkube-apiserver-node1                      1/1     Running   1               7m30s\nkube-apiserver-node2                      1/1     Running   1               7m13s\nkube-apiserver-node3                      1/1     Running   1               7m3s\nkube-controller-manager-node1             1/1     Running   1               7m30s\nkube-controller-manager-node2             1/1     Running   1               7m13s\nkube-controller-manager-node3             1/1     Running   1               7m3s\nkube-proxy-75s7d                          1/1     Running   0               5m56s\nkube-proxy-kpxtm                          1/1     Running   0               5m56s\nkube-proxy-stgwd                          1/1     Running   0               5m56s\nkube-proxy-vndvs                          1/1     Running   0               5m56s\nkube-scheduler-node1                      1/1     Running   1               7m37s\nkube-scheduler-node2                      1/1     Running   1               7m13s\nkube-scheduler-node3                      1/1     Running   1               7m3s\nnginx-proxy-node4                         1/1     Running   0               5m58s\nnodelocaldns-7znkj                        1/1     Running   0               4m28s\nnodelocaldns-g8dqm                        1/1     Running   0               4m27s\nnodelocaldns-gf58m                        1/1     Running   0               4m28s\nnodelocaldns-n88fj                        1/1     Running   0               4m28s\n```\n\n## Confirm DNS\n\n### Verify CoreDNS Config\n\n> This is to verify that Kubespray properly set the expected upstream servers in the DNS Configuration previous step\n\n```\nkubectl -n kube-system get cm coredns -o yaml | grep forward\n```\n\n### Verify All DNS Related Pods Are in a Running State\n\n```\nkubectl -n kube-system get pods -l k8s-app=kube-dns\nkubectl -n kube-system get pods -l k8s-app=nodelocaldns\n```\n\nWith kubespray version >= `2.22.x`:\n\n```\nkubectl -n kube-system get pods -l k8s-app=node-local-dns\n```\n\n## Verify etcd Status and Health\n\n> &#x20;Commands should be run on the control plane node to ensure health of the Kubernetes `etcd` database\n\n```\nexport $(grep -v '^#' /etc/etcd.env | xargs -d '\\n')\netcdctl -w table member list\netcdctl endpoint health --cluster -w table\netcdctl endpoint status --cluster -w table\netcdctl check perf\n```\n\n\n# STEP 10 - Custom Kernel Parameters\n\n## Create and apply custom kernel parameters\n\nApply these settings to ALL Kubernetes worker nodes to guard against `too many open files` errors.\n\n### Create Config\n\n```\ncat > /etc/sysctl.d/90-akash.conf << EOF\n# Common: tackle \"failed to create fsnotify watcher: too many open files\"\nfs.inotify.max_user_instances = 512\nfs.inotify.max_user_watches = 1048576\n\n# Custom: increase memory mapped files limit to allow Solana node\n# https://docs.solana.com/running-validator/validator-start\nvm.max_map_count = 1000000\nEOF\n```\n\n### Apply Config\n\n```\nsysctl -p /etc/sysctl.d/90-akash.conf\n```\n\n# STEP 11 - Review Firewall Policies\n\nIf local firewall instances are running on Kubernetes control-plane and worker nodes, add the following policies.\n\n## Kubernetes Port List\n\nIn this step we will cover common Kubernetes ports that need to be opened for cross server communications.  For an exhaustive and constantly updated reference, please use the following list published by the Kubernetes developers.\n\n* [Exhaustive list of Kubernetes Ports](https://kubernetes.io/docs/reference/ports-and-protocols/)\n\n## **Etcd Key Value Store Policies**\n\nEnsure the following ports are open in between all Kubernetes etcd instances:\n\n```\n- 2379/tcp for client requests; (Kubernetes control plane to etcd)\n- 2380/tcp for peer communication; (etcd to etcd communication)\n```\n\n## **API Server Policies**\n\n\n\nEnsure the following ports are open in between all Kubernetes API server instances:\n\n```\n- 6443/tcp - Kubernetes API server\n```\n\n## Worker Node Policies\n\nEnsure the following ports are open in between all Kubernetes worker nodes:\n\n```\n- 10250/tcp - Kubelet API server; (Kubernetes control plane to kubelet)\n```\n\n# Additional K8s Resources\n\n* [Kubespray Hosts.Yaml Examples](kubespray-hosts.yaml-examples.md)\n\n\n# Kubespray Hosts.Yaml Examples\n\n## Hosts.Yaml Overview\n\nThe Kubespray hosts.yaml inventory file is composed of 3 groups:\n\n* **kube\\_node**: list of Kubernetes nodes where the pods will run.\n* **kube\\_control\\_plane**: list of servers where Kubernetes control plane components (apiserver, scheduler, controller) will run.\n* **etcd**: list of servers to compose the etcd server. You should have at least 3 servers for failover purpose.\n\nPlease following these links for YAML examples and depending on your preferred topology:\n\n* [All-In-One Node](kubespray-hosts.yaml-examples.md#all-in-one-node)\n* [One Control Plane Node with Multiple Worker Nodes](kubespray-hosts.yaml-examples.md#one-control-plane-node-with-multiple-worker-nodes)\n* [Multiple Control Plane Nodes with Multiple Work Nodes](kubespray-hosts.yaml-examples.md#multiple-control-plane-nodes-with-multiple-work-nodes)\n\n## All-In-One Node\n\n### Topology\n\n* node1 - is a single control plane + etcd node\n* node1 - is also running the pods\n\n### &#x20;Pros\n\n* Easy to manage\n\n### Cons\n\n* Single point of failure for K8s/etcd/pods;\n* Thinner security barrier since pods are running on control plane / etcd nodes;\n\n### Example Hosts.yaml File\n\n```\n  children:\n    kube_control_plane:\n      hosts:\n        node1:\n    kube_node:\n      hosts:\n        node1:\n    etcd:\n      hosts:\n        node1:\n    k8s_cluster:\n      children:\n        kube_control_plane:\n        kube_node:\n    calico_rr:\n      hosts: {}\n```\n\n## One Control Plane Node with Multiple Worker Nodes\n\n### Topology\n\n* node1 - single control plane + etcd node\n* node2..N - kube nodes where the pods will run\n\n### Pros\n\n* Better security barrier since pods aren't running on control plane / etcd nodes\n* Can scale by adding either more control plane nodes or worker nodes\n\n### Cons\n\n* Single point of failure only for K8s/etcd but not the pods\n\n### Example Hosts.yaml File\n\n```\n children:\n    kube_control_plane:\n      hosts:\n        node1:\n    kube_node:\n      hosts:\n        node2:\n        node3:\n    etcd:\n      hosts:\n        node1:\n    k8s_cluster:\n      children:\n        kube_control_plane:\n        kube_node:\n    calico_rr:\n      hosts: {}\n```\n\n## Multiple Control Plane Nodes with Multiple Work Nodes\n\n### Topology\n\n* Nodes 1.-3 -  the control plane + etcd nodes; (This makes K8s High Available)\n* Node 4.-N -  the kube nodes on which the Pods will run\n\n### Pros\n\n* Highly available control plane / etcd\n* Better security barrier since pods aren't running on control plane / etcd nodes\n* Can scale by adding either more control plane nodes or worker nodes\n\n### Cons\n\n* More complex environment makes its configuration & management more difficult\n\n### Example Hosts.yaml File\n\n```\n  children:\n    kube_control_plane:\n      hosts:\n        node1:\n        node2:\n        node3:\n    kube_node:\n      hosts:\n        node4:\n        node5:\n        node6:\n    etcd:\n      hosts:\n        node1:\n        node2:\n        node3:\n    k8s_cluster:\n      children:\n        kube_control_plane:\n        kube_node:\n    calico_rr:\n      hosts: {}\n```","description":null,"slug":"docs/providers/build-a-cloud-provider"},{"title":"Community Solutions","body":"\nThe Akash community drives innovation in Provider tooling.  Our highlighted community contributions include:\n\n* [Praetor](#praetor) - allows users with limited Kubernetes experience to fully build a provider in a curated series of steps within a user friendly web app\n* [AkashDash](#akashdash---provider-earnings-portal) - offers Akash providers an instantaneous view of profitability metrics both on a per lease basis and in summation across all current/active leases.\n\n## Praetor\n\nThe Praetor application offers a streamlined mechanism to deploy an Akash Provider.  The web app allows the creation of the necessary Kubernetes cluster and the Provider instance all from within an extremely user friendly series of guided steps.\n\nIn addition to the Praetor Provider build app, the team has created great tools for viewing current Akash Providers and a Provider revenue calculator.\n\n* [Praetor Documentation](https://docs.praetorapp.com/)\n* [Praetor Provider Build App](https://akash.praetorapp.com/auth/login)\n* [Praetor Provider Revenue Calculator](https://akash.praetorapp.com/calculator)\n* [Praetor Current Akash Provider Report](https://akash.praetorapp.com/provider-status)\n\n\n## AkashDash - Provider Earnings Portal\n\n### Overview\n\nThe AkashDash web app offers Akash Providers an overview of expected and realized profitability both on a per deployment/lease basis and holistically across active leases.\n\n### AkashDash Access\n\nThe AkashDash app can be accessed [here](https://akashdash.com/).\n\n### AkashDash Support\n\nShould any support be needed in the usage of the AkashDash app, please use the [Akash Discord server](https://discord.akash.network/) and flag user `[cryptoandcoffee.com]#5657` who is the creator and maintainer of this tool.&#x20;\n\n### Example Provider Dashboard\n\n\n\n![](../../../assets/akashdash.png)","description":null,"slug":"docs/providers/community-solutions"},{"title":"Custom Kubernetes Cluster Settings","body":"\n\nThis document details necessary settings in custom Kubernetes environments for use as an Akash Provider\n\n\n# VMware Tanzu\n\n## Kube-System Namespace Labeling\n\n* Make sure kube-system namespace has kubernetes.io/metadata.name: kube-system label\n\n```\nkubectl label namespace kube-system kubernetes.io/metadata.name=kube-system\n```\n\nWhy? This is required by the akash-deployment-restrictions Network Policy to allow Pods access the kube-dns.","description":null,"slug":"docs/providers/custom-kubernetes-cluster-settings"},{"title":"Akash Provider FAQ and Guide","body":"\n\nUse the techniques detailed in this guide to verify Akash Provider functionality and troubleshoot issues as they appear.\n\nThe guide is broken down into the following categories:\n\n* [Provider Maintenance](#provider-maintenance)\n* [How to terminate the workload from the Akash Provider using CLI](#how-to-terminate-the-workload-from-the-akash-provider-using-cli)\n* [Provider Logs](#provider-logs)\n* [Provider Status and General Info](#provider-status-and-general-info)\n* [Provider Lease Management](#provider-lease-management)\n* [Provider Manifests](#provider-manifests)\n* [Provider Earnings](#provider-earnings)\n* [Dangling Deployments](#dangling-deployments)\n* [Heal Broken Deployment Replicas by Returning Lost command to Manifests](#heal-broken-deployment-replicas-by-returning-lost-command-to-manifests)\n* [Maintaining and Rotating Kubernetes/etcd Certificates: A How-To Guide](#maintaining-and-rotating-kubernetes-etcd-certificates-a-how-to-guide)\n* [Force New ReplicaSet Workaround](#force-new-replicaset-workaround)\n* [Kill Zombie Processes](#kill-zombie-processes)\n* [Close Leases Based on Image](#close-leases-based-on-image)\n* [Provider Bid Script Migration for GPU Model Pricing](#provider-bid-script-migration-gpu-models)\n* [GPU Provider Troubleshooting](#gpu-provider-troubleshooting)\n\n\n## Provider Maintenance\n\n### Stop Provider Services Prior to Maintenance\n\nWhen conducting maintenance on your Akash Provider, ensure the `akash-provider` service is stopped during the maintenance period.\n\n> An issue exist currently in which provider leases may be lost during maintenance activities if the `akash-provider` service is not stopped prior.  This issue is detailed further [here](https://github.com/akash-network/provider/issues/64).\n\n#### Steps to Stop the `akash-provider` Service\n\n```\nkubectl -n akash-services get statefulsets\nkubectl -n akash-services scale statefulsets akash-provider --replicas=0\n```\n\n#### Steps to Verify the `akash-provider` Service Has Been Stopped\n\n```\nkubectl -n akash-services get statefulsets\nkubectl -n akash-services get pods -l app=akash-provide\n```\n\n#### Steps to Start the `akash-provider` Service Post Maintenance\n\n```\nkubectl -n akash-services scale statefulsets akash-provider --replicas=1\n```\n\n## How to terminate the workload from the Akash Provider using CLI\n\n### How to terminate the workload from the Akash Provider using CLI\n\n#### Impact of Steps Detailed in the K8s Cluster\n\n* Steps outlined in this section will terminate the deployment in K8s cluster and remove the manifest.\n* Providers can close the bid to get the provider escrow back.\n* Closing the bid will terminate the associated application running on the provider.\n* Closing the bid closes the lease (payment channel), meaning the tenant won't get any further charge for the deployment from the moment the bid is closed.\n* Providers cannot close the deployment orders.  Only the tenants can close deployment orders and only then would the deployment escrow be returned to the tenant.\n\n#### Impact of Steps Detailed on the Blockchain\n\nThe lease will get closed and the deployment will switch from the open to paused state with the open escrow account. Use akash query deployment get CLI command to verify this of desired. The owner will still have to close his deployment (akash tx deployment close) in order to get the AKT back from the deployment's escrow account (5 AKT by default). The provider has no rights to close the user deployment on its own.\n\nOf course you don't have to kubectl exec inside the akash-provider Pod - as detailed in this guide - you can just do the same anywhere where you have:\n\n* Providers key\n* Akash CLI tool;\n* Any mainnet akash RPC node to broadcast the bid close transaction\n* It is also worth noting that in some cases running the transactions from the account that is already in use (such as running akash-provider service) can cause the account sequence mismatch errors (typically when two clients are trying to issue the transaction within the same block window which is \\~6.1s)\n\n#### STEP 1 - Find the deployment you want to close\n\n```\nroot@node1:~# kubectl -n lease get manifest --show-labels --sort-by='.metadata.creationTimestamp'\n...\n5reb3l87s85t50v77sosktvdeeg6pfbnlboigoprqv3d4   26s     akash.network/lease.id.dseq=8438017,akash.network/lease.id.gseq=1,akash.network/lease.id.oseq=1,akash.network/lease.id.owner=akash1h24fljt7p0nh82cq0za0uhsct3sfwsfu9w3c9h,akash.network/lease.id.provider=akash1nxq8gmsw2vlz3m68qvyvcf3kh6q269ajvqw6y0,akash.network/namespace=5reb3l87s85t50v77sosktvdeeg6pfbnlboigoprqv3d4,akash.network=true\n```\n\n#### STEP 2 - Close the bid\n\n```\nroot@node1:~# kubectl -n akash-services exec -i $(kubectl -n akash-services get pods -l app=akash-provider --output jsonpath='{.items[0].metadata.name}') -- bash -c \"akash tx market bid close --owner akash1h24fljt7p0nh82cq0za0uhsct3sfwsfu9w3c9h --dseq 8438017 --oseq 1 --gseq 1 -y\"\n```\n\n#### STEP 3 - Verification\n\n* To make sure your provider is working well, you can watch the logs while trying to deploy something there, to make sure it bids (i.e. broadcasts the tx on the network)\n\n```\nkubectl -n akash-services logs -l app=akash-provider --tail=100 -f | grep -Ev \"running check|check result|cluster resources|service available replicas below target\"\n```\n\n_**Example/Expected Messages**_\n\n```\nI[2022-11-11|12:09:10.778] Reservation fulfilled                        module=bidengine-order order=akash1h24fljt7p0nh82cq0za0uhsct3sfwsfu9w3c9h/8438017/1/1\nD[2022-11-11|12:09:11.436] submitting fulfillment                       module=bidengine-order order=akash1h24fljt7p0nh82cq0za0uhsct3sfwsfu9w3c9h/8438017/1/1 price=21.000000000000000000uakt\nI[2022-11-11|12:09:11.451] broadcast response                           cmp=client/broadcaster response=\"code: 0\\ncodespace: \\\"\\\"\\ndata: \\\"\\\"\\nevents: []\\ngas_used: \\\"0\\\"\\ngas_wanted: \\\"0\\\"\\nheight: \\\"0\\\"\\ninfo: \\\"\\\"\\nlogs: []\\nraw_log: '[]'\\ntimestamp: \\\"\\\"\\ntx: null\\ntxhash: AF7E9AB65B0200B0B8B4D9934C019F8E07FAFB5C396E82DA582F719A1FA15C14\\n\" err=null\nI[2022-11-11|12:09:11.451] bid complete                                 module=bidengine-order order=akash1h24fljt7p0nh82cq0za0uhsct3sfwsfu9w3c9h/8438017/1/1\n```\n\n* To ensure, you can always bounce the provider service which will have no impact on active workloads\n\n```\nkubectl -n akash-services delete pods -l app=akash-provider\n```\n\n# Provider Logs\n\nThe commands in this section peer into the provider’s logs and may be used to verify possible error conditions on provider start up and to ensure provider order receipt/bid process completion steps.\n\n### Command Template\n\nIssue the commands in this section from a control plane node within the Kubernetes cluster or a machine that has kubectl communication with the cluster.\n\n```\nkubectl logs <pod-name> -n akash-services\n```\n\n### Example Command Use\n\n* Using the example command syntax we will list the last ten entries in Provider logs and enter a live streaming session of new logs generated\n\n```\nkubectl -n akash-services logs $(kubectl -n akash-services get pods -l app=akash-provider --output jsonpath='{.items[-1].metadata.name}') --tail=10 -f\n```\n\n### Example Output\n\n* Note within the example the receipt of a deployment order with a DSEQ of 5949829\n* The sequence shown from `order-detected` thru reservations thru `bid-complete` provides an example of what we would expect to see when an order is received by the provider\n* The order receipt is one of many events sequences that can be verified within provider logs\n\n```\nkubectl -n akash-services logs $(kubectl -n akash-services get pods -l app=akash-provider --output jsonpath='{.items[-1].metadata.name}') --tail=10 -f\n\nI[2022-05-19|17:20:42.069] syncing sequence                             cmp=client/broadcaster local=22 remote=22\nI[2022-05-19|17:20:52.069] syncing sequence                             cmp=client/broadcaster local=22 remote=22\nI[2022-05-19|17:21:02.068] syncing sequence                             cmp=client/broadcaster local=22 remote=22\nD[2022-05-19|17:21:10.983] cluster resources                            module=provider-cluster cmp=service cmp=inventory-service dump=\"{\\\"nodes\\\":[{\\\"name\\\":\\\"node1\\\",\\\"allocatable\\\":{\\\"cpu\\\":1800,\\\"memory\\\":3471499264,\\\"storage_ephemeral\\\":46663523866},\\\"available\\\":{\\\"cpu\\\":780,\\\"memory\\\":3155841024,\\\"storage_ephemeral\\\":46663523866}},{\\\"name\\\":\\\"node2\\\",\\\"allocatable\\\":{\\\"cpu\\\":1900,\\\"memory\\\":3739934720,\\\"storage_ephemeral\\\":46663523866},\\\"available\\\":{\\\"cpu\\\":1295,\\\"memory\\\":3204544512,\\\"storage_ephemeral\\\":46663523866}}],\\\"total_allocatable\\\":{\\\"cpu\\\":3700,\\\"memory\\\":7211433984,\\\"storage_ephemeral\\\":93327047732},\\\"total_available\\\":{\\\"cpu\\\":2075,\\\"memory\\\":6360385536,\\\"storage_ephemeral\\\":93327047732}}\\n\"\nI[2022-05-19|17:21:12.068] syncing sequence                             cmp=client/broadcaster local=22 remote=22\nI[2022-05-19|17:21:22.068] syncing sequence                             cmp=client/broadcaster local=22 remote=22\nI[2022-05-19|17:21:29.391] order detected                               module=bidengine-service order=order/akash1ggk74pf9avxh3llu30yfhmr345h2yrpf7c2cdu/5949829/1/1\nI[2022-05-19|17:21:29.495] group fetched                                module=bidengine-order order=akash1ggk74pf9avxh3llu30yfhmr345h2yrpf7c2cdu/5949829/1/1\nI[2022-05-19|17:21:29.495] requesting reservation                       module=bidengine-order order=akash1ggk74pf9avxh3llu30yfhmr345h2yrpf7c2cdu/5949829/1/1\nD[2022-05-19|17:21:29.495] reservation requested                        module=provider-cluster cmp=service cmp=inventory-service order=akash1ggk74pf9avxh3llu30yfhmr345h2yrpf7c2cdu/5949829/1/1 resources=\"group_id:<owner:\\\"akash1ggk74pf9avxh3llu30yfhmr345h2yrpf7c2cdu\\\" dseq:5949829 gseq:1 > state:open group_spec:<name:\\\"akash\\\" requirements:<signed_by:<> attributes:<key:\\\"host\\\" value:\\\"akash\\\" > > resources:<resources:<cpu:<units:<val:\\\"100\\\" > > memory:<quantity:<val:\\\"2686451712\\\" > > storage:<name:\\\"default\\\" quantity:<val:\\\"268435456\\\" > > endpoints:<> > count:1 price:<denom:\\\"uakt\\\" amount:\\\"10000000000000000000000\\\" > > > created_at:5949831 \"\nD[2022-05-19|17:21:29.495] reservation count                            module=provider-cluster cmp=service cmp=inventory-service cnt=1\nI[2022-05-19|17:21:29.495] Reservation fulfilled                        module=bidengine-order order=akash1ggk74pf9avxh3llu30yfhmr345h2yrpf7c2cdu/5949829/1/1\nD[2022-05-19|17:21:29.496] submitting fulfillment                       module=bidengine-order order=akash1ggk74pf9avxh3llu30yfhmr345h2yrpf7c2cdu/5949829/1/1 price=4.540160000000000000uakt\nI[2022-05-19|17:21:29.725] broadcast response                           cmp=client/broadcaster response=\"code: 0\\ncodespace: \\\"\\\"\\ndata: \\\"\\\"\\nevents: []\\ngas_used: \\\"0\\\"\\ngas_wanted: \\\"0\\\"\\nheight: \\\"0\\\"\\ninfo: \\\"\\\"\\nlogs: []\\nraw_log: '[]'\\ntimestamp: \\\"\\\"\\ntx: null\\ntxhash: AFCA8D4A900A62D961F4AB82B607749FFCA8C10E2B0486B89A8416B74593DBFA\\n\" err=null\nI[2022-05-19|17:21:29.725] bid complete                                 module=bidengine-order order=akash1ggk74pf9avxh3llu30yfhmr345h2yrpf7c2cdu/5949829/1/1\nI[2022-05-19|17:21:32.069] syncing sequence                             cmp=client/broadcaster local=23 remote=22\n```\n\n\n#@ Provider Status and General Info\n\nUse the verifications included in this section for the following purposes:\n\n* [Determine Provider Status](#provider-status)\n* [Review Provider Configuration](#provider-configuration-review)\n* [Current Versions of Provider's Akash and Kubernetes Installs](#current-versions-of-providers-akash-and-kubernetes-installs)&#x20;\n\n### Provider Status\n\nObtain live Provider status including:\n\n* Number of active leases\n* Active leases and hard consumed by those leases\n* Available resources on a per node basis\n\n#### Command Template\n\nIssue the commands in this section from any machine that has the [Akash CLI ](/docs/docs/deployments/akash-cli/installation/)installed.\n\n```\nprovider-services status <provider-address>\n```\n\n#### Example Command Use\n\n```\nprovider-services status akash1q7spv2cw06yszgfp4f9ed59lkka6ytn8g4tkjf\n```\n\n#### Example Output\n\n```\nprovider-services status akash1wxr49evm8hddnx9ujsdtd86gk46s7ejnccqfmy\n{\n  \"cluster\": {\n    \"leases\": 3,\n    \"inventory\": {\n      \"active\": [\n        {\n          \"cpu\": 8000,\n          \"memory\": 8589934592,\n          \"storage_ephemeral\": 5384815247360\n        },\n        {\n          \"cpu\": 100000,\n          \"memory\": 450971566080,\n          \"storage_ephemeral\": 982473768960\n        },\n        {\n          \"cpu\": 8000,\n          \"memory\": 8589934592,\n          \"storage_ephemeral\": 2000000000000\n        }\n      ],\n      \"available\": {\n        \"nodes\": [\n          {\n            \"cpu\": 111495,\n            \"memory\": 466163988480,\n            \"storage_ephemeral\": 2375935850345\n          },\n          {\n            \"cpu\": 118780,\n            \"memory\": 474497601536,\n            \"storage_ephemeral\": 7760751097705\n          },\n          {\n            \"cpu\": 110800,\n            \"memory\": 465918152704,\n            \"storage_ephemeral\": 5760751097705\n          },\n          {\n            \"cpu\": 19525,\n            \"memory\": 23846356992,\n            \"storage_ephemeral\": 6778277328745\n          }\n        ]\n      }\n    }\n  },\n  \"bidengine\": {\n    \"orders\": 0\n  },\n  \"manifest\": {\n    \"deployments\": 0\n  },\n  \"cluster_public_hostname\": \"provider.bigtractorplotting.com\"\n}\n```\n\n## Provider Configuration Review\n\nReview the Provider’s attribute, and Host URI with the status command\n\n### Command Template\n\nIssue the commands in this section from any machine that has the [Akash CLI](/docs/docs/deployments/akash-cli/installation/) installed.\n\n```\nprovider-services query provider get <akash-address>\n```\n\n### Example Command Use\n\n```\nprovider-services query provider get <address>\n```\n\n### Example Output\n\n```\nattributes:\n- key: capabilities/storage/1/class\n  value: default\n- key: capabilities/storage/1/persistent\n  value: \"true\"\n- key: capabilities/storage/2/class\n  value: beta2\n- key: capabilities/storage/2/persistent\n  value: \"true\"\n- key: host\n  value: akash\n- key: organization\n  value: akash.network\n- key: region\n  value: us-west\n- key: tier\n  value: community\n- key: provider\n  value: \"1\"\n- key: chia-plotting\n  value: \"true\"\nhost_uri: https://provider.mainnet-1.ca.aksh.pw:8443\ninfo:\n  email: \"\"\n  website: \"\"\nowner: akash1q7spv2cw06yszgfp4f9ed59lkka6ytn8g4tkjf\n```\n\n## Current Versions of Provider's Akash and Kubernetes Installs\n\n* Command may be issued from any source with internet connectivity\n\n```\ncurl -sk https://provider.mainnet-1.ca.aksh.pw:8443/version | jq,    \n```\n\n## Provider Lease Management\n\nUse the verifications included in this section for the following purposes:\n\n* [List Provider Active Leases](#list-provider-active-leases)\n* [List Active Leases from Hostname Operator Perspective](#list-active-leases-from-hostname-operator-perspective)\n* [Provider Side Lease Closure](#provider-side-lease-closure)\n* [Ingress Controller Verifications](#ingress-controller-verifications)\n\n## List Provider Active Leases\n\n### Command Template\n\nIssue the commands in this section from any machine that has the [Akash CLI](/docs/docs/deployments/akash-cli/installation/) installed.\n\n```\nprovider-services query market lease list --provider <provider-address> --gseq 0 --oseq 0 --page 1 --limit 500 --state active\n```\n\n### Example Command Use\n\n```\nprovider-services query market lease list --provider akash1yvu4hhnvs84v4sv53mzu5ntf7fxf4cfup9s22j --gseq 0 --oseq 0 --page 1 --limit 500 --state active\n```\n\n### Example Output\n\n```\nleases:\n- escrow_payment:\n    account_id:\n      scope: deployment\n      xid: akash19gs08y80wlk5wl4696wz82z2wrmjw5c84cvw28/5903794\n    balance:\n      amount: \"0.455120000000000000\"\n      denom: uakt\n    owner: akash1q7spv2cw06yszgfp4f9ed59lkka6ytn8g4tkjf\n    payment_id: 1/1/akash1q7spv2cw06yszgfp4f9ed59lkka6ytn8g4tkjf\n    rate:\n      amount: \"24.780240000000000000\"\n      denom: uakt\n    state: open\n    withdrawn:\n      amount: \"32536\"\n      denom: uakt\n  lease:\n    closed_on: \"0\"\n    created_at: \"5903822\"\n    lease_id:\n      dseq: \"5903794\"\n      gseq: 1\n      oseq: 1\n      owner: akash19gs08y80wlk5wl4696wz82z2wrmjw5c84cvw28\n      provider: akash1q7spv2cw06yszgfp4f9ed59lkka6ytn8g4tkjf\n    price:\n      amount: \"24.780240000000000000\"\n      denom: uakt\n    state: active\n```\n\n## List Active Leases from Hostname Operator Perspective\n\n#### **Command Syntax**\n\nIssue the commands in this section from a control plane node within the Kubernetes cluster or a machine that has the kubectl communication with the cluster.\n\n```\nkubectl -n lease get providerhosts\n```\n\n#### **Example Output**\n\n```\nNAME                                                  AGE\ngtu5bo14f99elel76srrbj04do.ingress.akashtesting.xyz   60m\nkbij2mvdlhal5dgc4pc7171cmg.ingress.akashtesting.xyz   18m\n```\n\n## Provider Side Lease Closure\n\n### **Command Template**\n\nIssue the commands in this section from a control plane node within the Kubernetes cluster or a machine that has the kubectl communication with the cluster.\n\n```\nprovider-services tx market bid close --node $AKASH_NODE --chain-id $AKASH_CHAIN_ID --owner <TENANT-ADDRESS> --dseq $AKASH_DSEQ --gseq 1 --oseq 1 --from <PROVIDER-ADDRESS> --keyring-backend $AKASH_KEYRING_BACKEND -y --gas-prices=\"0.025uakt\" --gas=\"auto\" --gas-adjustment=1.15\n```\n\n### Example Command Use\n\n```\nprovider-services tx market bid close --node $AKASH_NODE --chain-id akashnet-2 --owner akash1n44zc8l6gfm0hpydldndpg8n05xjjwmuahc6nn --dseq 5905802 --gseq 1 --oseq 1 --from akash1yvu4hhnvs84v4sv53mzu5ntf7fxf4cfup9s22j --keyring-backend os -y --gas-prices=\"0.025uakt\" --gas=\"auto\" --gas-adjustment=1.15\n```\n\n### **Example Output (Truncated)**\n\n```\n{\"height\":\"5906491\",\"txhash\":\"0FC7DA74301B38BC3DF2F6EBBD2020C686409CE6E973E25B4E8F0F1B83235473\",\"codespace\":\"\",\"code\":0,\"data\":\"0A230A212F616B6173682E6D61726B65742E763162657461322E4D7367436C6F7365426964\",\"raw_log\":\"[{\\\"events\\\":[{\\\"type\\\":\\\"akash.v1\\\",\\\"attributes\\\":[{\\\"key\\\":\\\"module\\\",\\\"value\\\":\\\"deployment\\\"},{\\\"key\\\":\\\"action\\\",\\\"value\\\":\\\"group-paused\\\"},{\\\"key\\\":\\\"owner\\\",\\\"value\\\":\\\"akash1n44zc8l6gfm0hpydldndpg8n05xjjwmuahc6nn\\\"},{\\\"key\\\":\\\"dseq\\\",\\\"value\\\":\\\"5905802\\\"},{\\\"key\\\":\\\"gseq\\\",\\\"value\\\":\\\"1\\\"},{\\\"key\\\":\\\"module\\\",\\\"value\\\":\\\"market\\\"},{\\\"key\\\":\\\"action\\\",\\\"value\\\":\\\"lease-closed\\\"}\n```\n\n## Ingress Controller Verifications\n\n### Example Command Use\n\nIssue the commands in this section from a control plane node within the Kubernetes cluster or a machine that has the kubectl communication with the cluster.\n\n```\nkubectl get ingress -A\n```\n\n### Example Output\n\n* **NOTE -** in this example output the last entry (with namespace moc58fca3ccllfrqe49jipp802knon0cslo332qge55qk) represents an active deployment on the provider\n\n```\nNAMESPACE                                       NAME                                                  CLASS                 HOSTS                                                 ADDRESS                   PORTS   AGE\n\nmoc58fca3ccllfrqe49jipp802knon0cslo332qge55qk   5n0vp4dmbtced00smdvb84ftu4.ingress.akashtesting.xyz   akash-ingress-class   5n0vp4dmbtced00smdvb84ftu4.ingress.akashtesting.xyz   10.0.10.122,10.0.10.236   80      70s\n```\n\n# Provider Manifests\n\nUse the verifications included in this section for the following purposes:\n\n* [Retrieve Active Manifest List From Provider](#retrieve-active-manifest-list-from-provider)\n* [Retrieve Manifest Detail From Provider](#retrieve-manifest-detail-from-provider)\n\n## Retrieve Active Manifest List From Provider\n\n### **Command Syntax**\n\nIssue the commands in this section from a control plane node within the Kubernetes cluster or a machine that has kubectl communication with the cluster.\n\n```\nkubectl -n lease get manifests --show-labels\n```\n\n### Example Output\n\n* The show-labels options includes display of associated DSEQ / OSEQ / GSEQ / Owner labels\n\n```\nkubectl -n lease get manifests --show-labels\n\nNAME                                            AGE   LABELS\nh644k9qp92e0qeakjsjkk8f3piivkuhgc6baon9tccuqo   26h   akash.network/lease.id.dseq=5950031,akash.network/lease.id.gseq=1,akash.network/lease.id.oseq=1,akash.network/lease.id.owner=akash15745vczur53teyxl4k05u250tfvp0lvdcfqx27,akash.network/lease.id.provider=akash1xmz9es9ay9ln9x2m3q8dlu0alxf0ltce7ykjfx,akash.network/namespace=h644k9qp92e0qeakjsjkk8f3piivkuhgc6baon9tccuqo,akash.network=true\n```\n\n## Retrieve Manifest Detail From Provider\n\n### Command Template\n\nIssue the commands in this section from a control plane node within the Kubernetes cluster or a machine that has kubectl communication with the cluster.\n\n```\nkubectl -n lease get manifest <namespace> -o yaml\n```\n\n### Example Command Use\n\n* **Note -** use the \\`kubectl get ingress -A\\` covered in this guide to lookup the namespace of the deployment of interest\n\n```\nkubectl -n lease get manifest moc58fca3ccllfrqe49jipp802knon0cslo332qge55qk -o yaml\n```\n\n### Example Output\n\n```\napiVersion: akash.network/v2beta1\nkind: Manifest\nmetadata:\n  creationTimestamp: \"2022-05-16T14:42:29Z\"\n  generation: 1\n  labels:\n    akash.network: \"true\"\n    akash.network/lease.id.dseq: \"5905802\"\n    akash.network/lease.id.gseq: \"1\"\n    akash.network/lease.id.oseq: \"1\"\n    akash.network/lease.id.owner: akash1n44zc8l6gfm0hpydldndpg8n05xjjwmuahc6nn\n    akash.network/lease.id.provider: akash1yvu4hhnvs84v4sv53mzu5ntf7fxf4cfup9s22j\n    akash.network/namespace: moc58fca3ccllfrqe49jipp802knon0cslo332qge55qk\n  name: moc58fca3ccllfrqe49jipp802knon0cslo332qge55qk\n  namespace: lease\n  resourceVersion: \"75603\"\n  uid: 81fc7e4f-9091-44df-b4cf-5249ddd4863d\nspec:\n  group:\n    name: akash\n    services:\n    - count: 1\n      expose:\n      - external_port: 80\n        global: true\n        http_options:\n          max_body_size: 1048576\n          next_cases:\n          - error\n          - timeout\n          next_tries: 3\n          read_timeout: 60000\n          send_timeout: 60000\n        port: 8080\n        proto: TCP\n      image: pengbai/docker-supermario\n      name: supermario\n      unit:\n        cpu: 100\n        memory: \"268435456\"\n        storage:\n        - name: default\n          size: \"268435456\"\n  lease_id:\n    dseq: \"5905802\"\n    gseq: 1\n    oseq: 1\n    owner: akash1n44zc8l6gfm0hpydldndpg8n05xjjwmuahc6nn\n    provider: akash1yvu4hhnvs84v4sv53mzu5ntf7fxf4cfup9s22j\n```\n\n## Provider Earnings\n\nUse the verifications included in this section for the following purposes:\n\n* [Provider Earnings History](#provider-earnings-history)\n* [AKT Total Earned by Provider](#akt-total-earned-by-provider)\n* [AKT Total Earning Potential Per Active Deployment](#akt-total-earning-potential-per-active-deployment)\n* [Current Leases: Withdrawn vs Consumed](#current-leases-withdrawn-vs-consumed)\n\n## Provider Earnings History\n\nUse the commands detailed in this section to gather the daily earnings history of your provider\n\n### Command Template\n\n* Only the following variables need update in the template for your use:\n  * AKASH\\_NODE - populate value with the address of your RPC node\n  * PROVIDER - populate value with your provider address\n\n```\nexport AKASH_NODE=<your-RPC-node-address>\n\nPROVIDER=<your-provider-address>; STEP=23.59; BLOCK_TIME=6; HEIGHT=$(provider-services query block | jq -r '.block.header.height'); for i in $(seq 0 23); do BLOCK=$(echo \"scale=0; ($HEIGHT-((60/$BLOCK_TIME)*60*($i*$STEP)))/1\" | bc); HT=$(provider-services query block $BLOCK | jq -r '.block.header.time'); AL=$(provider-services query market lease list --height $BLOCK --provider $PROVIDER --gseq 0 --oseq 0 --page 1 --limit 200 --state active -o json | jq -r '.leases | length'); DCOST=$(provider-services query market lease list --height $BLOCK --provider $PROVIDER --gseq 0 --oseq 0 --page 1 --limit 200 -o json --state active | jq --argjson bt $BLOCK_TIME -c -r '(([.leases[].lease.price.amount // 0|tonumber] | add)*(60/$bt)*60*24)/pow(10;6)'); BALANCE=$(provider-services query bank balances --height $BLOCK $PROVIDER -o json | jq -r '.balances[] | select(.denom == \"uakt\") | .amount // 0|tonumber/pow(10;6)'); IN_ESCROW=$(echo \"($AL * 5)\" | bc); TOTAL=$( echo \"($BALANCE+$IN_ESCROW)\" | bc); printf \"%8d\\t%.32s\\t%4d\\t%12.4f\\t%12.6f\\t%4d\\t%12.4f\\n\" $BLOCK $HT $AL $DCOST $BALANCE $IN_ESCROW $TOTAL; done\n```\n\n### **Example Command Use**\n\n```\nPROVIDER=akash18ga02jzaq8cw52anyhzkwta5wygufgu6zsz6xc; STEP=23.59; BLOCK_TIME=6; HEIGHT=$(provider-services query block | jq -r '.block.header.height'); for i in $(seq 0 23); do BLOCK=$(echo \"scale=0; ($HEIGHT-((60/$BLOCK_TIME)*60*($i*$STEP)))/1\" | bc); HT=$(provider-services query block $BLOCK | jq -r '.block.header.time'); AL=$(provider-services query market lease list --height $BLOCK --provider $PROVIDER --gseq 0 --oseq 0 --page 1 --limit 200 --state active -o json | jq -r '.leases | length'); DCOST=$(provider-services query market lease list --height $BLOCK --provider $PROVIDER --gseq 0 --oseq 0 --page 1 --limit 200 -o json --state active | jq --argjson bt $BLOCK_TIME -c -r '(([.leases[].lease.price.amount // 0|tonumber] | add)*(60/$bt)*60*24)/pow(10;6)'); BALANCE=$(provider-services query bank balances --height $BLOCK $PROVIDER -o json | jq -r '.balances[] | select(.denom == \"uakt\") | .amount // 0|tonumber/pow(10;6)'); IN_ESCROW=$(echo \"($AL * 5)\" | bc); TOTAL=$( echo \"($BALANCE+$IN_ESCROW)\" | bc); printf \"%8d\\t%.32s\\t%4d\\t%12.4f\\t%12.6f\\t%4d\\t%12.4f\\n\" $BLOCK $HT $AL $DCOST $BALANCE $IN_ESCROW $TOTAL; done\n```\n\n### Example Output\n\n* Column Headers\n\n```\nblock height, timestamp, active leases, daily earning, balance, AKT in escrow, total balance (AKT in escrow + balance)\n```\n\n* Output generated from `Example Command Use`\n\n```\n 6514611\t2022-06-28T15:32:53.445887205Z\t  52\t    142.8624\t 1523.253897\t 260\t   1783.2539\n 6500457\t2022-06-27T15:56:52.370736803Z\t  61\t    190.8000\t 1146.975982\t 305\t   1451.9760\n 6486303\t2022-06-26T15:25:08.727479091Z\t  38\t    116.9280\t 1247.128473\t 190\t   1437.1285\n 6472149\t2022-06-25T15:18:50.058601546Z\t  39\t    119.3184\t 1211.060233\t 195\t   1406.0602\n 6457995\t2022-06-24T15:17:19.284205728Z\t  56\t    186.8688\t 1035.764462\t 280\t   1315.7645\n 6443841\t2022-06-23T14:51:42.110369321Z\t  50\t    182.6352\t 1005.680589\t 250\t   1255.6806\n 6429687\t2022-06-22T14:58:52.656092131Z\t  36\t    120.3984\t  962.763599\t 180\t   1142.7636\n 6415533\t2022-06-21T15:04:57.22739534Z\t  29\t    226.7568\t  837.161130\t 145\t    982.1611\n 6401379\t2022-06-20T15:08:17.114891411Z\t   8\t     57.5136\t  760.912627\t  40\t    800.9126\n 6387225\t2022-06-19T15:12:16.883456449Z\t   6\t     53.9856\t  697.260245\t  30\t    727.2602\n 6373071\t2022-06-18T15:16:16.007190056Z\t   6\t    257.1696\t  635.254956\t  30\t    665.2550\n 6358917\t2022-06-17T15:20:52.671364197Z\t   5\t     33.2208\t  560.532818\t  25\t    585.5328\n```\n\n## AKT Total Earned by Provider\n\nUse the commands detailed in this section to gather the total earnings of your provider&#x20;\n\n### Command Template\n\nIssue the commands in this section from any machine that has the [Akash CLI ](../../guides/cli/detailed-steps/)installed.\n\n* **Note** - ensure queries are not limited only to leases created by your account by issuing `unset AKASH_FROM` prior to the `akash query market` command execution\n\n```\nprovider-services query market lease list --provider <PROVIDER-ADDRESS> --page 1 --limit 1000 -o json | jq -r '([.leases[].escrow_payment.withdrawn.amount|tonumber] | add) / pow(10;6)'\n```\n\n### **Example Command Use**\n\n```\nprovider-services query market lease list --provider akash1yvu4hhnvs84v4sv53mzu5ntf7fxf4cfup9s22j --page 1 --limit 1000 -o json | jq -r '([.leases[].escrow_payment.withdrawn.amount|tonumber] | add) / pow(10;6)'\n```\n\n### Example Output\n\n```\n8.003348\n```\n\n## AKT Total Earning Potential Per Active Deployment\n\n### Legend for Command Syntax\n\nIn the equations used in the calculation of earning potential, several figures are used that are indeed not static.&#x20;\n\nFor accurate earning potential based on today's actual financial/other realities, consider if the following numbers should be updated prior to command execution.\n\n#### Figures in Current Command Syntax\n\n* 1.79 price of 1 AKT in USD\n* 6.088 block time (current available via: [https://mintscan.io/akash](https://mintscan.io/akash))\n* 30.436875 used as the average number of days in a month\\\n\n\n### Command Syntax\n\nIssue the commands in this section from any machine that has the [Akash CLI](/docs/docs/deployments/akash-cli/installation/) installed.\n\n**Note** - ensure queries are not limited only to leases created by your account by issuing `unset AKASH_FROM` prior to the `akash query market` command execution\n\n```\nprovider-services query market lease list --provider <PROVIDER-ADDRESS> --gseq 0 --oseq 0 --page 1 --limit 100 --state active -o json | jq -r '[\"owner\",\"dseq\",\"gseq\",\"oseq\",\"rate\",\"monthly\",\"USD\"], (.leases[] | [(.lease.lease_id | .owner, .dseq, .gseq, .oseq), (.escrow_payment | .rate.amount, (.rate.amount|tonumber), (.rate.amount|tonumber))]) | @csv' | awk -F ',' '{if (NR==1) {$1=$1; printf $0\"\\n\"} else {$6=(($6*((60/6.088)*60*24*30.436875))/10^6); $7=(($7*((60/6.088)*60*24*30.436875))/10^6)*1.79; print $0}}' | column -t\n```\n\n### Example Command Use\n\n```\nprovider-services query market lease list --provider akash1yvu4hhnvs84v4sv53mzu5ntf7fxf4cfup9s22j --gseq 0 --oseq 0 --page 1 --limit 100 --state active -o json | jq -r '[\"owner\",\"dseq\",\"gseq\",\"oseq\",\"rate\",\"monthly\",\"USD\"], (.leases[] | [(.lease.lease_id | .owner, .dseq, .gseq, .oseq), (.escrow_payment | .rate.amount, (.rate.amount|tonumber), (.rate.amount|tonumber))]) | @csv' | awk -F ',' '{if (NR==1) {$1=$1; printf $0\"\\n\"} else {$6=(($6*((60/6.088)*60*24*30.436875))/10^6); $7=(($7*((60/6.088)*60*24*30.436875))/10^6)*1.79; print $0}}' | column -t\n```\n\n### Example Output\n\n```\n\"owner\"                                         \"dseq\"     \"gseq\"  \"oseq\"  \"rate\"                  \"monthly\"  \"USD\"\n\n\"akash1n44zc8l6gfm0hpydldndpg8n05xjjwmuahc6nn\"  \"5850047\"  1       1       \"4.901120000000000000\"  1.92197    3.44032\n\"akash1n44zc8l6gfm0hpydldndpg8n05xjjwmuahc6nn\"  \"5850470\"  1       1       \"2.901120000000000000\"  1.13767    2.03643\n```\n\n## Current Leases: Withdrawn vs Consumed\n\nUse the commands detailed in this section to compare the amount of AKT consumed versus the amount of AKT withdrawn per deployment.  This review will ensure that withdraw of consumed funds is occurring as expected.\n\n### Command Syntax\n\nOnly the following variables need update in the template for your use:\n\n* AKASH\\_NODE - populate value with the address of your RPC node\n* PROVIDER - populate value with your provider address\n\n```\nexport AKASH_NODE=<your-RPC-node-address>\n\nPROVIDER=<your-provider-address>; HEIGHT=$(provider-services query block | jq -r '.block.header.height'); provider-services query market lease list --height $HEIGHT --provider $PROVIDER --gseq 0 --oseq 0 --page 1 --limit 10000 --state active -o json | jq --argjson h $HEIGHT -r '[\"owner\",\"dseq/gseq/oseq\",\"rate\",\"monthly\",\"withdrawn\",\"consumed\",\"days\"], (.leases[] | [(.lease.lease_id | .owner, (.dseq|tostring) + \"/\" + (.gseq|tostring) + \"/\" + (.oseq|tostring)), (.escrow_payment | (.rate.amount|tonumber), (.rate.amount|tonumber), (.withdrawn.amount|tonumber)), (($h-(.lease.created_at|tonumber))*(.escrow_payment.rate.amount|tonumber)/pow(10;6)), (($h-(.lease.created_at|tonumber))/((60/6)*60*24))]) | @csv' | awk -F ',' '{if (NR==1) {$1=$1; printf $0\"\\n\"} else {block_time=6; rate_akt=(($4*((60/block_time)*60*24*30.436875))/10^6); $4=rate_akt; withdrawn_akt=($5/10^6); $5=withdrawn_akt; $6; $7; print $0}}' | column -t \n```\n\n### Example Command Use\n\n```\nPROVIDER=akash18ga02jzaq8cw52anyhzkwta5wygufgu6zsz6xc; HEIGHT=$(provider-services query block | jq -r '.block.header.height'); provider-services query market lease list --height $HEIGHT --provider $PROVIDER --gseq 0 --oseq 0 --page 1 --limit 10000 --state active -o json | jq --argjson h $HEIGHT -r '[\"owner\",\"dseq/gseq/oseq\",\"rate\",\"monthly\",\"withdrawn\",\"consumed\",\"days\"], (.leases[] | [(.lease.lease_id | .owner, (.dseq|tostring) + \"/\" + (.gseq|tostring) + \"/\" + (.oseq|tostring)), (.escrow_payment | (.rate.amount|tonumber), (.rate.amount|tonumber), (.withdrawn.amount|tonumber)), (($h-(.lease.created_at|tonumber))*(.escrow_payment.rate.amount|tonumber)/pow(10;6)), (($h-(.lease.created_at|tonumber))/((60/6)*60*24))]) | @csv' | awk -F ',' '{if (NR==1) {$1=$1; printf $0\"\\n\"} else {block_time=6; rate_akt=(($4*((60/block_time)*60*24*30.436875))/10^6); $4=rate_akt; withdrawn_akt=($5/10^6); $5=withdrawn_akt; $6; $7; print $0}}' | column -t \n```\n\n### Example Output\n\n```\n\"owner\"                                         \"dseq/gseq/oseq\"  \"rate\"  \"monthly\"  \"withdrawn\"  \"consumed\"  \"days\"\n\"akash1zrce7fke2pxmnrwlwdjxcgyfcz43vljw5tekr2\"  \"6412884/1/1\"     23      10.0807    2.33866      2.358627    7.121458333333333\n\"akash1ynq8anzujggr7w38dltlx3u77le3z3ru3x9vez\"  \"6443412/1/1\"     354     155.155    25.1846      25.49154    5.000694444444444\n\"akash1y48wwg95plz4ht5sakdqg5st8pmeuuljw6y9tc\"  \"6503695/1/1\"     45      19.7231    0.488925     0.527895    0.8146527777777778\n\"akash1ga6xuntfwsqrutv9dwz4rjcy5h8efn7yw6dywu\"  \"6431684/1/1\"     66      28.9272    5.47028      5.527302    5.815763888888889\n\"akash1f9mn3dhajkcrqxzk5c63kzka7t9tur3xehrn2r\"  \"6426723/1/1\"     69      30.2421    6.06048      6.120024    6.1594444444444445\n\"akash12r63l4ldjvjqmagmq9fe82r78cqent5hucyg48\"  \"6496087/1/1\"     114     49.9652    2.10661      2.204874    1.343125\n\"akash12r63l4ldjvjqmagmq9fe82r78cqent5hucyg48\"  \"6496338/1/1\"     98      42.9525    1.78683      1.871212    1.3259722222222223\n\"akash1tfj0hh6h0zqak0fx7jhhjyc603p7d7y4xmnlp3\"  \"6511999/1/1\"     66      28.9272    0.169422     0.226182    0.23798611111111112\n```\n\n# Dangling Deployments\n\nAs part of routine Akash Provider maintenance it is a good idea to ensure that there are no \"dangling deployments\" in your provider's Kubernetes cluster.\n\nWe define a \"dangling deployment\" as a scenario in which the lease for a deployment was closed but due to a communication issue the associated deployment in Kubernetes is not closed.  Vice versa applies too, where the dangling deployment could sit active on the chain but not on the provider. This should be a rare circumstance but we want to cleanse the provider of any such \"dangling deployments\" from time to time.\n\nPlease use this [Dangling Deployment Script](https://gist.github.com/andy108369/f211bf6c06f2a6e3635b20bdfb9f0fca) to both discover and close any such deployments.\n\n## Heal Broken Deployment Replicas by Returning Lost command to Manifests\n\nPrior to the `provider` version `0.2.1` (`akash/provider helm-chart version 4.2.0`) there was an issue which was affecting some deployments.\n\n### Issue\n\n&#x20;The deployments with the `command` explicitly set in their SDL manifest files were losing it upon `akash-provider` pod/service restart.\n\nThis was leading to their replica pods running in `CrashLoopBackOff` state on the provider side reserving additional resources, while the original replica was still running which was not visible to the client.\n\n### Impact\n\n* Double amount of the resources are being occupied by the deployment on the provider side\n* Manifests of these deployments are missing the command\n\nThe good news is that both issues can be fixed without the customer intervention.\n\nOnce you have updated your provider to 0.2.1 version or greater following the instructions, you can patch the manifests with the correct command which will get rid of the deployments left in `CrashLoopBackOff` state.\n\n**STEP1** - Backup manifests\n\nBefore patching the manifests, please make sure to back them up.\n\n```\nmkdir before\ncd before\nfor i in manifests providerhosts providerleasedips; do kubectl -n lease get $i -o yaml > $i-backup.yaml; done\n```\n\nThey can help in troubleshooting the issues should any arise later.\n\n**STEP2** - Collect the deployments which are affected by the lost command issue\n\n```\nkubectl get deployment -l akash.network/manifest-service -A -o=jsonpath='{range .items[*]}{.metadata.namespace} {.metadata.name}{\"\\n\"}{end}' |\n  while read ns app; do\n    kubectl -n $ns rollout status --timeout=60s deployment/${app} >/dev/null 2>&1\n    rc=$?\n    if [[ $rc -ne 0 ]]; then\n      kubectl -n $ns rollout history deployment/${app} -o json |\n        jq -r '[(.metadata | .annotations.\"deployment.kubernetes.io/revision\", .namespace, .name), (.spec.template.spec.containers[0].command|tostring)] | @tsv'\n      echo\n     fi\n   done\n```\n\n_**Example Output:**_\n\n> revision, namespace, pod, command\n\n```\n3\t2anv3d7diieucjlga92fk8e5ej12kk8vmtkpi9fpju79a\tcloud-sql-proxy-7bfb55ddb\t[\"sh\",\"-c\"]\n4\t2anv3d7diieucjlga92fk8e5ej12kk8vmtkpi9fpju79a\tcloud-sql-proxy-57c8f9ff48\tnull\n\n3\t2dl4vdk2f7ia1m0vme8nqkv0dadnnj15becr5pmfu9j22\tcloud-sql-proxy-7dc7f5b856\t[\"sh\",\"-c\"]\n4\t2dl4vdk2f7ia1m0vme8nqkv0dadnnj15becr5pmfu9j22\tcloud-sql-proxy-864fd4cff4\tnull\n\n1\t2k83g8gstuugse0952arremk4gphib709gi7b6q6srfdo\tapp-78756d77ff\t[\"bash\",\"-c\"]\n2\t2k83g8gstuugse0952arremk4gphib709gi7b6q6srfdo\tapp-578b949f48\tnull\n\n7\t2qpj8537lq7tiv9fabdhk8mn4j75h3anhtqb1b881fhie\tcloud-sql-proxy-7c5f486d9b\t[\"sh\",\"-c\"]\n8\t2qpj8537lq7tiv9fabdhk8mn4j75h3anhtqb1b881fhie\tcloud-sql-proxy-6c95666bc8\tnull\n\n1\tb49oi05ph3bo7rdn2kvkkpk4tcigb3ts0o7sp40fcdk5o\tapp-b58f9bb4f\t[\"bash\",\"-c\"]\n2\tb49oi05ph3bo7rdn2kvkkpk4tcigb3ts0o7sp40fcdk5o\tapp-6dd87bb7c6\tnull\n3\tb49oi05ph3bo7rdn2kvkkpk4tcigb3ts0o7sp40fcdk5o\tapp-57c67cc57d\t[\"bash\",\"-c\"]\n4\tb49oi05ph3bo7rdn2kvkkpk4tcigb3ts0o7sp40fcdk5o\tapp-655567846f\tnull\n```\n\nThe pods with the null commands are the bad replicas in this case, affected by the lost command issue.\n\nYou might see some pods with `null` commands for those replicas which stuck in `Pending` state because of insufficient resources on the provider, just ignore those.\n\nThey will start back again once provider regains enough capacity.\n\n**STEP3** - Patch the manifests\n\n```\nkubectl get deployment -l akash.network/manifest-service -A -o=jsonpath='{range .items[*]}{.metadata.namespace} {.metadata.name}{\"\\n\"}{end}' |\n  while read ns app; do\n    kubectl -n $ns rollout status --timeout=60s deployment/${app} >/dev/null 2>&1\n    rc=$?\n    if [[ $rc -ne 0 ]]; then\n      command=$(kubectl -n $ns rollout history deployment/${app} -o json | jq -sMc '.[0].spec.template.spec.containers[0].command | select(length > 0)')\n      if [[ $command != \"null\" && ! -z $command ]]; then\n        index=$(kubectl -n lease get manifests $ns -o json | jq --arg app $app -r '[.spec.group.services[]] | map(.name == $app) | index(true)')\n        if [[ $index == \"null\" || -z $index ]]; then\n          echo \"Error: index=$index, skipping $ns/$app ...\"\n          continue\n        fi\n        echo \"Patching manifest ${ns} to return the ${app} app its command: ${command} (index: ${index})\"\n        kubectl -n lease patch manifests $ns --type='json' -p='[{\"op\": \"add\", \"path\": \"/spec/group/services/'${index}'/command\", \"value\":'${command}'}]'\n\n        ### to debug:  --dry-run=client -o json | jq -Mc '.spec.group.services[0].command'\n        ### locate service by its name instead of using the index: kubectl -n lease get manifests $ns -o json | jq --indent 4 --arg app $app --argjson command $command -r '(.spec.group.services[] | select(.name == $app)) |= . + { command: $command }' | kubectl apply -f -\n        echo\n      else\n        echo \"Skipping ${ns}/${app} which does not use command in SDL.\"\n      fi\n     fi\n   done\n```\n\n_**Example Output**_:\n\n```\nPatching manifest 2anv3d7diieucjlga92fk8e5ej12kk8vmtkpi9fpju79a to return the cloud-sql-proxy its command: [\"sh\",\"-c\"]\nmanifest.akash.network/2anv3d7diieucjlga92fk8e5ej12kk8vmtkpi9fpju79a patched\n\nPatching manifest 2dl4vdk2f7ia1m0vme8nqkv0dadnnj15becr5pmfu9j22 to return the cloud-sql-proxy its command: [\"sh\",\"-c\"]\nmanifest.akash.network/2dl4vdk2f7ia1m0vme8nqkv0dadnnj15becr5pmfu9j22 patched\n\nPatching manifest 2k83g8gstuugse0952arremk4gphib709gi7b6q6srfdo to return the app its command: [\"bash\",\"-c\"]\nmanifest.akash.network/2k83g8gstuugse0952arremk4gphib709gi7b6q6srfdo patched\n\nPatching manifest 2qpj8537lq7tiv9fabdhk8mn4j75h3anhtqb1b881fhie to return the cloud-sql-proxy its command: [\"sh\",\"-c\"]\nmanifest.akash.network/2qpj8537lq7tiv9fabdhk8mn4j75h3anhtqb1b881fhie patched\n\nPatching manifest b49oi05ph3bo7rdn2kvkkpk4tcigb3ts0o7sp40fcdk5o to return the app its command: [\"bash\",\"-c\"]\nmanifest.akash.network/b49oi05ph3bo7rdn2kvkkpk4tcigb3ts0o7sp40fcdk5o patched\n```\n\n**STEP4** - Bounce the provider pod/service\n\n```\nkubectl -n akash-services delete pods -l app=akash-provider\n```\n\nThat's all.  The bad replicas will disappear on their own.\n\nExample with one namespace:\n\n_**Before**_:\n\n```\n0obkk0j6vdnp7qmsj477a88ml4i0639628gcn016smrg0   cloud-sql-proxy-69f75ffbdc-c5t69                                  1/1     Running            0                 20d\n0obkk0j6vdnp7qmsj477a88ml4i0639628gcn016smrg0   syncer-59c447b98c-t9xv9                                           1/1     Running            36 (15h ago)      20d\n0obkk0j6vdnp7qmsj477a88ml4i0639628gcn016smrg0   cloud-sql-proxy-56b5685cc7-qjvh2                                  0/1     CrashLoopBackOff   5587 (48s ago)    19d\n```\n\n_**After:**_\n\n```\n0obkk0j6vdnp7qmsj477a88ml4i0639628gcn016smrg0   cloud-sql-proxy-69f75ffbdc-c5t69                                  1/1     Running            0                  20d\n0obkk0j6vdnp7qmsj477a88ml4i0639628gcn016smrg0   syncer-59c447b98c-t9xv9                                           1/1     Running            36 (15h ago)       20d\n```\n\n## Persistent Storage Deployments\n\n* Persistent storage enabled deployments are of statefulset kind.\n* These do not have replicas and thus `CrashLoopBackOff` containers.\n* There is no impact, so you can skip them.\n* However, if you still want to fix their manifests, then apply the following procedure\n\n**STEP1** - Verify the statefulset deployments\n\nHere you can ignore the \"null\" ones, they are normal deployments just not using the command in their SDL manifest files.\n\n```\nkubectl get statefulset -l akash.network/manifest-service -A -o=jsonpath='{range .items[*]}{.metadata.namespace} {.metadata.name}{\"\\n\"}{end}' |\n  while read ns app; do\n    kubectl -n $ns get statefulset $app -o json | jq -r '[(.metadata | .namespace, .name), (.spec.template.spec.containers[0].command|tostring)] | @tsv'\n    echo\n   done\n```\n\n_**Example Output:**_\n\n```\n4ibg2ii0dssqtvb149thrd4a6a46g4mkcln2v70s6p20c\thnsnode\t[\"hsd\",\"--bip37=true\",\"--public-host=REDACTED\",\"--listen=true\",\"--port=REDACTED\",\"--max-inbound=REDACTED\"]\n\n66g95hmtta0bn8dajdcimo55glf60sne7cg8u9mv6j9l6\tpostgres\t[\"sh\",\"-c\"]\n\nesnphe9a86mmn3ibdcrncul82nnck7p4dpdj69ogu4b7o\tvalidator\tnull\n\nidr99rvt44lt6m1rp7vc1o0thpfqdqgcnfplj2a92ju86\tweb\tnull\n\nk9ch280ud97qle6bqli9bqk65pn7h07tohmrmq88sofq2\twiki\tnull\n\ntahcqnrs6dvo9ugee59q94nthgq5mm645e89cmml906m2\tnode\tnull\n```\n\n**STEP2** - Patch the manifest\n\n```\nkubectl get statefulset -l akash.network/manifest-service -A -o=jsonpath='{range .items[*]}{.metadata.namespace} {.metadata.name}{\"\\n\"}{end}' |\n  while read ns app; do\n    command=$(kubectl -n $ns get statefulset $app -o json | jq -Mc '.spec.template.spec.containers[0].command')\n    if [[ $command != \"null\" && ! -z $command ]]; then\n      echo \"Patching manifest ${ns} to return the ${app} its command: ${command}\"\n      kubectl -n lease patch manifests $ns --type='json' -p='[{\"op\": \"add\", \"path\": \"/spec/group/services/0/command\", \"value\":'${command}'}]'\n      ## to debug:  --dry-run=client -o json | jq -Mc '.spec.group.services[0].command'\n      echo\n    else\n      echo \"Skipping ${ns}/${app} which does not use command in SDL.\"\n    fi\n   done\n```\n\nThat's all. There is no need bouncing the `akash-provider` pod/service for the statefulset deployment.\n\n## Maintaining and Rotating Kubernetes/etcd Certificates: A How-To Guide\n\n> The following doc is based on [https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-certs/](https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-certs/) & [https://txconsole.com/certificate-renewal-manually-in-kubernetes/](https://txconsole.com/certificate-renewal-manually-in-kubernetes/)\n\n\n\nWhen K8s certs expire, you won't be able to use your cluster. Make sure to rotate your certs proactively.\n\nThe following procedure explains how to rotate them manually.\n\nEvidence that the certs have expired:\n\n```\nroot@node1:~# kubectl get nodes -o wide\nerror: You must be logged in to the server (Unauthorized)\n```\n\nYou can always view the certs expiration using the  `kubeadm certs check-expiration` command:\n\n```\nroot@node1:~# kubeadm certs check-expiration\n[check-expiration] Reading configuration from the cluster...\n[check-expiration] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'\n[check-expiration] Error reading configuration from the Cluster. Falling back to default configuration\n\nCERTIFICATE                         EXPIRES                  RESIDUAL TIME   CERTIFICATE AUTHORITY   EXTERNALLY MANAGED\nadmin.conf                          Feb 20, 2023 17:12 UTC   <invalid>       ca                      no\napiserver                           Mar 03, 2023 16:42 UTC   10d             ca                      no\n!MISSING! apiserver-etcd-client\napiserver-kubelet-client            Feb 20, 2023 17:12 UTC   <invalid>       ca                      no\ncontroller-manager.conf             Feb 20, 2023 17:12 UTC   <invalid>       ca                      no\n!MISSING! etcd-healthcheck-client\n!MISSING! etcd-peer\n!MISSING! etcd-server\nfront-proxy-client                  Feb 20, 2023 17:12 UTC   <invalid>       front-proxy-ca          no\nscheduler.conf                      Feb 20, 2023 17:12 UTC   <invalid>       ca                      no\n\nCERTIFICATE AUTHORITY   EXPIRES                  RESIDUAL TIME   EXTERNALLY MANAGED\nca                      Feb 18, 2032 17:12 UTC   8y              no\n!MISSING! etcd-ca\nfront-proxy-ca          Feb 18, 2032 17:12 UTC   8y              no\nroot@node1:~#\n```\n\n## Rotate K8s Certs\n\n### Backup etcd DB\n\nIt is crucial to back up your etcd DB as it contains your K8s cluster state! So make sure to backup your etcd DB first before rotating the certs!\n\n#### Take the etcd DB Backup\n\n> Replace the etcd key & cert with your locations found in the prior steps\n\n```\nexport $(grep -v '^#' /etc/etcd.env | xargs -d '\\n')\netcdctl -w table member list\netcdctl endpoint health --cluster -w table\netcdctl endpoint status --cluster -w table\netcdctl snapshot save node1.etcd.backup\n```\n\nYou can additionally backup the current certs:\n\n```\ntar czf etc_kubernetes_ssl_etcd_bkp.tar.gz /etc/kubernetes /etc/ssl/etcd\n```\n\n### Renew the Certs\n\n> IMPORTANT: For an HA Kubernetes cluster with multiple control plane nodes, the `kubeadm certs renew` command (followed by the `kube-apiserver`, `kube-scheduler`, `kube-controller-manage`r pods and `etcd.service` restart) needs to be executed on all the control-plane nodes, on one control plane node at a time, starting with the primary control plane node. This approach ensures that the cluster remains operational throughout the certificate renewal process and that there is always at least one control plane node available to handle API requests. To find out whether you have an HA K8s cluster (multiple control plane nodes) use this command `kubectl get nodes -l node-role.kubernetes.io/control-plane`\n\nNow that you have the etcd DB backup, you can rotate the K8s certs using the following commands:\n\n#### Rotate the k8s Certs\n\n```\nkubeadm certs renew all\n```\n\n#### Update your kubeconfig\n\n```\nmv -vi /root/.kube/config /root/.kube/config.old\ncp -pi /etc/kubernetes/admin.conf /root/.kube/config\n```\n\n#### Bounce the following services in this order\n\n```\nkubectl -n kube-system delete pods -l component=kube-apiserver\nkubectl -n kube-system delete pods -l component=kube-scheduler\nkubectl -n kube-system delete pods -l component=kube-controller-manager\nsystemctl restart etcd.service\n```\n\n#### Verify the Certs Status\n\n```\nkubeadm certs check-expiration\n```\n\nRepeat the process for all control plane nodes, one at a time, if you have a HA Kubernetes cluster.\n\n# Force New ReplicaSet Workaround\n\n\n\nThe steps outlined in this guide provide a workaround for known issue which occurs when a deployment update is attempted and fails due to the provider being out of resources.  This is happens because K8s won't destroy an old pod instance until it ensures the new one has been created.\\\n\\\nGitHub issue description can be found [here](https://github.com/akash-network/support/issues/82).\n\n## Requirements\n\n#### Install JQ\n\n```\napt -y install jq\n```\n\n## Steps to Implement\n\n### 1). Create \\`/usr/local/bin/akash-force-new-replicasets.sh\\` file\n\n```\ncat > /usr/local/bin/akash-force-new-replicasets.sh <<'EOF'\n#!/bin/bash\n#\n# Version: 0.2 - 25 March 2023\n# Files:\n# - /usr/local/bin/akash-force-new-replicasets.sh\n# - /etc/cron.d/akash-force-new-replicasets\n#\n# Description:\n# This workaround goes through the newest deployments/replicasets, pods of which can't get deployed due to \"insufficient resources\" errors and it then removes the older replicasets leaving the newest (latest) one.\n# This is only a workaround until a better solution to https://github.com/akash-network/support/issues/82 is found.\n#\n\nkubectl get deployment -l akash.network/manifest-service -A -o=jsonpath='{range .items[*]}{.metadata.namespace} {.metadata.name}{\"\\n\"}{end}' |\n  while read ns app; do\n    kubectl -n $ns rollout status --timeout=10s deployment/${app} >/dev/null 2>&1\n    rc=$?\n    if [[ $rc -ne 0 ]]; then\n      if kubectl -n $ns describe pods | grep -q \"Insufficient\"; then\n        OLD=\"$(kubectl -n $ns get replicaset -o json -l akash.network/manifest-service --sort-by='{.metadata.creationTimestamp}' | jq -r '(.items | reverse)[1:][] | .metadata.name')\"\n        for i in $OLD; do kubectl -n $ns delete replicaset $i; done\n      fi\n    fi\n  done\nEOF\n```\n\n### 2). Mark As Executable File\n\n```\nchmod +x /usr/local/bin/akash-force-new-replicasets.sh\n```\n\n### 3). Create Cronjob\n\nCreate the crontab job /`etc/cron.d/akash-force-new-replicasets` to run the workaround every 5 minutes.\n\n```\ncat > /etc/cron.d/akash-force-new-replicasets << 'EOF'\nPATH=/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin\nSHELL=/bin/bash\n\n*/5 * * * * root /usr/local/bin/akash-force-new-replicasets.sh\nEOF\n```\n\n\n# Kill Zombie Processes\n\n## Issue\n\nIt is possible for certain deployments to initiate subprocesses that do not properly implement the `wait()` function.\nThis improper handling can result in the formation of `<defunct>` processes, also known as \"zombie\" processes.\nZombie processes occur when a subprocess completes its task but still remains in the system's process table due to the parent process not reading its exit status.\nOver time, if not managed correctly, these zombie processes have the potential to accumulate and occupy all available process slots in the system, leading to resource exhaustion.\n\nThese zombie processes aren't too harmful much (they don't occupy cpu/mem / nor impact cgroup cpu/mem limits) unless they take up the whole process table space so no new processes will be able to spawn, i.e. the limit:\n\n```\n$ cat /proc/sys/kernel/pid_max\n4194304\n```\n\nTo address this issue, tenants should ensure they manage and terminate child processes appropriately to prevent them from becoming zombie processes.\n\nOne of the correct ways to approach that would be this example:\n\n```\n#!/bin/bash\n\n# Start the first process\n./my_first_process &\n\n# Start the second process\n./my_second_process &\n\n# Wait for any process to exit\nwait -n\n\n# Exit with status of process that exited first\nexit $?\n```\n\nOr using a proper container init (tini) / supervision system (such as s6, supervisor, runsv, ...) that would reap adopted child processes.\n\nRefs\n\n- https://docs.docker.com/config/containers/multi-service_container/#use-a-wrapper-script\n- https://blog.phusion.nl/2015/01/20/docker-and-the-pid-1-zombie-reaping-problem/\n\n\n## Example of the zombie processes on the provider\n\nSomeone's running a wrongly configured image, with the `service ssh start` in it, which fails to start, hence creating bunch of `<defunct>` zombie `sshd` processes growing every `20` seconds:\n\n```\nroot      712532  696516  0 14:28 ?        00:00:00      \\_ [bash] <defunct>\nsyslog    713640  696516  0 14:28 ?        00:00:00      \\_ [sshd] <defunct>\nroot      807481  696516  0 14:46 ?        00:00:00      \\_ [bash] <defunct>\nroot      828096  696516  0 14:50 ?        00:00:00      \\_ [bash] <defunct>\nroot      835000  696516  0 14:51 pts/0    00:00:00      \\_ [haproxy] <defunct>\nroot      836102  696516  0 14:51 ?        00:00:00      \\_ SCREEN -S webserver\nroot      836103  836102  0 14:51 ?        00:00:00      |   \\_ /bin/bash\nroot      856974  836103  0 14:55 ?        00:00:00      |       \\_ caddy run\nroot      849813  696516  0 14:54 pts/0    00:00:00      \\_ [haproxy] <defunct>\npollina+  850297  696516  1 14:54 ?        00:00:40      \\_ haproxy -f /etc/haproxy/haproxy.cfg\nroot      870519  696516  0 14:58 ?        00:00:00      \\_ SCREEN -S wallpaper\nroot      870520  870519  0 14:58 ?        00:00:00      |   \\_ /bin/bash\nroot      871826  870520  0 14:58 ?        00:00:00      |       \\_ bash change_wallpaper.sh\nroot     1069387  871826  0 15:35 ?        00:00:00      |           \\_ sleep 20\nsyslog    893600  696516  0 15:02 ?        00:00:00      \\_ [sshd] <defunct>\nsyslog    906839  696516  0 15:05 ?        00:00:00      \\_ [sshd] <defunct>\nsyslog    907637  696516  0 15:05 ?        00:00:00      \\_ [sshd] <defunct>\nsyslog    913724  696516  0 15:06 ?        00:00:00      \\_ [sshd] <defunct>\nsyslog    914913  696516  0 15:06 ?        00:00:00      \\_ [sshd] <defunct>\nsyslog    922492  696516  0 15:08 ?        00:00:00      \\_ [sshd] <defunct>\n```\n\n## Steps to implement a workaround for the providers\n\nProviders can't control this, hence they are recommended to implement the following workaround across all worker nodes.\n\n1. create `/usr/local/bin/kill_zombie_parents.sh` script\n\n```\ncat > /usr/local/bin/kill_zombie_parents.sh <<'EOF'\n#!/bin/bash\n\n# This script detects zombie processes and kills their parent processes.\n\n# Get a list of zombie processes.\nzombies=$(ps -eo pid,ppid,stat,cmd | awk '$3 == \"Z\" { print $2 }' | sort -u)\n\n# If there are no zombies, exit.\nif [[ -z \"$zombies\" ]]; then\n    #echo \"No zombie processes found.\"\n    exit 0\nfi\n\n# Kill parent processes of the zombies.\nfor parent in $zombies; do\n    # Double check that the parent process is still alive.\n    if kill -0 $parent 2>/dev/null; then\n        echo \"Killing parent process $parent.\"\n        kill -TERM $parent\n        sleep 2  # Give the process a chance to terminate.\n        \n        # Force kill if it didn't terminate.\n        if kill -0 $parent 2>/dev/null; then\n            echo \"Force killing parent process $parent.\"\n            kill -KILL $parent\n        fi\n    fi\ndone\nEOF\n```\n\n2. mark it as executable\n```\nchmod +x /usr/local/bin/kill_zombie_parents.sh\n```\n\n3. create cronjob\n\nThis way the workaround will automatically run every 5 minutes.\n\n```\ncat > /etc/cron.d/kill_zombie_parents << 'EOF'\nPATH=/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin\nSHELL=/bin/bash\n\n*/5 * * * * root /usr/local/bin/kill_zombie_parents.sh\nEOF\n```\n\n\n# Close Leases Based on Image\n\n\n\nBelow is the suboptimal way of terminating the leases with the selected (unwanted) images (until Akash natively supports that).\n\nSuboptimal because once the deployment gets closed the provider will have to be restarted to recover from the `account sequence mismatch` error. Providers already do it automatically through the K8s's `liveness` probe set to the akash-provider deployment.\n\nThe other core problem is that the `image` is **unknown** until the client transfers the SDL to the provider (`tx send-manifest`) which can only happen after provider bids, client accepts the bid.\n\nFollow the steps associated with your Akash Provider install method:\n\n* [Akash Provider Built with Helm Charts](#akash-provider-built-with-helm-charts)\n* [Praetor Based Akash Provider Installs](#praetor-based-akash-provider-installs)\n\n## Akash Provider Built with Helm Charts\n\n### Create Script\n\n* Create script file -  `/usr/local/bin/akash-kill-lease.sh` - and populate with the following content:\n\n```\n#!/bin/bash\n# Files:\n# - /etc/cron.d/akash-kill-lease\n# - /usr/local/bin/akash-kill-lease.sh\n\n# Uncomment IMAGES to activate this script.\n# IMAGES=\"packetstream/psclient\"\n\n# You can provide multiple images, separated by the \"|\" character as in this example:\n# IMAGES=\"packetstream/psclient|traffmonetizer/cli\"\n\n# Quit if no images were specified\ntest -z $IMAGES && exit 0\n\nkubectl -n lease get manifests -o json | jq --arg md_lid \"akash.network/lease.id\" -r '.items[] | [(.metadata.labels | .[$md_lid+\".owner\"], .[$md_lid+\".dseq\"], .[$md_lid+\".gseq\"], .[$md_lid+\".oseq\"]), (.spec.group | .services[].image)] | @tsv' | grep -Ei \"$IMAGES\" | while read owner dseq gseq oseq image; do kubectl -n akash-services exec -i $(kubectl -n akash-services get pods -l app=akash-provider -o name) -- env AKASH_OWNER=$owner AKASH_DSEQ=$dseq AKASH_GSEQ=$gseq AKASH_OSEQ=$oseq provider-services tx market bid close; done\n\n```\n\n### Make the Script Executable\n\n```\nchmod +x /usr/local/bin/akash-kill-lease.sh\n```\n\n### Create Cron Job\n\n* Create the Cron Job file - `/etc/cron.d/akash-kill-lease` - with the following content:\n\n```\n# Files:\n# - /etc/cron.d/akash-kill-lease\n# - /usr/local/bin/akash-kill-lease.sh\n\nPATH=/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin\nSHELL=/bin/bash\n\n*/5 * * * * root /usr/local/bin/akash-kill-lease.sh\n```\n\n## Praetor Based Akash Provider Installs\n\n### Cron Job\n\n* Create cron file - `/etc/cron.d/akash-kill-lease` - and populate with the following content:\n\n```\n# Files:\n# - /etc/cron.d/akash-kill-lease\n# - /usr/local/bin/akash-kill-lease.sh\n\nPATH=/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin\nSHELL=/bin/bash\n\n# TODO: change \"deathless\" with your username that runs akash-provider.service\n# $ grep ^User /etc/systemd/system/akash-provider.service\n# User=deathless\n\n*/5 * * * * deathless /usr/local/bin/akash-kill-lease.sh\n```\n\n### Kill Lease Script\n\nCreate script file - `/usr/local/bin/akash-kill-lease.sh` - and populate with the following content:\n\n```\n#!/bin/bash\n# Files:\n# - /etc/cron.d/akash-kill-lease\n# - /usr/local/bin/akash-kill-lease.sh\n\n# Optimized for Praetor based provider\n\n# Uncomment IMAGES to activate this script.\nIMAGES=\"packetstream/psclient\"\n\n# You can provide multiple images, separated by the \"|\" character as in this example:\nIMAGES=\"packetstream/psclient|traffmonetizer/cli\"\n\n# Quit if no images were specified\ntest -z $IMAGES && exit 0\n\nexport AKASH_KEYRING_BACKEND=test\n## TODO: point to your key: check \"AKASH_KEYRING_BACKEND=test provider-services keys list\" -- you can export your key from the \"file\" or \"os\" keyring backend or recover using your mnemonic seed: \"AKASH_KEYRING_BACKEND=test provider-services keys add default --recover\";  the key password is in ~/.praetor/key-pass.txt file\nexport AKASH_FROM=default\nexport AKASH_YES=1\n\nexport AKASH_GAS=auto\nexport AKASH_GAS_PRICES=0.025uakt\nexport AKASH_GAS_ADJUSTMENT=1.5\n\nexport AKASH_BROADCAST_MODE=sync\n\n## TODO: point AKASH_NODE to your RPC node\nexport AKASH_NODE=http://node.d3akash.cloud:26657\nexport AKASH_CHAIN_ID=akashnet-2\n#export AKASH_CHAIN_ID=$(curl -s -k $AKASH_NODE/status | jq -r '.result.node_info.network')\n\nkubectl -n lease get manifests -o json | jq --arg md_lid \"akash.network/lease.id\" -r '.items[] | [(.metadata.labels | .[$md_lid+\".owner\"], .[$md_lid+\".dseq\"], .[$md_l\nid+\".gseq\"], .[$md_lid+\".oseq\"]), (.spec.group | .services[].image)] | @tsv' | grep -Ei \"$IMAGES\" | while read owner dseq gseq oseq image; do\n  env AKASH_OWNER=$owner AKASH_DSEQ=$dseq AKASH_GSEQ=$gseq AKASH_OSEQ=$oseq ~/bin/provider-services tx market bid close\n\n  # to address: \"Unknown desc = account sequence mismatch, expected 5605, got 5604: incorrect account sequence\" error, provider service has to restart\n  sudo systemctl restart akash-provider\ndone\n```\n\n## Provider Bid Script Migration - GPU Models\n\n\n\nA new bid script for Akash Providers has been released that now includes the ability to specify pricing of multiple GPU models.\n\nThis document details the recommended procedure for Akash providers needing migration to the new bid script from prior versions.\n\n#### New Features of Bid Script Release\n\n* Support for parameterized price targets (configurable through the Akash/Provider Helm chart values), eliminating the need to manually update your bid price script\n* Pricing based on GPU model, allowing you to specify different prices for various GPU models\n\n#### How to Migrate from Prior Bid Script Releases\n\n#### STEP 1 -  Backup your current bid price script\n\n> This command will produce an `old-bid-price-script.sh` file which is your currently active bid price script with your custom modifications\n\n```\nhelm -n akash-services get values akash-provider -o json | jq -r '.bidpricescript | @base64d' > old-bid-price-script.sh\n```\n\n#### STEP 2 - Verify Previous Custom Target Price Values\n\n```\ncat old-bid-price-script.sh | grep ^TARGET\n```\n\n#### Example/Expected Output\n\n```\n# cat old-bid-price-script.sh | grep ^TARGET\nTARGET_CPU=\"1.60\"          # USD/thread-month\nTARGET_MEMORY=\"0.80\"       # USD/GB-month\nTARGET_HD_EPHEMERAL=\"0.02\" # USD/GB-month\nTARGET_HD_PERS_HDD=\"0.01\"  # USD/GB-month (beta1)\nTARGET_HD_PERS_SSD=\"0.03\"  # USD/GB-month (beta2)\nTARGET_HD_PERS_NVME=\"0.04\" # USD/GB-month (beta3)\nTARGET_ENDPOINT=\"0.05\"     # USD for port/month\nTARGET_IP=\"5\"              # USD for IP/month\nTARGET_GPU_UNIT=\"100\"      # USD/GPU unit a month\n```\n\n### STEP 3 - Backup Akash/Provider Config\n\n> This command will backup your akash/provider config in the  `provider.yaml` file (excluding the old bid price script)\n\n```\nhelm -n akash-services get values akash-provider | grep -v '^USER-SUPPLIED VALUES' | grep -v ^bidpricescript > provider.yaml\n```\n\n### STEP 4 - Update provider.yaml File Accordingly\n\n> Update your `provider.yaml` file with the price targets you want. If you don't specify these keys, the bid price script will default values shown below\n\n`price_target_gpu_mappings` sets the GPU price in the following way and in the example provided:\n\n* `a100` nvidia models will be charged by `120` USD/GPU unit a month\n* `t4` nvidia models will be charged by `80` USD/GPU unit a month\n* Unspecified nvidia models will be charged `130` USD/GPU unit a month (if `*` is not explicitly set in the mapping it will default to `100` USD/GPU unit a month)\n* Extend with more models your provider is offering if necessary with syntax of `<model>=<USD/GPU unit a month>`\n\n```\nprice_target_cpu: 1.60\nprice_target_memory: 0.80\nprice_target_hd_ephemeral: 0.02\nprice_target_hd_pers_hdd: 0.01\nprice_target_hd_pers_ssd: 0.03\nprice_target_hd_pers_nvme: 0.04\nprice_target_endpoint: 0.05\nprice_target_ip: 5\nprice_target_gpu_mappings: \"a100=120,t4=80,*=130\"\n```\n\n### STEP 5 - Download New Bid Price Script\n\n```\nmv -vi price_script_generic.sh price_script_generic.sh.old\n\nwget https://raw.githubusercontent.com/akash-network/helm-charts/main/charts/akash-provider/scripts/price_script_generic.sh\n```\n\n### STEP 6 - Upgrade Akash/Provider Chart to Version 6.0.5\n\n```\nhelm repo update akash\n\nhelm search repo akash/provider\n```\n\n#### Expected/Example Output\n\n```\n# helm repo update akash\n# helm search repo akash/provider\nNAME          \tCHART VERSION\tAPP VERSION\tDESCRIPTION                          \nakash/provider\t6.0.5        \t0.4.6      \tInstalls an Akash provider (required)\n```\n\n### STEP 7 - Upgrade akash-provider Deployment with New Bid Script\n\n```\nhelm upgrade akash-provider akash/provider -n akash-services -f provider.yaml --set bidpricescript=\"$(cat price_script_generic.sh | openssl base64 -A)\"\n```\n\n#### Verification of Bid Script Update\n\n```\nhelm list -n akash-services | grep akash-provider\n```\n\n#### Expected/Example Output\n\n```\n# helm list -n akash-services | grep akash-provider\nakash-provider         \takash-services\t28      \t2023-09-19 12:25:33.880309778 +0000 UTC\tdeployed\tprovider-6.0.5                \t0.4.6 \n```\n\n## GPU Provider Troubleshooting\n\n\n\nShould your Akash Provider encounter issues during the installation process or in post install hosting of GPU resources, follow the troubleshooting steps in this guide to isolate the issue.\n\n> _**NOTE**_ - these steps should be conducted on each Akask Provider/Kubernetes worker nodes that host GPU resources unless stated otherwise within the step\n\n* [Basic GPU Resource Verifications](#basic-gpu-resource-verifications)\n* [Examine Linux Kernel Logs for GPU Resource Errors and Mismatches](#examine-linux-kernel-logs-for-gpu-resource-errors-and-mismatches)\n* [Ensure Correct Version/Presence of NVIDIA Device Plugin](#ensure-correct-version-presence-of-nvidia-device-plugin)\n* [CUDA Drivers Fabric Manager](#cuda-drivers-fabric-manager)\n\n## Basic GPU Resource Verifications\n\n* Conduct the steps in this section for basic verification and to ensure the host has access to GPU resources\n\n### Prep/Package Installs\n\n```\napt update && apt -y install python3-venv\n\npython3 -m venv /venv\nsource /venv/bin/activate\npip install torch\n```\n\n### Confirm GPU Resources Available on Host\n\n> _**NOTE**_ - example verification steps were conducted on a host with a single NVIDIA T4 GPU resource.  Your output will be different based on the type and number of GPU resources on the host.\n\n```\nnvidia-smi -L\n```\n\n#### Example/Expected Output\n\n```\n# nvidia-smi -L\n\nGPU 0: Tesla T4 (UUID: GPU-faa48437-7587-4bc1-c772-8bd099dba462)\n```\n\n### Confirm CUDA Install & Version\n\n```\npython3 -c \"import torch;print(torch.version.cuda)\"\n```\n\n#### Example/Expected Output\n\n```\n# python3 -c \"import torch;print(torch.version.cuda)\"\n\n11.7\n```\n\n### Confirm CUDA GPU Support iis Available for Hosted GPU Resources\n\n```\npython3 -c \"import torch; print(torch.cuda.is_available())\"\n```\n\n#### Example/Expected Output\n\n```\n# python3 -c \"import torch; print(torch.cuda.is_available())\"\n\nTrue\n```\n\n## Examine Linux Kernel Logs for GPU Resource Errors and Mismatches\n\n```\ndmesg -T | grep -Ei 'nvidia|nvml|cuda|mismatch'\n```\n\n#### Example/Expected Output\n\n> _**NOTE**_ - example output is from a healthy host which loaded NVIDIA drivers successfully and has no version mismatches.  Your output may look very different if there are issues within the host.\n\n```\n# dmesg -T | grep -Ei 'nvidia|nvml|cuda|mismatch'\n\n[Thu Sep 28 19:29:02 2023] nvidia: loading out-of-tree module taints kernel.\n[Thu Sep 28 19:29:02 2023] nvidia: module license 'NVIDIA' taints kernel.\n[Thu Sep 28 19:29:02 2023] nvidia-nvlink: Nvlink Core is being initialized, major device number 237\n[Thu Sep 28 19:29:02 2023] NVRM: loading NVIDIA UNIX x86_64 Kernel Module  535.104.05  Sat Aug 19 01:15:15 UTC 2023\n[Thu Sep 28 19:29:02 2023] nvidia-modeset: Loading NVIDIA Kernel Mode Setting Driver for UNIX platforms  535.104.05  Sat Aug 19 00:59:57 UTC 2023\n[Thu Sep 28 19:29:02 2023] [drm] [nvidia-drm] [GPU ID 0x00000004] Loading driver\n[Thu Sep 28 19:29:03 2023] audit: type=1400 audit(1695929343.571:3): apparmor=\"STATUS\" operation=\"profile_load\" profile=\"unconfined\" name=\"nvidia_modprobe\" pid=300 comm=\"apparmor_parser\"\n[Thu Sep 28 19:29:03 2023] audit: type=1400 audit(1695929343.571:4): apparmor=\"STATUS\" operation=\"profile_load\" profile=\"unconfined\" name=\"nvidia_modprobe//kmod\" pid=300 comm=\"apparmor_parser\"\n[Thu Sep 28 19:29:04 2023] [drm] Initialized nvidia-drm 0.0.0 20160202 for 0000:00:04.0 on minor 0\n[Thu Sep 28 19:29:05 2023] nvidia_uvm: module uses symbols nvUvmInterfaceDisableAccessCntr from proprietary module nvidia, inheriting taint.\n[Thu Sep 28 19:29:05 2023] nvidia-uvm: Loaded the UVM driver, major device number 235.\n```\n\n## Ensure Correct Version/Presence of NVIDIA Device Plugin\n\n> _**NOTE**_ - conduct this verification step on the Kubernetes control plane node on which Helm was installed during your Akash Provider build\n\n```\nhelm -n nvidia-device-plugin list\n```\n\n#### Example/Expected Output\n\n```\n# helm -n nvidia-device-plugin list\n\nNAME    NAMESPACE               REVISION    UPDATED                                    STATUS      CHART                          APP VERSION\nnvdp    nvidia-device-plugin    1           2023-09-23 14:30:34.18183027 +0200 CEST    deployed    nvidia-device-plugin-0.14.1    0.14.1 \n```\n\n## CUDA Drivers Fabric Manager\n\n* In rare circumstances it has been found that the CUDA Drivers Fabric Manager needs to be installed on worker nodes hosting GPU resources\n* If the output of the `torch.cuda.is_available()` command - covered in prior section in this doc - is an error condition, consider installing the CUDA Drivers Fabric Manager to resolve issue\n* Frequently encountered error message encounter when issue exists:\\\n  \\\n  `torch.cuda.is_available() function: Error 802: system not yet initialized (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:109.)`\n* Further details on the CUDA Drivers Fabric Manager are available [here](https://forums.developer.nvidia.com/t/error-802-system-not-yet-initialized-cuda-11-3/234955)\n\n> _**NOTE**_ - replace `525` in the following command with the NVIDIA driver version used on your host\n\n```\napt-get install cuda-drivers-fabricmanager-525\n```","description":null,"slug":"docs/providers/provider-faq-and-guide"},{"title":"Example GPU SDLs","body":"\n\nIn the following sections example GPU enabled Akash SDL examples are provided:\n\n- [Specific GPU Vendor](#specific-gpu-vendor)\n  - [Overview](#overview)\n  - [SDL Example](#sdl-example)\n  - [Confirmation](#confirmation)\n  - [Specific GPU Vendor \\& Model](#specific-gpu-vendor--model)\n  - [Overview](#overview-1)\n  - [SDL Example](#sdl-example-1)\n  - [Confirmation](#confirmation-1)\n- [Specific GPU Vendor \\& List of Acceptable Models](#specific-gpu-vendor--list-of-acceptable-models)\n  - [Overview](#overview-2)\n  - [SDL Example](#sdl-example-2)\n  - [Confirmation](#confirmation-2)\n\n\n## Specific GPU Vendor\n\n### Overview\n\nIn the example SDL provided below the following request is made:\n\n* GPU Quantity: 1\n* GPU Vendor: NVIDIA\n\nBased on these inclusions in the SDL - only providers that have NVIDIA chips will respond with a bid.\n\n> _**NOTE**_ - using this SDL example no specific GPU model/type (I.e. NVIDIA A100) is specified.  Based on this syntax any provider with a NVIDIA GPU will respond with a bid.  If a specific GPU model/type is needed - use the SDL example [here](#specific-gpu-vendor--model).\n\n### SDL Example\n\n```\n---\nversion: \"2.0\"\n\nservices:\n  obtaingpu:\n    image: ubuntu:22.04\n    command:\n      - \"sh\"\n      - \"-c\"\n    args:\n      - 'uptime;\n        nvidia-smi;\n        sleep infinity'\n    expose:\n      - port: 8080\n        as: 80\n        to:\n          - global: true\n\nprofiles:\n  compute:\n    obtaingpu:\n      resources:\n        cpu:\n          units: 1.0\n        memory:\n          size: 1Gi\n        gpu:\n          units: 1\n          attributes:\n            vendor:\n              nvidia:\n        storage:\n          size: 1Gi\n  placement:\n    akash:\n      pricing:\n        obtaingpu: \n          denom: uakt\n          amount: 100000\n\ndeployment:\n  obtaingpu:\n    akash:\n      profile: obtaingpu\n      count: 1\n\n```\n\n### Confirmation\n\n* The SDL used in the provided example prints the GPU Model/Chip Type to the logs as depicted below\n* We can use these logs to determine the success of the deployment\n\n\n\n![](../../../assets/gpuCheck.png)\n\n\n### Specific GPU Vendor & Model\n\n### Overview\n\nIn the example SDL provided below the following request is made:\n\n* GPU Quantity: 1\n* GPU Vendor: NVIDIA\n* GPU Model: T4\n\nBased on these inclusions in the SDL - only providers that have NVIDIA T4 chips will respond with a bid.\n\n### SDL Example\n\n```\n---\nversion: \"2.0\"\n\nservices:\n  obtaingpu:\n    image: ubuntu:22.04\n    command:\n      - \"sh\"\n      - \"-c\"\n    args:\n      - 'uptime;\n        nvidia-smi;\n        sleep infinity'\n    expose:\n      - port: 8080\n        as: 80\n        to:\n          - global: true\n\nprofiles:\n  compute:\n    obtaingpu:\n      resources:\n        cpu:\n          units: 1.0\n        memory:\n          size: 1Gi\n        gpu:\n          units: 1\n          attributes:\n            vendor:\n              nvidia:\n                - model: t4\n        storage:\n          size: 1Gi\n  placement:\n    akash:\n      pricing:\n        obtaingpu: \n          denom: uakt\n          amount: 100000\n\ndeployment:\n  obtaingpu:\n    akash:\n      profile: obtaingpu\n      count: 1\n\n```\n\n### Confirmation\n\n* The SDL used in the provided example prints the GPU Model/Chip Type to the logs as depicted below\n* We can use these logs to determine the success of the deployment and confirm that the requested GPU model/type was allocated\n\n\n![](../../../assets/gpuCheck.png)\n\n\n## Specific GPU Vendor & List of Acceptable Models\n\n### Overview\n\nIn the example SDL provided below the following request is made:\n\n* GPU Quantity: 1\n* GPU Vendor: NVIDIA\n* GPU Model: T4 or A4000\n\nBased on these inclusions in the SDL - only providers that have NVIDIA T4 OR A4000 chips will respond with a bid.  The purpose of this SDL use would be in scenario in which specific models of GPU are necessary but there are multiple, acceptable models.&#x20;\n\n&#x20;In this example we specific that providers with NVIDIA T4 or A4000 models should bid on the deployment.  But it is possible to list many different models if desired.\n\n### SDL Example\n\n```\n---\nversion: \"2.0\"\n\nservices:\n  obtaingpu:\n    image: ubuntu:22.04\n    command:\n      - \"sh\"\n      - \"-c\"\n    args:\n      - 'uptime;\n        nvidia-smi;\n        sleep infinity'\n    expose:\n      - port: 8080\n        as: 80\n        to:\n          - global: true\n\nprofiles:\n  compute:\n    obtaingpu:\n      resources:\n        cpu:\n          units: 1.0\n        memory:\n          size: 1Gi\n        gpu:\n          units: 1\n          attributes:\n            vendor:\n              nvidia:\n                - model: t4\n                - model: a4000\n        storage:\n          size: 1Gi\n  placement:\n    akash:\n      pricing:\n        obtaingpu: \n          denom: uakt\n          amount: 100000\n\ndeployment:\n  obtaingpu:\n    akash:\n      profile: obtaingpu\n      count: 1\n\n\n```\n\n### Confirmation\n\n* The SDL used in the provided example prints the GPU Model/Chip Type to the logs as depicted below\n* We can use these logs to determine the success of the deployment and confirm that the selected GPU model/type was allocated\n\n\n![](../../../assets/gpuCheck.png)\n\n","description":null,"slug":"docs/testnet/example-gpu-sdls"},{"title":"GPU Testnet Client Instructions","body":"\nDeployments may be launched on the GPU Testnet via the Akash Console,  Cloudmos Deploy, and the Akash CLI.   Follow the links provided below based on your preferred deployment tool.\n\n* [Akash Console](#akash-console)\n* [Cloudmos Deploy](#cloudmos-deploy)\n* [Akash CLI](detailed-steps/)\n\n\n## Akash Console\n\n### Akash Console for GPU Testnet Overview\n\nAkash Console is a web based application that makes it easy to deploy applications onto the Akash Network. Post deployment, Akash Console provides a dashboard to view the status and details of workloads. The dashboard also has the ability to perform administrative tasks including closing the deployment, updating the deployment, redeploying, and increasing the funding available to the deployment.\n\nThis guide will cover the following topics that are relevant for Akash Console use in the GPU Testnet:\n\n* [Akash Console Access](#akash-console-access)\n* [GPU Testnet Settings](#gpu-testnet-settings)\n* [Keplr Account Selection and Funding](#keplr-account-selection-and-funding)\n* [MInesweeper Deployment Example](#minesweeper-deployment-example)\n\n\n## Akash Console Access\n\n### Before Getting Started\n\nThe Keplr browser extension must be installed and with sufficient funds (5AKT minimum for a single deployment plus a small amount for transaction fees).\n\n&#x20;Follow our [Keplr Walle](https://docs.akash.network/tokens-and-wallets/keplr)t guide to create your first wallet if necessary.\n\n### Akash Console Access\n\nThe Akash Console web app is available via the following URL:\n\n* [https://console.akash.network/](https://console.akash.network/)\n\n\n## GPU Testnet Settings\n\n### Configure GPU Testnet Network Settings\n\n* Prior to launching a deployment, we need need to configure a few settings for GPU Testnet use\n\n#### Access Settings\n\n* Begin by accessing the Settings via selection of the gear icon and as depicted below\n\n\n\n![](../../../assets/testnetSettings.png)\n\n#### Initial Settings\n\n* The Console is most likely current configured to interact with the Akash Mainnet\n* In the depiction below we find this to be the case.  The Console is connected `mainnet-2` which is the current Mainnet version.\n\n\n\n![](../../../assets/initialSettings.png)\n\n#### Update Settings for GPU Testnet Use\n\n* To interact with the GPU Testnet via the Akash Console, select the `Change` button in the `RPC Endpoint` row\n\n\n\n![](../../../assets/changeRPC.png)\n\n* Update the RPC Endpoint to the following value and as depicted in the screenshot\n* Press the `Verify` button following the updated value\n\n```\nhttps://rpc.testnet-02.aksh.pw:443\n```\n\n\n\n![](../../../assets/updatedRPC.png)\n\n#### Add New Chain in Keplr\n\n* Add the Akash Testnet network to Keplr by pressing the `Add Custom Chain For Testnet` button and then press the `Submit` buttton when complete\n\n\n\n![](../../../assets/acceptNewKeplExt.png)\n\n\n## Keplr Account Selection and Funding\n\n### Keplr Account Selection\n\nSelect a pre-existing tAkash account within Keplr.  If you do not have a pre-existing Akash account - use the `Add Wallet` feature within Keplr to create a new account.\n\n\n\n![](../../../assets/keplrAccountSelection.png)\n\n### Connect Wallet\n\nUse the `Connect Wallet` button to connect the account selected in Keplr in the prior step to the Akash Console.\n\n\n\n![](../../../assets/connectWallet.png)\n\n### Fund Selected Account via Testnet Faucet\n\nVisit the Testnet faucet [here](http://faucet.testnet-02.aksh.pw/) to fund your provider account. &#x20;\n\nEnter the address of the account selected in previous step.  The address of the account can be copied from the Keplr wallet.\n\n##### Example Use of Akash Testnet Faucet\n\n\n\n![](../../../assets/testnetFaucet.png)\n\n##### Example/Expected Faucet Output\n\nThe following screen shot shows an example/expected output following successful submission to the Testnet faucet.  Note that the displayed transaction hash will be unique and different than the one displayed in this example.  Receipt of this screen confirms successful funding of your account with `25AKT`.\n\n\n\n\n![](../../../assets/testnetFaucet.png)\n\n## Minesweeper Deployment Example\n\nIn this section we will use the Akash Console to launch an example Minesweeper deployment on the Akash Testmet. You can follow the same process for any other workload so long as it is containerized and you have an appropriate SDL.\n\n### STEP 1 - Create the Deployment\n\n* Begin the process of creating a new deployment by selecting the `+` symbol from the left hand navigation pane\n* A number of deployment types are presented in the gallery\n* For our purposes we will import a SDL for the Minesweeper deployment example.  Select `Import SDL` to proceed.\n\n\n\n\n![](../../../assets/createDeployment.png)\n\n#### Copy Awesome Akash Minesweeper SDL\n\n* The Minesweeper SDL (Stack Definition Language) file - which is the recipe for an Akash Deployment - can be found in the Awesome Akash repository [here](https://github.com/akash-network/awesome-akash/blob/master/minesweeper/deploy.yaml).\n* Copy the contents of the SDL into the Akash Console editor as demonstrated below.\n* Select `Save & Close` when complete\n\n\n\n![](../../../assets/minesweeperSDL.png)\n\n#### Name Deployment and Proceed\n\n* Optionally assign the deployment a useful/descript name\n* Select `Create Deployment` when complete to proceed to the next step\n\n\n\n![](../../../assets/nameDeployment.png)\n\n### STEP 2 - Pre-Flight Check\n\n* A `Pre-Flight` screen is displayed which ensures all necessary criteria - such as a connected wallet, sufficient funding to launch a deployment, etc - are presented\n* Most likely - if this is your first time using the Akash Console - a `Missing Certificate` warning will be displayed\n* Select the `Create Certificate` option and approve Keplr gas fee prompts that follow\n* Once the creation of the certificate step is complete the warning of `Missing Certificate` should update to `Valid Certificate`\n* Select the `Next` button when all Pre-Flight Checks are green\n* Accept additional gas fee prompts by Keplr to allow the dpeloyment creation to complete\n\n\n\n\n![](../../../assets/prefllightCheck.png)\n\n### STEP 3 - Select Provider\n\n* A list of Akash providers who bid on the deployment is presented\n* Select the Akash provider of your choice\n\n> NOTE - ensure to select `ALL` instead of the default `Only Audited` selection near the top right of the pane for Testnet purposes.  This will allow bids from all providers to display.\n\n\n\n![](../../../assets/prefllightCheck.png)\n\n\n* Following the initial select of the provider - new options will be presented as depicted below\n* Select `Submit Deploy Request` to complete the deployment process\n* Accept Keplr gas fee prompt to proceed\n\n\n\n\n![](../../../assets/sendManifest.png)\n\n### STEP 4 - Deployment Confirmation and Testing\n\n* Following successful deployment of the Minesweeper SDL to the selected Akash provider, a status page will be presented as depicted in the example below\n* This page can be used to view logs of the deployment and obtain general deployment info\n* Allow the deployment a couple of minutes to fully deploy and then test Minesweeper by selecting the generated URL of the deployment\n\n##### Select URL to Test Deployment\n\n\n\n![](../../../assets/sendManifest.png)\n\n##### Expected Result of Initial Deployment Testing\n\n\n\n![](../../../assets/minesweeperComplete.png)\n\n### Summary and Next Steps\n\n* This completes our walk through of a example SDL and associated deployment onto the Akash Testnet network\n* The steps presented in this guide can be used for any future deployments and associated SDLs\n\n## Cloudmos Deploy\n\n### Deployment Instructions\n\nThe developers of Cloudmos Deploy have detailed the steps necessary to launch deployments on the GPU Testnet via these [instructions](https://medium.com/@cloudmos/akash-gpu-testnet-02-guide-a79fd5573295).\n\n### Additional Assistance\n\nPlease visit the following Discord channels if further assistance is needed while using Cloudmos Deploy for GPU Testnet.\n\n* [General GPU Testnet Support](https://discord.com/channels/747885925232672829/1067981460461588480)\n* [Cloudmos Deploy Support Channe](https://discord.com/channels/747885925232672829/874808726513651782)\n\n\n## Akash CLI for GPU Testnet\n\nExplore detailed steps and options of the Akash CLI.  In this guide we will define each environment variable and use within each command.\n\n#### Overview of Getting Started with Akash CLI Steps\n\n* [Install Akash](#install-akash-cli)\n* [Create an Account](#create-an-account)\n* [Fund your Account](#fund-your-account)\n* [Configure your Network](#configure-your-network)\n* [Create your Configuration](#create-your-configuration)\n* [Create your Certificate](#create-your-certificate)\n* [Create your Deployment](#create-your-deployment)\n* [View your Bids](#view-your-bids)\n* [Create a Lease](#create-a-lease)\n* [Send the Manifest](#send-the-manifest)\n* [Update the Deployment](#update-the-deployment)\n* [Close Deployment](#close-deployment)\n\n## Install Akash CLI\n\nSelect a tab below to view instructions for MacOS, Linux, or compiling from source.\n\n{% tabs %}\n{% tab title=\"MacOS\" %}\n##### MacOS\n\n**Download Akash Binary**\n\n_**NOTE**_ - in the commands below we download the Akash binary for version `0.3.1-rc1` and the `darwin_all`. If an alternate binary is needed, visit this [GitHub Release](https://github.com/akash-network/provider/releases/tag/v0.3.1-rc1) page and find/replace the link used appropriately\n\n```\ncd ~/Downloads\n\nwget https://github.com/akash-network/provider/releases/download/v0.3.1-rc1/provider-services_0.3.1-rc1_darwin_all.zip\n\nunzip provider-services_0.3.1-rc1_darwin_all.zip\n```\n\n**Move the Akash Binary**\n\nMove the binary file into a directory included in your path\n\n```\nsudo mv provider-services /usr/local/bin\n```\n\n**Verify Akash Installation**\n\nVerify the installation by using a simple command to check the Akash version\n\n```\nprovider-services version\n```\n\n**Expect/Example Output**\n\n```\nv0.3.1-rc1\n```\n{% endtab %}\n\n{% tab title=\"Linux\" %}\n**Download Akash Binary**\n\n> _**NOTE**_ - in the commands below we download the Akash binary for version `0.3.1-rc1` and a `AMD-64` architecture. If an alternate binary is needed, visit this [GitHub Release](https://github.com/akash-network/provider/releases/tag/v0.3.1-rc1) page and find/replace the link used appropriately.\n\n```\ncd ~\n\napt install jq -y\n\napt install unzip -y\n\nwget https://github.com/akash-network/provider/releases/download/v0.3.1-rc1/provider-services_0.3.1-rc1_linux_amd64.zip\n\nunzip provider-services_0.3.1-rc1_linux_amd64.zip\n\nmkdir bin; mv provider-services /root/bin/\n```\n\n**Add Akash Install Location to User’s Path**\n\nAdd the software’s install location to the user’s path for easy use of Akash commands.\n\n**NOTE:** Below we provide the steps to add the Akash install directory to a user’s path on a Linux Ubuntu server. Please take a look at a guide for your operating system and how to add a directory to a user’s path.\n\nOpen the user’s path file in an editor:\n\n```\nvi /etc/environment\n```\n\nView within text editor prior to the update:\n\n```\nPATH=\"/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin\"\n```\n\nAdd the following directory, which is the Akash install location, to `PATH`. In this example the active user is root. If logged in as another username, replace /root with your current/home directory.\n\n```\n/root/bin\n```\n\nView within the text editor following the update:\n\n```\nPATH=\"/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/root/bin\"\n```\n\n##### Make the Path Active in the Current Session\n\n```\n. /etc/environment\n```\n\n##### Verify Akash Install\n\nDisplay the version of Akash software installed. This confirms the software installed and that the new user path addition worked.\n\n```\nprovider-services version\n```\n\n**Expected/Example Result**\n\n```\nv0.3.1-rc1\n```\n{% endtab %}\n{% endtabs %}\n\n\n## Create an Account\n\nConfigure the name of your key. The command below will set the name of your key to `myWallet`, run the below command and replace `myWallet` with a name of your choice:\n\n```bash\nAKASH_KEY_NAME=myWallet\n```\n\nVerify you have the shell variables set up . The below command should return the name you've used:\n\n```bash\necho $AKASH_KEY_NAME\n```\n\nWe now need to point Akash to where the keys are stored for your configuration. To do this we will set the AKASH\\_KEYRING\\_BACKEND environmental variable.\n\n```bash\nAKASH_KEYRING_BACKEND=os\n```\n\nCopy and paste this command into Terminal to create an Akash account:\n\n```bash\nprovider-services keys add $AKASH_KEY_NAME\n```\n\nRead the output and save your mnemonic phrase is a safe place. Let's set a Shell Variable in Terminal `AKASH_ACCOUNT_ADDRESS` to save your account address for later.\n\n```bash\nexport AKASH_ACCOUNT_ADDRESS=\"$(provider-services keys show $AKASH_KEY_NAME -a)\"\n\necho $AKASH_ACCOUNT_ADDRESS\n```\n\nNote that if you close your Terminal window this variable will not be saved.\n\n## Fund your Account\n\n### Fund Created Account via Testnet Faucet\n\nVisit the Testnet faucet [here](http://faucet.testnet-02.aksh.pw/) to fund your provider account. &#x20;\n\nEnter the address of the `myWallet` account created in previous steps as prompted by the faucet.\n\n\n\n![](../../../assets/testnetFaucet.png)\n\n##### Example/Expected Faucet Output\n\nThe following screen shot shows an example/expected output following successful submission to the Testnet faucet. Note that the displayed transaction hash will be unique and different than the one displayed in this example. Receipt of this screen confirms successful funding of your account with `25AKT`.\n\n\n\n![](../../../assets/faucetOutput2.png)\n\n## Configure your Network\n\n### Configure the Testnet Chain ID and RPC Node\n\n```bash\nexport AKASH_CHAIN_ID=testnet-02\nexport AKASH_NODE=http://rpc.testnet-02.aksh.pw:26657\n```\n\n### Confirm your network variables are setup\n\nYour values may differ depending on the network you're connecting to.\n\n```bash\necho $AKASH_NODE $AKASH_CHAIN_ID $AKASH_KEYRING_BACKEND\n```\n\nYou should see something similar to:\n\n```\nhttp://rpc.testnet-02.aksh.pw:26657 testnet-02 os\n```\n\n### Set Additional Environment Variables\n\nSet the below set of environment variables to ensure smooth operations\n\n| Variable               | Description                                                                               | Recommended Value |\n| ---------------------- | ----------------------------------------------------------------------------------------- | ----------------- |\n| AKASH\\_GAS             | Gas limit to set per-transaction; set to \"auto\" to calculate sufficient gas automatically | `auto`            |\n| AKASH\\_GAS\\_ADJUSTMENT | Adjustment factor to be multiplied against the estimate returned by the tx simulation     | `1.15`            |\n| AKASH\\_GAS\\_PRICES     | Gas prices in decimal format to determine the transaction fee                             | `0.025uakt`       |\n| AKASH\\_SIGN\\_MODE      | Signature mode                                                                            | `amino-json`      |\n\n```\nexport AKASH_GAS=auto\nexport AKASH_GAS_ADJUSTMENT=1.25\nexport AKASH_GAS_PRICES=0.025uakt\nexport AKASH_SIGN_MODE=amino-json\n```\n\n### Check your Account Balance\n\nCheck your account has sufficient balance by running:\n\n```bash\nprovider-services query bank balances --node $AKASH_NODE $AKASH_ACCOUNT_ADDRESS\n```\n\nYou should see a response similar to:\n\n```\nbalances:\n- amount: \"25000000\"\n  denom: uakt\npagination:\n  next_key: null\n  total: \"0\"\n```\n\n{% hint style=\"info\" %}\nYour account must have a minimum balance of 5 AKT to create a deployment. This 5 AKT funds the escrow account associated with the deployment and is used to pay the provider for their services. It is recommended you have more than this minimum balance to pay for transaction fees. For more information on escrow accounts, see [here](https://github.com/akash-network/docs/blob/master/guides/cli/detailed-steps/broken-reference/README.md)\n{% endhint %}\n\n## Create your Configuration\n\nCreate a deployment configuration `deploy.yml` to deploy an example GPU enabled workload via the steps and SDL provided in this section.\n\n#### Akash GPU Enabled Manifest/SDL\n\nYou may use the sample deployment file as-is or modify it for your own needs as described in our [SDL (Stack Definition Language)](https://github.com/akash-network/docs/blob/master/sdl/README.md) documentation.\n\n##### EXAMPLE GPU ENABLED SDL:\n\n```bash\ncat > deploy.yml <<EOF\n---\nversion: \"2.0\"\n\nservices:\n  gpu-test:\n    # Nvidia cuda compatibility https://docs.nvidia.com/deploy/cuda-compatibility/\n    # for nvidia 510 drivers\n    ## image: nvcr.io/nvidia/k8s/cuda-sample:vectoradd-cuda10.2\n    # for nvidia 525 drivers use below image\n    image: nvcr.io/nvidia/k8s/cuda-sample:vectoradd-cuda11.6.0\n    command:\n      - \"sh\"\n      - \"-c\"\n    args:\n      - 'sleep infinity'\n    expose:\n      - port: 3000\n        as: 80\n        to:\n          - global: true\nprofiles:\n  compute:\n    gpu-test:\n      resources:\n        cpu:\n          units: 1\n        memory:\n          size: 1Gi\n        gpu:\n          units: 1\n          attributes:\n            vendor:\n              nvidia:\n                - model: a4000\n        storage:\n          - size: 512Mi\n  placement:\n    westcoast:\n      pricing:\n        gpu-test:\n          denom: uakt\n          amount: 100000\ndeployment:\n  gpu-test:\n    westcoast:\n      profile: gpu-test\n      count: 1\nEOF\n```\n\n## Create your Certificate\n\nIn this step we will create a local certificate and then store the certification on the block chain\n\n* Ensure that prior steps in this guide have been completed and that you have a funded wallet before attempting certificate creation.\n* **Your certificate needs to be created only once per account** and can be used across all deployments.\n\n##### Generate Cert\n\n* Note: If it errors with `Error: certificate error: cannot overwrite certificate`, then add `--overwrite` should you want to overwrite the cert. Normally you can ignore that error and proceed with publishing the cert (next step).\n\n```\nprovider-services tx cert generate client --from $AKASH_KEY_NAME\n```\n\n##### Publish Cert to the Blockchain\n\n```\nprovider-services tx cert publish client --from $AKASH_KEY_NAME\n```\n\n## Create your Deployment\n\n### Akash Deployment\n\nTo deploy on Akash, run:\n\n```\nprovider-services tx deployment create deploy.yml --from $AKASH_KEY_NAME \n```\n\nYou should see a response similar to:\n\n```javascript\n{\n  \"height\":\"140325\",\n  \"txhash\":\"2AF4A01B9C3DE12CC4094A95E9D0474875DFE24FD088BB443238AC06E36D98EA\",\n  \"codespace\":\"\",\n  \"code\":0,\n  \"data\":\"0A130A116372656174652D6465706C6F796D656E74\",\n  \"raw_log\":\"[{\\\"events\\\":[{\\\"type\\\":\\\"akash.v1\\\",\\\"attributes\\\":[{\\\"key\\\":\\\"module\\\",\\\"value\\\":\\\"deployment\\\"},{\\\"key\\\":\\\"action\\\",\\\"value\\\":\\\"deployment-created\\\"},{\\\"key\\\":\\\"version\\\",\\\"value\\\":\\\"2b86f778de8cc9df415490efa162c58e7a0c297fbac9cdb8d6c6600eda56f17e\\\"},{\\\"key\\\":\\\"owner\\\",\\\"value\\\":\\\"akash1vn06ycjjnvsvl639fet9lajjctuturrtx7fvuj\\\"},{\\\"key\\\":\\\"dseq\\\",\\\"value\\\":\\\"140324\\\"},{\\\"key\\\":\\\"module\\\",\\\"value\\\":\\\"market\\\"},{\\\"key\\\":\\\"action\\\",\\\"value\\\":\\\"order-created\\\"},{\\\"key\\\":\\\"owner\\\",\\\"value\\\":\\\"akash1vn06ycjjnvsvl639fet9lajjctuturrtx7fvuj\\\"},{\\\"key\\\":\\\"dseq\\\",\\\"value\\\":\\\"140324\\\"},{\\\"key\\\":\\\"gseq\\\",\\\"value\\\":\\\"1\\\"},{\\\"key\\\":\\\"oseq\\\",\\\"value\\\":\\\"1\\\"}]},{\\\"type\\\":\\\"message\\\",\\\"attributes\\\":[{\\\"key\\\":\\\"action\\\",\\\"value\\\":\\\"create-deployment\\\"},{\\\"key\\\":\\\"sender\\\",\\\"value\\\":\\\"akash1vn06ycjjnvsvl639fet9lajjctuturrtx7fvuj\\\"},{\\\"key\\\":\\\"sender\\\",\\\"value\\\":\\\"akash1vn06ycjjnvsvl639fet9lajjctuturrtx7fvuj\\\"}]},{\\\"type\\\":\\\"transfer\\\",\\\"attributes\\\":[{\\\"key\\\":\\\"recipient\\\",\\\"value\\\":\\\"akash17xpfvakm2amg962yls6f84z3kell8c5lazw8j8\\\"},{\\\"key\\\":\\\"sender\\\",\\\"value\\\":\\\"akash1vn06ycjjnvsvl639fet9lajjctuturrtx7fvuj\\\"},{\\\"key\\\":\\\"amount\\\",\\\"value\\\":\\\"5000uakt\\\"},{\\\"key\\\":\\\"recipient\\\",\\\"value\\\":\\\"akash14pphss726thpwws3yc458hggufynm9x77l4l2u\\\"},{\\\"key\\\":\\\"sender\\\",\\\"value\\\":\\\"akash1vn06ycjjnvsvl639fet9lajjctuturrtx7fvuj\\\"},{\\\"key\\\":\\\"amount\\\",\\\"value\\\":\\\"5000000uakt\\\"}]}]}]\",\n  \"logs\":[\n    {\n      \"msg_index\":0,\n      \"log\":\"\",\n      \"events\":[\n        {\n          \"type\":\"akash.v1\",\n          \"attributes\":[\n            {\n              \"key\":\"module\",\n              \"value\":\"deployment\"\n            },\n            {\n              \"key\":\"action\",\n              \"value\":\"deployment-created\"\n            },\n            {\n              \"key\":\"version\",\n              \"value\":\"2b86f778de8cc9df415490efa162c58e7a0c297fbac9cdb8d6c6600eda56f17e\"\n            },\n            {\n              \"key\":\"owner\",\n              \"value\":\"akash1vn06ycjjnvsvl639fet9lajjctuturrtx7fvuj\"\n            },\n            {\n              \"key\":\"dseq\",\n              \"value\":\"140324\"\n            },\n            {\n              \"key\":\"module\",\n              \"value\":\"market\"\n            },\n            {\n              \"key\":\"action\",\n              \"value\":\"order-created\"\n            },\n            {\n              \"key\":\"owner\",\n              \"value\":\"akash1vn06ycjjnvsvl639fet9lajjctuturrtx7fvuj\"\n            },\n            {\n              \"key\":\"dseq\",\n              \"value\":\"140324\"\n            },\n            {\n              \"key\":\"gseq\",\n              \"value\":\"1\"\n            },\n            {\n              \"key\":\"oseq\",\n              \"value\":\"1\"\n            }\n          ]\n        },\n        {\n          \"type\":\"message\",\n          \"attributes\":[\n            {\n              \"key\":\"action\",\n              \"value\":\"create-deployment\"\n            },\n            {\n              \"key\":\"sender\",\n              \"value\":\"akash1vn06ycjjnvsvl639fet9lajjctuturrtx7fvuj\"\n            },\n            {\n              \"key\":\"sender\",\n              \"value\":\"akash1vn06ycjjnvsvl639fet9lajjctuturrtx7fvuj\"\n            }\n          ]\n        },\n        {\n          \"type\":\"transfer\",\n          \"attributes\":[\n            {\n              \"key\":\"recipient\",\n              \"value\":\"akash17xpfvakm2amg962yls6f84z3kell8c5lazw8j8\"\n            },\n            {\n              \"key\":\"sender\",\n              \"value\":\"akash1vn06ycjjnvsvl639fet9lajjctuturrtx7fvuj\"\n            },\n            {\n              \"key\":\"amount\",\n              \"value\":\"5000uakt\"\n            },\n            {\n              \"key\":\"recipient\",\n              \"value\":\"akash14pphss726thpwws3yc458hggufynm9x77l4l2u\"\n            },\n            {\n              \"key\":\"sender\",\n              \"value\":\"akash1vn06ycjjnvsvl639fet9lajjctuturrtx7fvuj\"\n            },\n            {\n              \"key\":\"amount\",\n              \"value\":\"5000000uakt\"\n            }\n          ]\n        }\n      ]\n    }\n  ],\n  \"info\":\"\",\n  \"gas_wanted\":\"100000\",\n  \"gas_used\":\"94653\",\n  \"tx\":null,\n  \"timestamp\":\"\"\n}\n```\n\n#### Find your Deployment \\#\n\nFind the Deployment Sequence (DSEQ) in the deployment you just created. You will need to replace the AKASH\\_DSEQ with the number from your deployment to configure a shell variable.\n\n```bash\nexport AKASH_DSEQ=CHANGETHIS\n```\n\nNow set the Order Sequence (OSEQ) and Group Sequence (GSEQ). Note that if this is your first time deploying on Akash, OSEQ and GSEQ will be 1.\n\n```bash\nAKASH_OSEQ=1\nAKASH_GSEQ=1\n```\n\nVerify we have the right values populated by running:\n\n```bash\necho $AKASH_DSEQ $AKASH_OSEQ $AKASH_GSEQ\n```\n\n## View your Bids\n\nAfter a short time, you should see bids from providers for this deployment with the following command:\n\n```bash\nprovider-services query market bid list --owner=$AKASH_ACCOUNT_ADDRESS --node $AKASH_NODE --dseq $AKASH_DSEQ --state=open\n```\n\n#### Choose a Provider\n\nNote that there are bids from multiple different providers. In this case, both providers happen to be willing to accept a price of _1 uAKT_. This means that the lease can be created using _1 uAKT_ or _0.000001 AKT_ per block to execute the container. You should see a response similar to:\n\n```\nbids:\n- bid:\n    bid_id:\n      dseq: \"140324\"\n      gseq: 1\n      oseq: 1\n      owner: akash1vn06ycjjnvsvl639fet9lajjctuturrtx7fvuj\n      provider: akash10cl5rm0cqnpj45knzakpa4cnvn5amzwp4lhcal\n    created_at: \"140326\"\n    price:\n      amount: \"1\"\n      denom: uakt\n    state: open\n  escrow_account:\n    balance:\n      amount: \"50000000\"\n      denom: uakt\n    id:\n      scope: bid\n      xid: akash1vn06ycjjnvsvl639fet9lajjctuturrtx7fvuj/140324/1/1/akash10cl5rm0cqnpj45knzakpa4cnvn5amzwp4lhcal\n    owner: akash10cl5rm0cqnpj45knzakpa4cnvn5amzwp4lhcal\n    settled_at: \"140326\"\n    state: open\n    transferred:\n      amount: \"0\"\n      denom: uakt\n- bid:\n    bid_id:\n      dseq: \"140324\"\n      gseq: 1\n      oseq: 1\n      owner: akash1vn06ycjjnvsvl639fet9lajjctuturrtx7fvuj\n      provider: akash1f6gmtjpx4r8qda9nxjwq26fp5mcjyqmaq5m6j7\n    created_at: \"140326\"\n    price:\n      amount: \"1\"\n      denom: uakt\n    state: open\n  escrow_account:\n    balance:\n      amount: \"50000000\"\n      denom: uakt\n    id:\n      scope: bid\n      xid: akash1vn06ycjjnvsvl639fet9lajjctuturrtx7fvuj/140324/1/1/akash1f6gmtjpx4r8qda9nxjwq26fp5mcjyqmaq5m6j7\n    owner: akash1f6gmtjpx4r8qda9nxjwq26fp5mcjyqmaq5m6j7\n    settled_at: \"140326\"\n    state: open\n    transferred:\n      amount: \"0\"\n      denom: uakt\n```\n\nFor this example, we will choose `akash10cl5rm0cqnpj45knzakpa4cnvn5amzwp4lhcal` Run this command to set the provider shell variable:\n\n```\nAKASH_PROVIDER=akash10cl5rm0cqnpj45knzakpa4cnvn5amzwp4lhcal\n```\n\nVerify we have the right value populated by running:\n\n```\necho $AKASH_PROVIDER\n```\n\n## Create a Lease\n\nCreate a lease for the bid from the chosen provider above by running this command:\n\n```\nprovider-services tx market lease create --dseq $AKASH_DSEQ --provider $AKASH_PROVIDER --from $AKASH_KEY_NAME\n```\n\n#### Confirm the Lease\n\nYou can check the status of your lease by running:\n\n```\nprovider-services query market lease list --owner $AKASH_ACCOUNT_ADDRESS --node $AKASH_NODE --dseq $AKASH_DSEQ\n```\n\nNote the bids will close automatically after 5 minutes, and you may get the response:\n\n```\nbid not open\n```\n\nIf this happens, close your deployment and open a new deployment again. To close your deployment run this command:\n\n```\nprovider-services tx deployment close --dseq $AKASH_DSEQ  --owner $AKASH_ACCOUNT_ADDRESS --from $AKASH_KEY_NAME \n```\n\nIf your lease was successful you should see a response that ends with:\n\n```\n    state: active\n```\n\n{% hint style=\"info\" %}\nPlease note that once the lease is created, the provider will begin debiting your deployment's escrow account, even if you have not completed the deployment process by uploading the manifest in the following step.\n{% endhint %}\n\n## Update the Deployment\n\n#### Update the Manifest\n\nUpdate the deploy.yml manifest file with the desired change.\n\n_**NOTE:**_\\*\\* Not all attributes of the manifest file are eligible for deployment update. If the hardware specs of the manifest are updated (I.e. CPU count), a re-deployment of the workload is necessary. Other attributes, such as deployment image and funding, are eligible for updates.\n\n#### Issue Transaction for On Chain Update\n\n```\nprovider-services tx deployment update deploy.yml --dseq $AKASH_DSEQ --from $AKASH_KEY_NAME \n```\n\n#### Send Updated Manifest to Provider\n\n```\nprovider-services send-manifest deploy.yml --dseq $AKASH_DSEQ --provider $AKASH_PROVIDER --from $AKASH_KEY_NAME\n```\n\n## Close Deployment\n\n#### Close the Deployment\n\nShould you need to close the deployment follow this step.\n\n```\nprovider-services tx deployment close --from $AKASH_KEY_NAME\n```\n","description":null,"slug":"docs/testnet/gpu-testnet-client-instructions"},{"title":"GPU Testnet Submission Instructions","body":"\n\n\n## GPU Provider Challenge - Task Submission Guidance\n\nEnsure proper Testnet Form submission for the GPU Provider Challenge via additional guidance provided in these sections:\n\n- [GPU Provider Challenge - Task Submission Guidance](#gpu-provider-challenge---task-submission-guidance)\n  - [Overview](#overview)\n  - [Provider Address](#provider-address)\n  - [Tenant/Deployment Address](#tenantdeployment-address)\n  - [Launch Test Deployment](#launch-test-deployment)\n    - [Provide DSEQ (Deployment ID) of Test Deployment](#provide-dseq-deployment-id-of-test-deployment)\n    - [Provide Screenshot of Test Deployment](#provide-screenshot-of-test-deployment)\n\n### Overview\n\nThe timestamp of your Akash Testnet Provider build completion will be the form submission date/time.  When your provider is fully built and tested - using the suggested steps in this document - submit the form immediately to timestamp the completion date/time and to qualify for limited, associated rewards.\n\n### Provider Address\n\nDuring your Provider build a dedicated account was created in this [step](https://docs.akash.network/other-resources/experimental/testnet/provider-build-with-gpu/akash-provider-install#create-provider-account).  Use this Akash address for submission as the `Akash GPU Testnet Provider Address`.\n\n### Tenant/Deployment Address\n\nProvide the tenant Akash address of the account that is used to launch your Testnet verification deployments (detailed further below).\n\nThe tenant address used to launch the test deployment/deployments can be isolated easily such as in the example below.\n\nThe address/account used to launch the deployment will be used in Testnet task grading to ensure that the deployment lease was created on your specific provider.\n\n\n\n![](../../../assets/tenantAddress.png)\n\n### Launch Test Deployment\n\nAs described in the Testnet Submission form, a deployment much be launched on your provider using this [SDL](https://gist.github.com/chainzero/86402b1ab2cef63a7e83d4fbad73b0e0).\n\nUse these [Deployment Guides](/docs/docs/testnet/gpu-testnet-client-instructions/) for further details on launching Testnet deployments.\n\nThe deployment will be used to verify that your provider is fully functional and satisfies a healthy, running state.\n\nThe deployment log output will also be used to verify the GPU resources of your provider as described in this [section](#provide-screenshot-of-test-deployment).\n\nThe test deployment can be closed once you have captured the necessary information to satsify task.\n\n> _**NOTE**_ - if your Testnet provider hosts multiple GPU instances - launch multiple test deployment using each of the GPUs and submit DSEQs/screenshots (as reviewed in subsequent sections) for each GPU model/instance.\n\n#### Provide DSEQ (Deployment ID) of Test Deployment\n\nSubmit the DSEQ of your test deployment with the submission form.  The DSEQ will be used to verify that the deployment was successfully launched on your provider.\n\nThe DSEQ can be found easily in the deployment tool (Akash Console, Cloudmos Deploy, or Akash CLI).  Example of isolating the deployment's DSEQ is shown below.\n\n\n\n![](../../../assets/isoalteDSEQ.png)\n\n#### Provide Screenshot of Test Deployment\n\nAttach a screenshot of the deployment launched on your provider.  This step will verify that the provider is fully functional and additional allows verification of the GPU model hosted in the provider.\\\n\\\nThese logs can be found easily in the deployment tool (Akash Console, Cloudmos Deploy, or Akash CLI).  Example of isolating the deployment's logs is shown below.\n\n> _**NOTE**_ - ensure that the DSEQ is captured within the screen as well.  In the example provided below the DSEQ is captured in the screenshot (DSEQ = 1066270 in example).   Capture of the DSEQ allows confirmation that this deployment was tested in your provider.\n\n\n\n![](../../../assets/gpuCheck.png)\n\n\n","description":null,"slug":"docs/testnet/gpu-testnet-submission-instructions"},{"title":"Testnet Issue Reporting","body":"\n\n## Provider Log Collection\n\n### Overview\n\nTo allow the Akash core team to investigate GPU Testnet provider issues, please collect logs via the steps covered in this guide.\n\nMost of the time the kubectl logs command won't show the entire logs due to K8s log rotation, so please make sure to use this advanced technique to save the provider logs:\n\n### Save akash-provider Logs\n\n#### STEP 1 - Locate the running akash-provider pod\n\n```\nkubectl -n akash-services get pods -l app=akash-provider -o wide\n```\n\n#### STEP 2 - Locate the log directory of the akash-provider container on that\n\n```\nssh <node>\ncrictl inspect $(crictl ps --name provider --state Running -q) | jq -r '.status.logPath'\n```\n\n#### STEP 3 - Archive the entire directory to the provider-logs.tar.gz file\n\n> Make sure you remove the /0.log in the end of the logPath like so:\n\n```\ntar czf provider-logs.tar.gz /var/log/pods/akash-services_akash-provider-0_<unique-ID>/provider\n```\n\n#### STEP 4 - Upload the provider-logs.tar.gz file to the internet\n\n```\ncurl -T provider-logs.tar.gz https://transfer.sh ; echo\n```\n\nThis command will produce the output that looks like this:\n\n```\nhttps://transfer.sh/h5Op4hD54c/provider-logs.tar.gz\n```\n\nShare this link with the Akash Core Team for them to troubleshoot the issue.\n\n#### Example from one of the real nodes of ours:\n\n```\nroot@node1:~# kubectl -n akash-services get pods -l app=akash-provider -o wide\nNAME               READY   STATUS    RESTARTS   AGE   IP              NODE    NOMINATED NODE   READINESS GATES\nakash-provider-0   1/1     Running   0          9d    10.233.90.133   node1   <none>           <none>\n\nroot@node1:~# crictl inspect $(crictl ps --name provider --state Running -q) | jq -r '.status.logPath'\n/var/log/pods/akash-services_akash-provider-0_19426f5d-2d7f-4649-b956-fe4d26b1fbb8/provider/0.log\n\nroot@node1:~# tar czf provider-logs.tar.gz /var/log/pods/akash-services_akash-provider-0_19426f5d-2d7f-4649-b956-fe4d26b1fbb8/provider\n\nroot@node1:~# curl -T provider-logs.tar.gz https://transfer.sh ; echo\n```","description":null,"slug":"docs/testnet/testnet-issue-reporting"},{"title":"Akash Validator Using Omnibus","body":"\n\nBuilding an Akash Validator is simplified via the use of Cosmos Omnibus and as detailed in this guide.\n\n> _**NOTE**_ - using the procedures in this guide the Validator private key will be stored within the associated Akash deployment.  Ensure that only known, trusted providers are used for such deployments.  Additionally consider storing the private key outside of the deployment through the use of TMKMS which is documented [here](/docs/docs/validators/akash-validator-with-tmkms-and-stunnel/).\n\nThe Akash Validator will be built with the following attributes:\n\n* Two Sentry nodes will be created to protect the validator from distributed denial of service (DDOS) attacks\n* The Akash Validator will sit behind the Sentry nodes and will not be directly exposed to the internet\n* The Sentry nodes and the Akash Validator will launched on the Akash Network as deployments\n* This guide will detail the launch of the deployments via the Cloudmos Deploy tool for ease\n\n### Sections within this Guide\n\n[STEP 1 - Cloudmos Deploy Review](#cloudmos-deploy-review)\n\n[STEP 2 - FileBase Buckets](#filebase-buckets)\n\n[STEP 3 - Akash SDL Review](#akash-sdl-review)\n\n[STEP 4 - Cloudmos Initial Deployment](#cloudmos-initial-deployment)\n\n[STEP 5 - Edit SDL with Known IDs](#edit-sdl-with-known-ids)\n\n[STEP 6 - Redeploy Validator and Sentries with Cloudmos](#redeploy-validator-and-sentries-with-cloudmos)\n\n\n## Cloudmos Deploy Review\n\nWhile all steps within this guide could be accomplished via the Akash CLI, we will use the Cloudmos Deploy tool as it has become our most popular deploy tool.\\\n\\\nIf you have not installed Cloudmos prior and/or are not overly familiar with the app, please review our [Cloudmos Deploy documentation](broken-reference) before getting started with the Validator build steps detailed in subsequent sections.\n\n## FileBase Buckets\n\nThe Omnibus framework used to build our Akash Validator will store the following info in a S3 bucket for re-use when a deployment is restarted.  All data stored will be encrypted.\n\n* _Validator's Private Key_\n* _Node IDs_ - more detail on the use of these IDs later in this guide\n\n### Bucket Creation\n\nWe recommend the use of [FileBase](https://console.filebase.com/) S3 buckets for this purpose. If you do not have a FileBase account, create a free account for this use.\n\nCreate the following buckets within FileBase for upcoming use in the Validator build.\n\n* akashnode1\n* akashnode2\n* akashvalidator\n\n### Access Keys\n\nVisit the FileBase \\`Access Keys\\` menu option and capture the `KEY` and `SECRET` for use in subsequent steps.\n\n\n## Akash SDL Review\n\nAkash deployments are created using [Stack Definition Language (SDL)](../../readme/stack-definition-language.md) files that serve as the recipe for your deployments.  In this section we detail the Omnibus SDL used for the Akash Validator deployment.\n\n### Akash Validator SDL Template\n\nThe SDL template for the Akash Validator build is located in this [GitHub repository](https://github.com/akash-network/cosmos-omnibus/blob/master/\\_examples/validator-and-private-sentries/deploy.yml).\n\nCopy the SDL located in the repository into a local editor for customizations covered in upcoming steps.\n\n### Initial SDL Customizations\n\nAll SDL edits necessary are environment variable related.  Within the SDL, we will only edit values within the `services` section and  `env` subsections.  All other segments of the SDL can be used from the template without need to edit.\n\n#### Validator Service\n\n* The first service listed in the SDL and which requires customization is the `validator` service\n* Recommended `env` variable updates are detailed in the code block below\n* **NOTE** - eventually we will need to additionally update the AKASH\\_UNCONDITIONAL\\_PEER\\_IDS variable. But at this point in the process these IDs - which are the IDs of the Sentry nodes - are not known.  Based on this we will update this field later.\n\nUpdate the following env variables with suggested values:\n\n```\nAKASH_P2P_PEX=true\n\nP2P_PERSISTENT_PEERS=f997dbd1048af671527857db422291a11d067975@65.21.198.247:26656,20180c45451739668f6e272e007818139dba31e7@88.198.62.198:2020,43544bc781b88d6785420427926d86a5332940b3@142.132.131.184:26656,ef80a9b5e100dd6a4bb0fa536322f437565aad39@38.146.3.167:26656,aa01698ec0d8bb96398e89b57ecb08bcca50fa21@65.21.199.148:26636,d2643edd1b3dce6615bc9925e20122c44d2ff763@172.106.17.158:26656,30b8008d4ea5069a8724a0aa73833493efa88e67@65.108.140.62:26656,157f7c0e1363bea36a10bfae2a9127f5c6dd2991@18.220.238.235:26656,8e8e0282408001bc9dfd8bc3696ed2a5201245b0@168.119.190.132:26656,a8da9010d07b69894765cfd27b1eca62f1cb1d55@13.214.178.23:26656,be3a538cebb28e7224db10920bb7fe32456e1aad@116.202.244.153:26656,070b3c936e2995bc269a2981702b87de05411baa@148.251.13.186:28656,eeacfa49aa225f5232d0456bd3e4111be11b516e@65.108.6.185:26656,e18d9d0c1ad94d6394913fbf902c9fc0f38b369e@34.148.214.23:26656\n\nSTATESYNC_RPC_SERVERS=https://akash-rpc.polkachu.com:443,https://akash-rpc.polkachu.com:443\n\nS3_KEY=<specify the key captured in FileBase>\n\nS3_SECRET=<specify the secret captured in FileBase>\n\nKEY_PASSWORD=<password of your choice that is used in encryption of files in FileBase>\n\nKEY_PATH=akashvalidator\n```\n\n##### Example/Populated Variables\n\n```\nvalidator:\n    image: ghcr.io/akash-network/cosmos-omnibus:v0.3.42-akash-v0.22.7\n    env:\n      - MONIKER=validator\n      - CHAIN_JSON=https://raw.githubusercontent.com/akash-network/net/main/mainnet/meta.json\n      - MINIMUM_GAS_PRICES=0.025uakt\n      - FASTSYNC_VERSION=v2\n      - AKASH_MODE=validator\n      - AKASH_P2P_PEX=true\n      - AKASH_UNCONDITIONAL_PEER_IDS=<node-1-id>,<node-2-id>...\n      - AKASH_ADDR_BOOK_STRICT=false\n      - AKASH_DOUBLE_SIGN_CHECK_HEIGHT=10\n      - P2P_PERSISTENT_PEERS=f997dbd1048af671527857db422291a11d067975@65.21.198.247:26656,20180c45451739668f6e272e007818139dba31e7@88.198.62.198:2020,43544bc781b88d6785420427926d86a5332940b3@142.132.131.184:26656,ef80a9b5e100dd6a4bb0fa536322f437565aad39@38.146.3.167:26656,aa01698ec0d8bb96398e89b57ecb08bcca50fa21@65.21.199.148:26636,d2643edd1b3dce6615bc9925e20122c44d2ff763@172.106.17.158:26656,30b8008d4ea5069a8724a0aa73833493efa88e67@65.108.140.62:26656,157f7c0e1363bea36a10bfae2a9127f5c6dd2991@18.220.238.235:26656,8e8e0282408001bc9dfd8bc3696ed2a5201245b0@168.119.190.132:26656,a8da9010d07b69894765cfd27b1eca62f1cb1d55@13.214.178.23:26656,be3a538cebb28e7224db10920bb7fe32456e1aad@116.202.244.153:26656,070b3c936e2995bc269a2981702b87de05411baa@148.251.13.186:28656,eeacfa49aa225f5232d0456bd3e4111be11b516e@65.108.6.185:26656,e18d9d0c1ad94d6394913fbf902c9fc0f38b369e@34.148.214.23:26656\n      - STATESYNC_RPC_SERVERS=node1,node2\n      - S3_KEY=A397A7<redacted>\n      - S3_SECRET=7ZHVFsE2<redaacted>\n      - KEY_PASSWORD=Validator4Akash!#!\n      - KEY_PATH=akashvalidator\n\n```\n\n#### Sentry Node #1 Service\n\n* The second service listed in the SDL that requires customization is for the `node1` service.\n* Recommended `env` variable updates are detailed in the code block below\n* **NOTE** - eventually we will need to additionally update the `AKASH_PRIVATE_PEER_IDS` and `AKASH_UNCONDITIONAL_PEER_IDS`  variables.  But at this point in the process these IDs - which is the ID of the Validator node - is not known. We will update these fields later.\n\nUpdate the following env variables with suggested values:\n\n```\nSTATESYNC_RPC_SERVERS=https://akash-rpc.polkachu.com:443,https://akash-rpc.polkachu.com:443\n\nS3_KEY=<specify the key captured in FileBase>\n\nS3_SECRET=<specify the secret captured in FileBase>\n\nKEY_PASSWORD=<password of your choice that is used in encryption of files in FileBase>\n\nKEY_PATH=akashnode1\n```\n\n##### Example/Populated Variables\n\n```\n  node1:\n    image: ghcr.io/akash-network/cosmos-omnibus:v0.3.42-akash-v0.22.7\n    env:\n      - MONIKER=private_node_1\n      - CHAIN_JSON=https://raw.githubusercontent.com/akash-network/net/main/mainnet/meta.json\n      - MINIMUM_GAS_PRICES=0.025uakt\n      - FASTSYNC_VERSION=v2\n      - AKASH_MODE=full\n      - AKASH_P2P_PEX=true\n      - AKASH_PRIVATE_PEER_IDS=<validatorid>\n      - AKASH_UNCONDITIONAL_PEER_IDS=<validatorid>\n      - AKASH_ADDR_BOOK_STRICT=false\n      - STATESYNC_RPC_SERVERS=https://akash-rpc.polkachu.com:443,https://akash-rpc.polkachu.com:443\n      - STATESYNC_SNAPSHOT_INTERVAL=500\n      - S3_KEY=<redacted>\n      - S3_SECRET=<redacted>\n      - KEY_PASSWORD=<redacted>\n      - KEY_PATH=akashnode1\n```\n\n#### Sentry Node #2 Service\n\n* The third and final service list in the SDL that requires customization is for the `node2` service.\n* Recommended `env` variable updates are detailed in the code block below\n* **NOTE** - eventually we will need to additionally update the `AKASH_PRIVATE_PEER_IDS` and `AKASH_UNCONDITIONAL_PEER_IDS` variables. But at this point in the process these IDs - which is the ID of the Validator node is not known - and based on this we will update this field later.\n\nUpdate the following env variables with suggested values:\n\n```\nSTATESYNC_RPC_SERVERS=https://akash-rpc.polkachu.com:443,https://akash-rpc.polkachu.com:443\n\nS3_KEY=<specify the key captured in FileBase>\n\nS3_SECRET=<specify the secret captured in FileBase>\n\nKEY_PASSWORD=<password of your choice that is used in encryption of files in FileBase>\n\nKEY_PATH=akashnode2\n```\n\n##### Example/Populated Variables\n\n```\n  node2:\n    image: ghcr.io/akash-network/cosmos-omnibus:v0.3.42-akash-v0.22.7\n    env:\n      - MONIKER=private_node_1\n      - CHAIN_JSON=https://raw.githubusercontent.com/akash-network/net/main/mainnet/meta.json\n      - MINIMUM_GAS_PRICES=0.025uakt\n      - FASTSYNC_VERSION=v2\n      - AKASH_MODE=full\n      - AKASH_P2P_PEX=true\n      - AKASH_PRIVATE_PEER_IDS=<validatorid>\n      - AKASH_UNCONDITIONAL_PEER_IDS=<validatorid>\n      - AKASH_ADDR_BOOK_STRICT=false\n      - STATESYNC_RPC_SERVERS=https://akash-rpc.polkachu.com:443,https://akash-rpc.polkachu.com:443\n      - STATESYNC_SNAPSHOT_INTERVAL=500\n      - S3_KEY=<redacted>\n      - S3_SECRET=<redacted>\n      - KEY_PASSWORD=<redacted>\n      - KEY_PATH=akashnode2\n```\n\n## Cloudmos Initial Deployment\n\nThe Akash validator build - using a Sentry architecture - requires that the Validator ID is known by the Sentry nodes. Additionally the Sentry node IDs must be known by the Validator. As these IDs are unknown prior to the build process - and are then stored for re-use in the S3 bucket during the build process - we must create an initial deployment, capture the necessary IDs, close the initial deployments, and launch anew the deployments with the IDs specified.\n\nIn this section we will create the initial deployments for ID captures.\n\n### **Create Deployments via Cloudmos**\n\n* Within the Cloudmos Deploy app, begin the process of creating a new deployment\n\n![](../../../assets/initialDeployCreateDeployment.png)\n\n* Select Empty Template for eventual entry of our custom SDL\n\n![](../../../assets/initialDeployEmptyTemplate.png)\n\n* Copy and paste the SDL we created and edited prior\n* Proceed thru steps of accepting necessary gas fees to create the deployment\n* When prompted select the desired Provider to host the deployment and complete the deployment process\n\n![](../../../assets/initialDeploySDLINsert.png)\n\n## Capture Validator and Sentry Node IDs\n\n* Within the Cloudmos deployment shell tab - as shown in the depiction below - we can capture the necessary IDs for the eventual, permanent Akash Validator deployment\n* Select the SHELL tab and select the `validator` instance from Service drop down\n* Enter the command `akash tendermint show-node-id` in the command line entry box on the bottom of the display\n* Copy the exposed ID for the validator to a text pad for use in a later step\n* Repeat this same process to collect the IDs of node1 and node2 (same steps as above but change the Service drop down to the nodes)\n\n![](../../../assets/initialDeployIDCaptures.png)\n\n### Close the Deployment\n\nWith the necessary IDs now captured, close the current deployment. As mentioned previously - we will use the IDs captured to create a permanent deployment in subsequent steps.\n\n## Edit SDL with Known IDs\n\nIn this step we will revisit and edit our initial SDL file with the known Validator and Node IDs.\n\n### Validator Service Update\n\n* Update the following entry in the SDL under the Validator service\n\n`AKASH_UNCONDITIONAL_PEER_IDS`\n\n* Use the node1 and node2 IDs captured in the previous steps as the comma separated values of this variable\n* Once populated this variable should appear as below in the greater section (example IDs shown and should be your own)\n\n```\n   validator:\n    image: ghcr.io/akash-network/cosmos-omnibus:v0.3.42-akash-v0.22.7\n    env:\n      - MONIKER=validator\n      - CHAIN_JSON=https://raw.githubusercontent.com/akash-network/net/main/mainnet/meta.json\n      - MINIMUM_GAS_PRICES=0.025uakt\n      - FASTSYNC_VERSION=v2\n      - AKASH_MODE=validator\n      - AKASH_P2P_PEX=true\n      - AKASH_UNCONDITIONAL_PEER_IDS=c955c77516b4c6fc62406a63303395fc97662c1e,b3035d5dfbfeb359c716bcb714ab383e6b73a5fd\n      - AKASH_ADDR_BOOK_STRICT=false\n      - AKASH_DOUBLE_SIGN_CHECK_HEIGHT=10\n      - P2P_PERSISTENT_PEERS=f997dbd1048af671527857db422291a11d067975@65.21.198.247:26656,20180c45451739668f6e272e007818139dba31e7@88.198.62.198:2020,43544bc781b88d6785420427926d86a5332940b3@142.132.131.184:26656,ef80a9b5e100dd6a4bb0fa536322f437565aad39@38.146.3.167:26656,aa01698ec0d8bb96398e89b57ecb08bcca50fa21@65.21.199.148:26636,d2643edd1b3dce6615bc9925e20122c44d2ff763@172.106.17.158:26656,30b8008d4ea5069a8724a0aa73833493efa88e67@65.108.140.62:26656,157f7c0e1363bea36a10bfae2a9127f5c6dd2991@18.220.238.235:26656,8e8e0282408001bc9dfd8bc3696ed2a5201245b0@168.119.190.132:26656,a8da9010d07b69894765cfd27b1eca62f1cb1d55@13.214.178.23:26656,be3a538cebb28e7224db10920bb7fe32456e1aad@116.202.244.153:26656,070b3c936e2995bc269a2981702b87de05411baa@148.251.13.186:28656,eeacfa49aa225f5232d0456bd3e4111be11b516e@65.108.6.185:26656,e18d9d0c1ad94d6394913fbf902c9fc0f38b369e@34.148.214.23:26656\n      - STATESYNC_RPC_SERVERS=https://akash-rpc.polkachu.com:443,https://akash-rpc.polkachu.com:443\n      - S3_KEY=<redacted>\n      - S3_SECRET=<redacted>\n      - KEY_PASSWORD=<redacted>\n      - KEY_PATH=akashvalidator\n```\n\n### Node Service Update\n\n* Update the following entries in the SDL under the node1 and node2 services\n\n`AKASH_PRIVATE_PEER_IDS`\n\n`AKASH_UNCONDITIONAL_PEER_IDS`\n\n* Use the validator ID captured in the previous steps as the comma separated values of this variable\n* Once populated these variables should appear as below in the greater section (example ID shown and should be your own)\n* Only `node1` example shown. Identical updates should be made to the `node2` service as well.\n\n```\n  node1:\n    image: ghcr.io/akash-network/cosmos-omnibus:v0.3.42-akash-v0.22.7\n    env:\n      - MONIKER=private_node_1\n      - CHAIN_JSON=https://raw.githubusercontent.com/akash-network/net/main/mainnet/meta.json\n      - MINIMUM_GAS_PRICES=0.025uakt\n      - FASTSYNC_VERSION=v2\n      - AKASH_MODE=full\n      - AKASH_P2P_PEX=true\n      - AKASH_PRIVATE_PEER_IDS=2d76800f5a149510229aadf480f8ec02ac6e5297\n      - AKASH_UNCONDITIONAL_PEER_IDS=2d76800f5a149510229aadf480f8ec02ac6e5297\n      - AKASH_ADDR_BOOK_STRICT=false\n      - STATESYNC_RPC_SERVERS=https://akash-rpc.polkachu.com:443,https://akash-rpc.polkachu.com:443\n      - STATESYNC_SNAPSHOT_INTERVAL=500\n      - S3_KEY=<redacted>\n      - S3_SECRET=<redacted>\n      - KEY_PASSWORD=<redacted>\n      - KEY_PATH=akashnode1\n```\n\n## Redeploy Validator and Sentries with Cloudmos\n\nWithin Cloudmos Deploy using the identical steps covered in the initial Validator deployment but use the updated SDL within IDs populated\n\n\n## Validator Verifications\n\n### Confirm Your Validator is Running\n\n#### Ensure Validator Sync\n\n* Ensure that the field \\`catching\\_up\\` is false and that the latest block corresponds to the current block of the blockchain\n* Execute this verification on both the sentry and validator nodes\n\n```\nakash status\n```\n\n##### Example Output when Validator is in Sync\n\n```\n{\"NodeInfo\":{\"protocol_version\":{\"p2p\":\"8\",\"block\":\"11\",\"app\":\"0\"},\"id\":\"136d67725800cb5a8baeb3e97dbdc3923879461e\",\"listen_addr\":\"tcp://0.0.0.0:26656\",\"network\":\"akashnet-2\",\"version\":\"0.34.19\",\"channels\":\"40202122233038606100\",\"moniker\":\"cznode\",\"other\":{\"tx_index\":\"on\",\"rpc_address\":\"tcp://0.0.0.0:26657\"}},\"SyncInfo\":{\"latest_block_hash\":\"4441B6D626166822979597F252B333C131C1DCB2F0467FF282EF0EAA3936B8CC\",\"latest_app_hash\":\"1ED64AE88E5CFD53651CD2B6B4E970292778071C8FBE23289393B4A06F57975F\",\"latest_block_height\":\"8284780\",\"latest_block_time\":\"2022-10-31T15:28:17.565762161Z\",\"earliest_block_hash\":\"E25CE5DD10565D6D63CDA65C8653A15F962A4D2960D5EC45D1DC0A4DE06F8EE3\",\"earliest_app_hash\":\"19526102DDBCE254BA71CC8E44185721D611635F638624C6F950EF31D3074E2B\",\"earliest_block_height\":\"5851001\",\"earliest_block_time\":\"2022-05-12T17:51:58.430492536Z\",\"catching_up\":false},\"ValidatorInfo\":{\"Address\":\"354E7FA2BF8C5B9C0F1C80F1C222818EC992D377\",\"PubKey\":{\"type\":\"tendermint/PubKeyEd25519\",\"value\":\"1+dVHZD7kfqnDU6I+bKbCv4ZE1LPieyMH+mwsOowhqY=\"},\"VotingPower\":\"0\"}}\n```\n\n#### Confirm Validator's Staking Status\n\n##### Template\n\n```\nakash query staking validator <akashvaloper-address>\n```\n\n##### Example\n\n```\nakash query staking validator akashvaloper16j3ge9lkpgtdkzntlja08gt6l63fql60xdupxq\n```\n\n##### Example Output\n\n* Status will display as \\`BOND\\_STATUS\\_UNBONDED\\` after initial build\n\n```\ncommission:\n  commission_rates:\n    max_change_rate: \"0.010000000000000000\"\n    max_rate: \"0.200000000000000000\"\n    rate: \"0.100000000000000000\"\n  update_time: \"2022-10-31T15:24:25.040091667Z\"\nconsensus_pubkey:\n  '@type': /cosmos.crypto.ed25519.PubKey\n  key: 1+dVHZD7kfqnDU6I+bKbCv4ZE1LPieyMH+mwsOowhqY=\ndelegator_shares: \"1000000.000000000000000000\"\ndescription:\n  details: \"\"\n  identity: \"\"\n  moniker: cznode\n  security_contact: \"\"\n  website: \"\"\njailed: false\nmin_self_delegation: \"1\"\noperator_address: akashvaloper1jy7ej9t6r8q5dyjrst88nt9rjgkdltgx97wfvd\nstatus: BOND_STATUS_UNBONDED\ntokens: \"1000000\"\nunbonding_height: \"0\"\nunbonding_time: \"1970-01-01T00:00:00Z\"\n```\n\n#### Active Set Confirmation\n\nYour validator is active if the following command returns anything\n\n> _**NOTE**_ - this command will only display output of your validator is in the active set\n\n```\nakash query tendermint-validator-set | grep \"$(akash tendermint show-validator)\"\n```\n\n","description":null,"slug":"docs/validators/akash-validator-using-omnibus"},{"title":"Akash Validator with TMKMS and Stunnel","body":"\n\n\nIn this guide we will create an Akash Validator as a deployment on the Akash network.  The Tendermint Key Management System (TMKMS) will be used so that we do not store the validator's private key on the validator server itself.\n\nAn implementation of Stunnel is included to provide secure peer to peer communication between the Akash validator and the TMKMS server.\n\nThe Validator deployment will take advantage of statesync for rapid blockchain synchronization.\n\nSections in this guide:\n\n* [STEP 1 - Validator Topology](#validator-topology)\n* [STEP 2 - Obtain Private Key](#obtain-private-key)\n* [STEP 3 - Akash Validator Deployment](#akash-validator-deployment)\n* [STEP 4 - TMKMS Setup](#tmkms-setup)\n* [STEP 5 - Start and Verify the TMKMS Service](#start-and-verify-the-tmkms-service)\n* [STEP 6 - Stunnel Client](#stunnel-client)\n* [STEP 7 - Verify Validator Status](#verify-validator-status)\n* [Additional Resources](#additional-resources)\n\n\n## Additional Resources\n\nCreation of an Akash Validator with TMKMS and Stunnel via streamlined steps is the focus of this guide.  For further information and details, reference the following GitHub repository maintained by the creators of the Omnibus project.\n\n* [Validator and TMKMS via Omnibus source code](https://github.com/akash-network/cosmos-omnibus/tree/09679171d513586b5e1a9aafe73db55ebdbf5098/\\_examples/validator-and-tmkms)\n\n\n## Validator Topology\n\nIn this guide we create a Validator within an Akash Deployment.\n\nThe topology of the environment will be as follows:\n\n* **Akash Validator** as a deployment and as created in the [Akash Validator Deployment](#akash-validator-deployment) section of this guide\n* **Tendermint Key Managment System (TMKMS)** used for storage of the Validators private key on a secured server.  The TMKMS instance - configured in the [TMKMS Setup](broken-reference) section of this guide - may be created on any secure server of your choosing.  The TMKMS server must have connectivity to the Akash Validator.\n* **Stunnel** provides a secure communication path between the validator and the TMKMS node.  A Stunnel server will be co-located with the Validator.  A Stunnel client will be co-located with the TMKMS server.\n\n\n## Obtain Private Key\n\nIn the [TMKMS Setup](broken-reference) section of this guide we will import your Validators private key.\n\nIf you have a pre-existing Akash Validator the private key from this instance may be used.\n\nIf this is a new Akash Validator - create an Akash validator instance for the purpose of private key generation, capture the private key, and then shut down the validator.  Use the instructions in [Akash Node Deployment Via Omnibus](/docs/docs/validators/akash-validator-using-omnibus/) to easily build an Akash Node for this purpose and to obtain the validator private key as detailed below.  It is NOT necessary to complete the additional steps to turn the Node into a Validator.  Completing only the Node build will allow the capture of the needed Validator private key.\n\n### Example Validator Private Key Retrieval\n\n* Display contents of key file on the validator\n\n```\ncat ~/.akash/config/priv_validator_key.json\n```\n\n* Example Output\n\n```\n{\n  \"address\": \"134FCAC9<redacted>\",\n  \"pub_key\": {\n    \"type\": \"tendermint/PubKeyEd25519\",\n    \"value\": \"BrL0wA8DWiVvm<redacted>\"\n  },\n  \"priv_key\": {\n    \"type\": \"tendermint/PrivKeyEd25519\",\n    \"value\": \"3RphlkX7PucBKSdhFKviFV5TI<redacted>\"\n  }\n}\n```\n\n## Akash Validator Deployment\n\n### Cloudmos Deploy\n\n* Within this guide we will use the Cloudmos Deploy application to create the Akash Validator\n* Please review our[ Cloudmos Deploy ](broken-reference)docs to install and configure the application if this is your first time using\n\n### Create the Akash Validator Deployment\n\n* Use the steps that follow - within Cloudmos Deploy - to create your Akash Validator deployment\n* The Akash SDL used additionally spins up a Stunnel server to facilitate secure communication with the TMKMS server created in later steps\n\n#### Create New Deployment\n\n* Use the `CREATE DEPLOYMENT` button to launch a new deployment\n\n![](../../../assets/validatorCreateDeployment.png)\n\n#### Empty Template Option\n\n* Select the `Empty` option as we will be copying a pre-constructed Akash SDL for the deployment\n\n![](../../../assets/validatorBlankTemplate.png)\n\n#### Copy SDL into Editor\n\n* Copy the following Akash SDL into the Editor pane\n* Reference the [Populated Editor](#populated-editor) section of this guide for further clarity\n* Note that the SDL is using persistent storage to allow data residency should your deployment restart.  Data will only persist thru the life of the associated Akash lease.\n\n> To ensure the most to update SDL is utilized, view the latest version [here](https://github.com/akash-network/cosmos-omnibus/blob/09679171d513586b5e1a9aafe73db55ebdbf5098/\\_examples/validator-and-tmkms/deploy.yml).\n\n##### SDL Edits\n\n* Consider updating the Pre-Shared Key (PSK) value in the  `proxy` service > `env` stanza to your own unique value.  The pre-shared key must match that which is defined in the upcoming [Stunnel Client](#stunnel-client) configuration.\n* Update the MONIKER in the `node` service > `env` stanza to your own name/organization name.\n*   No additional edits of this SDL are necessary for Akash Validator creation.\n\n\n\n```\n---\nversion: \"2.0\"\n\nservices:\n  node:\n    image: ghcr.io/akash-network/cosmos-omnibus:v0.3.42-akash-v0.22.7\n    env:\n      - MONIKER=my-moniker-1\n      - CHAIN_JSON=https://raw.githubusercontent.com/akash-network/net/main/mainnet/meta.json\n      - MINIMUM_GAS_PRICES=0.025uakt\n      - P2P_POLKACHU=1\n      - STATESYNC_POLKACHU=1\n      - AKASH_PRIV_VALIDATOR_LADDR=tcp://0.0.0.0:26658 # requires remote signer\n    expose:\n      - port: 26657\n        to:\n          - service: proxy # only exposed to proxy, not globally\n      - port: 26658\n        to:\n          - service: proxy # only exposed to proxy, not globally\n    params:\n      storage:\n        data:\n          mount: /root/.akash\n  proxy:\n    image: ghcr.io/ovrclk/stunnel-proxy:v0.0.1\n    env:\n      - PSK=DmtaC6N3HOWFkJZpNZs2dkabFT5yQONw # must match PSK in Stunnel client\n      - STUNNEL_SVC_RPC_ACCEPT=36657 # accept 36657\n      - STUNNEL_SVC_RPC_CONNECT=node:26657 # proxy 36657 to node:26657\n      - STUNNEL_SVC_SIGNER_ACCEPT=36658 # accept 36658\n      - STUNNEL_SVC_SIGNER_CONNECT=node:26658 # proxy 36658 to node:26658\n    expose:\n      - port: 36657 # expose TLS encrypted 36657 globally through a random port\n        to:\n          - global: true\n      - port: 36658 # expose TLS encrypted 36658 globally through a random port\n        to:\n          - global: true\n\nprofiles:\n  compute:\n    node:\n      resources:\n        cpu:\n          units: 4\n        memory:\n          size: 8Gi\n        storage:\n          - size: 512Mi\n          - name: data\n            size: 120Gi\n            attributes:\n              persistent: true\n              class: beta2\n    proxy:\n      resources:\n        cpu:\n          units: 1\n        memory:\n          size: 512Mi\n        storage:\n          size: 512Mi\n  placement:\n    dcloud:\n      attributes:\n        host: akash\n      signedBy:\n        anyOf:\n          - akash1365yvmc4s7awdyj3n2sav7xfx76adc6dnmlx63\n      pricing:\n        node:\n          denom: uakt\n          amount: 10000\n        proxy:\n          denom: uakt\n          amount: 10000\n\ndeployment:\n  node:\n    dcloud:\n      profile: node\n      count: 1\n  proxy:\n    dcloud:\n      profile: proxy\n      count: 1\n```\n\n##### Populated Editor\n\n> _**NOTE**_- SDL spans past the view in this panel and bottom portion is not displayed\n\n* Select the `CREATE DEPLOYMENT` button to proceed\n\n![](../../../assets/validatorSDL.png)\n\n\n#### Deployment Deposit\n\n* An escrow account is created for the deployment that is deducted from by the provider for the cost of the workload over time\n* By default 5 AKT is specified as the initial escrow deposit\n* If a deployment's escrow runs out of funds (0 AKT), the lease will be closed by the provider. Consider increasing the initial deposit to an amount that will be enough to fund the deployment for some time. And/or consider a strategy to ensure the escrow is re-funded on a periodic basis to ensure no disruption to your validator.\n* When ready select `DEPOSIT` to proceed and `APPROVE` any Transaction/gas fee prompts that follow\n\n![](../../../assets/validatorDeploymentDeposit.png)\n\n#### Select Akash Provider\n\n* A list of Akash Providers that have bid on your deployment is displayed\n* Choose the desired Provider from the list and then select `ACCEPT BID` to proceed\n\n![](../../../assets/validatorSelectProvider.png)\n\n#### Deployment Logs\n\n* The `LOGS > EVENTS` pane for the new Deployment will display\n* _**NOTE**_ - after a period of time the logs will display a `Back-off restarting failed container` message. This is expected as the container will not start until it has established a connection with the TMKMS server in subsequent steps.\n* Select the `LEASES` tab to proceed into the next step\n\n![](../../../assets/validatorDeploymentLogs.png)\n\n#### Capture Deployment URI and Port\n\n* In upcoming TMKMS configuration sections we will need to specify our Akash Validator deployment's URI and port\n* Capture this info from the `LEASES` tab for later use and specifically in the Stunnel Client section\n* In the example the following values would be captured (these values will be different for your deployment):\n\n> * **URI** - using the Provider field - `provider.mainnet-1.ca.aksh.pw`\n> * _**Signer Port**_ - using the Forwarded Port field and capturing the port forward to `36658` which in this example is `31684`\n> * _**Proxy/RPC Port**_ -using the Forwarded Port field and capturing the port forward to `36657` which in this example is `32675`\n\n\n![](../../../assets/akashValdiatorForwardedPorts.png)\n\n\n## TMKMS Setup\n\n### Considerations\n\n* In a future step in this guide we deploy a Stunnel client which must co-exist on the same machine as the TMKMS server\n* For simplicity we use Docker Compose to build the Stunnel client\n* We do not offer a TMKMS image based on security concerns in using a third party TMKMS image.  However you may want to consider using the Linux instructions below for TMKMS server build and create a container image yourself so that both the TKMKS server and the Stunnel client may both be deployed as containers on the single host.\n\n### Prepare TMKMS Dependencies (Ubuntu Instructions)\n\n* All steps in this section should be performed on the TMKMS server unless otherwise noted\n\n#### **Rust Install**\n\n```\ncurl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh\n\nsource $HOME/.cargo/env\n```\n\n#### **GCC**\n\n```\nsudo apt update\n\nsudo apt install git build-essential ufw curl jq snapd --yes\n```\n\n#### **Libusb**\n\n```\napt install libusb-1.0-0-dev\n\nexport RUSTFLAGS=-Ctarget-feature=+aes,+ssse3\n```\n\n### Setup TMKMS\n\n* All steps in this section should be performed on the TMKMS server unless otherwise noted\n\n#### Compiling TMKMS from Source Code\n\n```\ncd ~\ngit clone https://github.com/iqlusioninc/tmkms.git\ncd ~/tmkms\ncargo install tmkms --features=softsign\nmkdir /etc/tmkms\ntmkms init /etc/tmkms/\n```\n\n#### Copy Validator Private Key into TMKMS Config File\n\n* Create the `priv_validator_key.json` file\n\n```\nvi /etc/tmkms/secrets/priv_validator_key.json\n```\n\n* Copy/paste the validator private key into the `priv_validator_key.json` file\n\n##### Example `priv_validator_key.json` file\n\n```\n{\n  \"address\": \"3407CC1<REDACTED>243E2865\",\n  \"pub_key\": {\n    \"type\": \"tendermint/PubKeyEd25519\",\n    \"value\": \"5uAJKqd<REDACTED>mr5LrY6wsRs=\"\n  },\n  \"priv_key\": {\n    \"type\": \"tendermint/PrivKeyEd25519\",\n    \"value\": \"d1feQqRc<REDACTED>p1pDs6B6avkutjrCxGw==\"\n  }\n}\n```\n\n#### **Import the Private Validator Key into TMKMS**\n\n```\ntmkms softsign import /etc/tmkms/secrets/priv_validator_key.json /etc/tmkms/secrets/priv_validator_key.softsign\n```\n\n#### **Delete Private Key File on the Validator**\n\n* Conduct this step on the Akash Validator machine\n* Securely delete the priv\\_validator\\_key.json from your validator node and store it safely offline in case of an emergency. The `priv_validator_key` will be what TMKMS will use to sign for your validator.\n* Return to the TMKMS server after this step to complete subsequent steps in this section\n\n```\nshred -uvz ~/.akash/config/priv_validator_key.json\n```\n\n### **Modify tmkms.toml**\n\n* Begin by deleting the existing `tmkms.toml` file and re-creating anew\n\n```\nrm /etc/tmkms/tmkms.toml\n\nvi /etc/tmkms/tmkms.toml\n```\n\n* Copy the following configuration into the new `tmkms.toml` file\n* No edits to the file syntax below should be necessary\n\n```\n# Tendermint KMS configuration file\n\n## Chain Configuration\n\n### akashnet-2-dev Network\n\n[[chain]]\nid = \"akashnet-2-dev\"\nkey_format = { type = \"bech32\", account_key_prefix = \"akashpub\", consensus_key_prefix = \"akashvalconspub\" }\nstate_file = \"/etc/tmkms/state/akashnet-2-dev-consensus.json\"\n\n## Signing Provider Configuration\n\n### Software-based Signer Configuration\n\n[[providers.softsign]]\nchain_ids = [\"akashnet-2-dev\"]\nkey_type = \"consensus\"\npath = \"/etc/tmkms/secrets/priv_validator_key.softsign\"\n\n## Validator Configuration\n\n[[validator]]\nchain_id = \"akashnet-2-dev\"\naddr = \"tcp://127.0.0.1:36658\"\nsecret_key = \"/etc/tmkms/secrets/kms-identity.key\"\nprotocol_version = \"v0.34\"\nreconnect = true\n```\n\n## Start and Verify the TMKMS Service\n\nAll steps in this section should be performed on the TMKMS server unless otherwise noted\n\n### Start the TMKMS Service\n\n```\ntmkms start -c /etc/tmkms/tmkms.toml\n```\n\n#### Initial Log Messages\n\n* The following connection error messages will initially display after the TMKMS service start\n* Wait approximately 5-10 minutes for the connection to establish and at which time these error messages should cease\n\n```\n2022-03-08T23:42:37.926816Z  INFO tmkms::commands::start: tmkms 0.11.0 starting up...\n2022-03-08T23:42:37.926968Z  INFO tmkms::keyring: [keyring:softsign] added consensus Ed25519 key: osmovalconspub1zcjduepq2qfkp3ahrhaafzuqglme9mares0eluj58xr6cy7c37cdmzq0eecqk0yehr\n2022-03-08T23:42:37.927216Z  INFO tmkms::connection::tcp: KMS node ID: 948f8fee83f7715f99b8b8a53d746ef00e7b0d9e\n2022-03-08T23:42:37.929454Z ERROR tmkms::client: [osmosis-1@tcp://123.456.32.123:26659] I/O error: Connection refused (os error 111)\n```\n\n#### Log Messages Indicating Successful TMKMS Connection\n\n* _**NOTE**_ - these verifications and log entries will not be seen until after the Stunnel Client is created and configured\n* Eventually the following TMKMS log messages should display indicating successful connection between the TMKMS server and the Akash validator\n\n```\n2022-09-15T00:08:10.604353Z  INFO tmkms::connection::tcp: KMS node ID: 7a1f7c7f726d94787045cca9fee05c1ec67cd09a\n2022-09-15T00:08:10.899641Z  INFO tmkms::session: [akashnet-2@tcp://127.0.0.1:36658] connected to validator successfully\n2022-09-15T00:08:10.899670Z  WARN tmkms::session: [akashnet-2@tcp://127.0.0.1:36658]: unverified validator peer ID! (aa9cdfb9e1af2d8033168f562941a95c98545372)\n```\n\n## Stunnel Client\n\n### Overview\n\n* The Stunnel client is used to connect to the Stunnel server which was co-located on the Akash Validator server in prior steps\n* We will install the Stunnel client via Docker Compose\n* The Stunnel client must co-exist on the same server as TMKMS which was built in the prior step\n\n### Stunnel Client Source Code\n\n#### Stunnel Repository with Docker Files\n\n* The Stunnel Dockerfile and Docker Compose files used in this section are located [here](https://github.com/akash-network/stunnel-proxy) for your review\n\n#### Pull Down a Copy of the Stunnel Repo\n\n* This step and all subsequent steps should be performed on the server TMKMS was installed on prior\n\n```\nmkdir ~/stunnel\ncd ~/stunnel\ngit clone https://github.com/akash-network/stunnel-proxy\n```\n\n### Customize the Stunnel Client Docker Compose File\n\n* Navigate into the `client` directory and make necessary updates to the `docker-compose.yml`file as detailed below\n\n```\ncd ~/stunnel/stunnel-proxy/client\nvi docker-compose.yml\n```\n\n#### Necessary Docker Compose File Updates\n\n* Use the details captured in the [Akash Validator Deployment](#akash-validator-deployment) section of the guide - within the `Capture Deployment URI and Port` subsection - for the Docker Compose file updates discussed below\n\n#### STUNNEL\\_SVC\\_RPC\\_CONNECT\n\n* Update the `STUNNEL_SVC_RPC_CONNECT` field\n* This field should use the exposed Proxy/RPC port captured during the Akash Validator deployment and within this step\n* Template - update the provider and exposed port variables with your unique Akash deployment values\n\n```\nSTUNNEL_SVC_RPC_CONNECT=<akash-provider-url>:<expose-proxy-port>\n```\n\n* Example of the setting based on the example deployment in this guide\n\n```\nSTUNNEL_SVC_RPC_CONNECT=provider.mainnet-1.ca.aksh.pw:32675\n```\n\n##### STUNNEL\\_SVC\\_SIGNER\\_CONNECT\n\n* Update the `STUNNEL_SVC_SIGNER_CONNECT` field\n* This field should use the exposed Signer port captured during the Akash Validator deployment and within this step\n* Template - update the provider and exposed port variables with your unique Akash deployment values\n\n```\nSTUNNEL_SVC_SIGNER_CONNECT=<akash-provider-url>:<signer-port>\n```\n\n* Example of the setting based on the example deployment in this guide\n\n```\nSTUNNEL_SVC_SIGNER_CONNECT=provider.mainnet-1.ca.aksh.pw:31684\n```\n\n##### Network > Server\\_Default Setting\n\n* Update the Network > Server\\_Default setting to external: false such as:\n\n```\nnetworks:\n  server_default:\n    external: false\n```\n\n##### Pre-Shared Key (PSK)\n\n* Ensure the PSK field matches the value defined earlier in the [Akash Validator Deployment](#akash-validator-deployment) section\n\n#### Example `docker-compose.yaml` File\n\n```\nversion: \"3.3\"\nservices:\n  stunnel:\n    build: ../\n    environment:\n      - PSK=DmtaC6N3HOWFkJZpNZs2dkabFT5yQONw\n      - STUNNEL_SVC_RPC_CLIENT=yes\n      - STUNNEL_SVC_RPC_ACCEPT=0.0.0.0:36657\n      - STUNNEL_SVC_RPC_CONNECT=provider.mainnet-1.ca.aksh.pw:32323\n      - STUNNEL_SVC_SIGNER_CLIENT=yes\n      - STUNNEL_SVC_SIGNER_ACCEPT=0.0.0.0:36658\n      - STUNNEL_SVC_SIGNER_CONNECT=provider.mainnet-1.ca.aksh.pw:32435\n    networks:\n      - server_default\n    ports:\n      - '36657:36657'\n      - '36658:36658'\nnetworks:\n  server_default:\n    external: false\n```\n\n### Docker Compose Up\n\n* Create the Stunnel Client Container\n\n```\ncd ~/stunnel/stunnel-proxy/client\ndocker compose up -d\n```\n\n### Stunnel Client Successful Connection Logs\n\n#### Confirm Name of Container\n\n```\ndocker container ls\n```\n\n##### Example Output\n\n```\nCONTAINER ID   IMAGE            COMMAND                  CREATED          STATUS          PORTS                                                                   NAMES\na4bd1b21f551   client-stunnel   \"/usr/local/configur…\"   44 minutes ago   Up 44 minutes   0.0.0.0:36657-36658->36657-36658/tcp, :::36657-36658->36657-36658/tcp   client-stunnel-1\n```\n\n#### View Stunnel Client Container Logs\n\n```\ndocker container logs <container-name-obtained-in-last-step>\n```\n\n##### Example\n\n```\ndocker container logs client-stunnel-1\n```\n\n#### &#x20;Example Logs Indicating Successful Connection\n\n```\nclient-stunnel-1  | 2022.09.15 00:08:09 LOG6[0]: s_connect: connecting 216.202.234.124:30017\nclient-stunnel-1  | 2022.09.15 00:08:09 LOG7[0]: s_connect: s_poll_wait 216.202.234.124:30017: waiting 10 seconds\nclient-stunnel-1  | 2022.09.15 00:08:09 LOG5[0]: s_connect: connected 216.202.234.124:30017\nclient-stunnel-1  | 2022.09.15 00:08:09 LOG5[0]: Service [signer] connected remote server from 172.18.0.2:48336\nclient-stunnel-1  | 2022.09.15 00:08:09 LOG7[0]: Setting remote socket options (FD=8)\nclient-stunnel-1  | 2022.09.15 00:08:09 LOG7[0]: Option TCP_NODELAY set on remote socket\nclient-stunnel-1  | 2022.09.15 00:08:09 LOG7[0]: Remote descriptor (FD=8) initialized\nclient-stunnel-1  | 2022.09.15 00:08:09 LOG6[0]: SNI: sending servername: provider.mainnet-1.ca.aksh.pw\nclient-stunnel-1  | 2022.09.15 00:08:09 LOG6[0]: Peer certificate not required\nclient-stunnel-1  | 2022.09.15 00:08:09 LOG7[0]: TLS state (connect): before SSL initialization\nclient-stunnel-1  | 2022.09.15 00:08:09 LOG6[0]: PSK client configured for identity \"node\"\nclient-stunnel-1  | 2022.09.15 00:08:09 LOG7[0]: TLS state (connect): SSLv3/TLS write client hello\nclient-stunnel-1  | 2022.09.15 00:08:09 LOG7[0]: TLS state (connect): SSLv3/TLS write client hello\nclient-stunnel-1  | 2022.09.15 00:08:09 LOG7[0]: TLS state (connect): SSLv3/TLS read server hello\nclient-stunnel-1  | 2022.09.15 00:08:09 LOG7[0]: TLS state (connect): SSLv3/TLS write change cipher spec\nclient-stunnel-1  | 2022.09.15 00:08:09 LOG6[0]: PSK client configured for identity \"node\"\nclient-stunnel-1  | 2022.09.15 00:08:09 LOG7[0]: Deallocating application specific data for session connect address\nclient-stunnel-1  | 2022.09.15 00:08:09 LOG7[0]: TLS state (connect): SSLv3/TLS write client hello\nclient-stunnel-1  | 2022.09.15 00:08:09 LOG7[0]: TLS state (connect): SSLv3/TLS write client hello\nclient-stunnel-1  | 2022.09.15 00:08:09 LOG7[0]: Deallocating application specific data for session connect address\nclient-stunnel-1  | 2022.09.15 00:08:09 LOG7[0]: TLS state (connect): SSLv3/TLS read server hello\nclient-stunnel-1  | 2022.09.15 00:08:09 LOG7[0]: TLS state (connect): TLSv1.3 read encrypted extensions\nclient-stunnel-1  | 2022.09.15 00:08:09 LOG7[0]: TLS state (connect): SSLv3/TLS read finished\nclient-stunnel-1  | 2022.09.15 00:08:09 LOG7[0]: TLS state (connect): SSLv3/TLS write finished\nclient-stunnel-1  | 2022.09.15 00:08:09 LOG7[0]:      1 client connect(s) requested\nclient-stunnel-1  | 2022.09.15 00:08:09 LOG7[0]:      1 client connect(s) succeeded\nclient-stunnel-1  | 2022.09.15 00:08:09 LOG7[0]:      0 client renegotiation(s) requested\nclient-stunnel-1  | 2022.09.15 00:08:09 LOG7[0]:      1 session reuse(s)\n```\n\n## Verify Validator Status\n\n### Overview\n\n* In this section we will verify the state of the Akash Validator following successful integration of TMKMS and Stunnel\n* Use Cloudmos Deploy for the validations in this section\n\n### Verify Stunnel Proxy Service\n\n* Navigate into your Akash Validator deployment within Cloudmos\n* Enter the `LOGS` tab of the deployment\n* From the `Services` drop down menu > select `proxy`\n* With Stunnel logs isolated we should see successful TLS connection messages such as the examples below\n\n```\nproxy: 2022.09.15 00:08:10 LOG7[12]: TLS state (accept): SSLv3/TLS write finished\nproxy: 2022.09.15 00:08:10 LOG7[12]: TLS state (accept): TLSv1.3 early data\nproxy: 2022.09.15 00:08:10 LOG7[12]: TLS state (accept): TLSv1.3 early data\nproxy: 2022.09.15 00:08:10 LOG7[12]: TLS state (accept): SSLv3/TLS read finished\nproxy: 2022.09.15 00:08:10 LOG7[12]:      8 server accept(s) requested\nproxy: 2022.09.15 00:08:10 LOG7[12]:      2 server accept(s) succeeded\nproxy: 2022.09.15 00:08:10 LOG7[12]:      0 server renegotiation(s) requested\nproxy: 2022.09.15 00:08:10 LOG7[12]:      4 session reuse(s)\nproxy: 2022.09.15 00:08:10 LOG7[12]:      1 internal session cache item(s)\nproxy: 2022.09.15 00:08:10 LOG7[12]:      0 internal session cache fill-up(s)\nproxy: 2022.09.15 00:08:10 LOG7[12]:      0 internal session cache miss(es)\nproxy: 2022.09.15 00:08:10 LOG7[12]:      0 external session cache hit(s)\nproxy: 2022.09.15 00:08:10 LOG7[12]:      0 expired session(s) retrieved\nproxy: 2022.09.15 00:08:10 LOG7[12]: New session callback\nproxy: 2022.09.15 00:08:10 LOG6[12]: No peer certificate received\nproxy: 2022.09.15 00:08:10 LOG6[12]: Session id: A33315836C947E567A8BBA625BAB4C137F6B26DEAF54142D56E87775A44C26BA\nproxy: 2022.09.15 00:08:10 LOG7[12]: TLS state (accept): SSLv3/TLS write session ticket\nproxy: 2022.09.15 00:08:10 LOG6[12]: TLS accepted: previous session reused\nproxy: 2022.09.15 00:08:10 LOG6[12]: TLSv1.3 ciphersuite: TLS_CHACHA20_POLY1305_SHA256 (256-bit encryption)\nproxy: 2022.09.15 00:08:10 LOG7[12]: Compression: null, expansion: null\nproxy: 2022.09.15 00:08:10 LOG6[12]: Session id: A33315836C947E567A8BBA625BAB4C137F6B26DEAF54142D56E87775A44C26BA\n```\n\n### Verify Validator Status\n\n* Navigate into your Akash Validator deployment within Cloudmos\n* Enter the `LOGS` tab of the deployment\n* From the `Services` drop down menu > select `node`\n* With Validator logs isolated we should see successful `executed block` and `committed state` messages such as those below\n\n```\nnode: \u001b[90m1:19AM\u001b[0m \u001b[32mINF\u001b[0m executed block \u001b[36mheight=\u001b[0m7627065 \u001b[36mmodule=\u001b[0mstate \u001b[36mnum_invalid_txs=\u001b[0m0 \u001b[36mnum_valid_txs=\u001b[0m3\nnode: \u001b[90m1:19AM\u001b[0m \u001b[32mINF\u001b[0m commit synced \u001b[36mcommit=\u001b[0m436F6D6D697449447B5B32362031333220343820373520313930203320343720323238203136372038352031303520323520313533203539203630203234392031343720313236203938203136312032323220313620313336203230322032343220313137203139332032303620313234203830203131322032305D3A3734363133397D\nnode: \u001b[90m1:19AM\u001b[0m \u001b[32mINF\u001b[0m committed state \u001b[36mapp_hash=\u001b[0m1A84304BBE032FE4A7556919993B3CF9937E62A1DE1088CAF275C1CE7C507014 \u001b[36mheight=\u001b[0m7627065 \u001b[36mmodule=\u001b[0mstate \u001b[36mnum_txs=\u001b[0m3\nnode: \u001b[90m1:19AM\u001b[0m \u001b[32mINF\u001b[0m indexed block \u001b[36mheight=\u001b[0m7627065 \u001b[36mmodule=\u001b[0mtxindex\nnode: \u001b[90m1:19AM\u001b[0m \u001b[32mINF\u001b[0m Timed out \u001b[36mdur=\u001b[0m4875.841288 \u001b[36mheight=\u001b[0m7627066 \u001b[36mmodule=\u001b[0mconsensus \u001b[36mround=\u001b[0m0 \u001b[36mstep=\u001b[0m1\nnode: \u001b[90m1:19AM\u001b[0m \u001b[32mINF\u001b[0m received proposal \u001b[36mmodule=\u001b[0mconsensus \u001b[36mproposal=\u001b[0m{\"Type\":32,\"block_id\":{\"hash\":\"EA1BA91C01F647AC96B855177BE4696FF8B5A470237598A87275B395A1E5C58B\",\"parts\":{\"hash\":\"79AF71041EA62A35DA4507A32491F4EF0ADC4E570869E9A5406208A33818EC97\",\"total\":1}},\"height\":7627066,\"pol_round\":-1,\"round\":0,\"signature\":\"PBwZZYMEehs/k5jv8RnPSSvYVnMdCHB4UylPnZuxmhL1146okZMaU+huMjJYV8sjAUR6JNoJC8wlwrKHQuUkCA==\",\"timestamp\":\"2022-09-15T01:20:16.268331046Z\"}\nnode: \u001b[90m1:20AM\u001b[0m \u001b[32mINF\u001b[0m received complete proposal block \u001b[36mhash=\u001b[0mEA1BA91C01F647AC96B855177BE4696FF8B5A470237598A87275B395A1E5C58B \u001b[36mheight=\u001b[0m7627066 \u001b[36mmodule=\u001b[0mconsensus\nnode: \u001b[90m1:20AM\u001b[0m \u001b[32mINF\u001b[0m finalizing commit of block \u001b[36mhash=\u001b[0mEA1BA91C01F647AC96B855177BE4696FF8B5A470237598A87275B395A1E5C58B \u001b[36mheight=\u001b[0m7627066 \u001b[36mmodule=\u001b[0mconsensus \u001b[36mnum_txs=\u001b[0m1 \u001b[36mroot=\u001b[0m1A84304BBE032FE4A7556919993B3CF9937E62A1DE1088CAF275C1CE7C507014\nnode: \u001b[90m1:20AM\u001b[0m \u001b[32mINF\u001b[0m minted coins from module account \u001b[36mamount=\u001b[0m4816074uakt \u001b[36mfrom=\u001b[0mmint \u001b[36mmodule=\u001b[0mx/bank\nnode: \u001b[90m1:20AM\u001b[0m \u001b[32mINF\u001b[0m executed block \u001b[36mheight=\u001b[0m7627066 \u001b[36mmodule=\u001b[0mstate \u001b[36mnum_invalid_txs=\u001b[0m0 \u001b[36mnum_valid_txs=\u001b[0m1\nnode: \u001b[90m1:20AM\u001b[0m \u001b[32mINF\u001b[0m commit synced \u001b[36mcommit=\u001b[0m436F6D6D697449447B5B31393520323430203835203130372032382034362035332032303720313735203232322039372031343420383220363720313731203137332031303420323130203135342035382032353420323431203231302032343420323120313431203137392033312034352031343120323438203134385D3A3734363133417D\nnode: \u001b[90m1:20AM\u001b[0m \u001b[32mINF\u001b[0m committed state \u001b[36mapp_hash=\u001b[0mC3F0556B1C2E35CFAFDE61905243ABAD68D29A3AFEF1D2F4158DB31F2D8DF894 \u001b[36mheight=\u001b[0m7627066 \u001b[36mmodule=\u001b[0mstate \u001b[36mnum_txs=\u001b[0m1\nnode: \u001b[90m1:20AM\u001b[0m \u001b[32mINF\u001b[0m indexed block \u001b[36mheight=\u001b[0m7627066 \u001b[36mmodule=\u001b[0mtxindex\n```","description":null,"slug":"docs/validators/akash-validator-with-tmkms-and-stunnel"},{"title":"Running a Validator","body":"\n## What is an Akash Validator?\n\nValidators are responsible for committing new blocks to the blockchain through voting. A validator's stake is slashed if they become unavailable or sign blocks at the same height. Please read about [Sentry Node Architecture](https://forum.cosmos.network/t/sentry-node-architecture-overview/454) to learn how to protect your node from DDOS attacks and to ensure high-availability on mainnet.\n\n## Akash Validator General Info\n\n### Validator Hardware Requirements and Recommendations\n\n* CPU - 4/8 Core\n* Memory - 8/16GB\n* Disk - SSD or NVMe\n  * Size - 512GB or larger\n\n### Active Validator Set\n\n* 100 (current) which was extended from 85 recently\n* Tokens to stake to be active = more than last active validator\n* Check with the command `$votingpower` in the Akash Discord server's > validators-status channel for current requirements to get into the active validator set\n* You can stake by yourself or from external wallets within your community\n\n### Akash Node Build\n\nBefore setting up your validator node, make sure you've already gone through the[ Full Node Setup ](/docs/docs/akash-nodes/akash-node-cli-build/)guide.\n\n## Create Your Validator\n\n### Validator Account and Network Configuration\n\n#### Validator Account\n\nPrior to validator creation we must create an Akash account for validator use.  Use the steps covered in this [guide](https://docs.akash.network/guides/cli/detailed-steps) to create this account.  _**NOTE -**_ only the \\`Create an Account\\` and \\`Fund your Account\\` sections of this guide need to be completed for this purpose.\n\n#### Network Configuration\n\nConfigure settings to communicate with the Akash blockchain as follows:\n\n```\nAKASH_NET=\"https://raw.githubusercontent.com/akash-network/net/main/mainnet\"\nexport AKASH_CHAIN_ID=\"$(curl -s \"$AKASH_NET/chain-id.txt\")\"\n```\n\n### Validator Creation\n\nYour `akashvalconspub` can be used to create a new validator by staking tokens. You can find your validator pubkey by running:\n\n```bash\nakash tendermint show-validator\n```\n\nThe file that stores this private key lives at `~/.akash/config/priv_validator_key.json`. To create your validator, just use the following command.\n\n> Note that in the output of this command your \\`akashvaloper\\` address will be revealed.  Note  this address for future use including the verification steps later in this guide.\n\n```bash\nakash tx staking create-validator \\\n  --amount=1000000uakt \\\n  --pubkey=\"$(akash tendermint show-validator)\" \\\n  --moniker=\"$AKASH_MONIKER\" \\\n  --chain-id=\"$AKASH_CHAIN_ID\" \\\n  --commission-rate=\"0.10\" \\\n  --commission-max-rate=\"0.20\" \\\n  --commission-max-change-rate=\"0.01\" \\\n  --min-self-delegation=\"1\" \\\n  --gas=\"auto\" \\\n  --gas-prices=\"0.025uakt\" \\\n  --gas-adjustment=1.5 \\\n  --from=\"$AKASH_KEY_NAME\"\n```\n\n::: tip When specifying commission parameters, the `commission-max-change-rate` is used to measure % _point_ change over the `commission-rate`. E.g. 1% to 2% is a 100% rate increase, but only 1 percentage point. :::\n\n::: tip `min-self-delegation` is a stritly positive integer that represents the minimum amount of self-delegated voting power your validator must always have. A `min-self-delegation` of 1 means your validator will never have a self-delegation lower than `1000000uakt` :::\n\nYou can confirm that you are in the validator set by using a third party explorer for the testnet you are joining.\n\n## Edit Validator Description\n\nYou can edit your validator's public description. This info is to identify your validator, and will be relied on by delegators to decide which validators to stake to. Make sure to provide input for every flag below. If a flag is not included in the command the field will default to empty (`--moniker` defaults to the machine name) if the field has never been set or remain the same if it has been set in the past.\n\nThe `$AKASH_KEY_NAME` specifies the key for the validator which you are editing. If you choose to not include certain flags, remember that the `--from` flag must be included to identify the validator to update.\n\nThe `--identity` can be used as to verify identity with systems like Keybase or UPort. When using with Keybase `--identity` should be populated with a 16-digit string that is generated with a [keybase.io](https://keybase.io) account. It's a cryptographically secure method of verifying your identity across multiple online networks. The Keybase API allows explorers to retrieve your Keybase avatar. This is how you can add a logo to your validator profile.\n\n```bash\nakash tx staking edit-validator\n  --new-moniker=\"$AKASH_MONIKER\" \\\n  --website=\"https://akash.network\" \\\n  --identity=6A0D65E29A4CBC8E \\\n  --details=\"The SUPERCLOUD IS HERE!\" \\\n  --chain-id=\"$AKASH_CHAIN_ID\" \\\n  --gas=\"auto\" \\\n  --gas-prices=\"0.025uakt\" \\\n  --gas-adjustment=1.5 \\\n  --from=\"$AKASH_KEY_NAME\" \\\n  --commission-rate=\"0.10\"\n```\n\n**Note**: The `commission-rate` value must adhere to the following invariants:\n\n* Must be between 0 and the validator's `commission-max-rate`\n*   Must not exceed the validator's `commission-max-change-rate` which is maximum\n\n    % point change rate **per day**. In other words, a validator can only change\n\n    its commission once per day and within `commission-max-change-rate` bounds.\n\n## View Validator Description\n\nView the validator's information with this command:\n\n```bash\nakash query staking validator $AKASH_VALIDATOR_ADDRESS\n```\n\n## Track Validator Signing Information\n\nIn order to keep track of a validator's signatures in the past you can do so by using the `signing-info` command:\n\n```bash\nakash query slashing signing-info $AKASH_VALIDATOR_PUBKEY \\\n  --chain-id=\"$AKASH_CHAIN_ID\"\n```\n\n## Unjail Validator\n\nWhen a validator is \"jailed\" for downtime, you must submit an `Unjail` transaction from the operator account in order to be able to get block proposer rewards again (depends on the zone fee distribution).\n\n```bash\nakash tx slashing unjail \\\n    --from=\"$AKASH_KEY_NAME\" \\\n    --chain-id=\"$AKASH_CHAIN_ID\"\n```\n\n## Confirm Your Validator is Running\n\n### Ensure Validator Sync\n\n* Ensure that the field \\`catching\\_up\\` is false and that the latest block corresponds to the current block of the blockchain\n\n```\nakash status\n```\n\n#### Example Output when Validator is in Sync\n\n```\n{\"NodeInfo\":{\"protocol_version\":{\"p2p\":\"8\",\"block\":\"11\",\"app\":\"0\"},\"id\":\"136d67725800cb5a8baeb3e97dbdc3923879461e\",\"listen_addr\":\"tcp://0.0.0.0:26656\",\"network\":\"akashnet-2\",\"version\":\"0.34.19\",\"channels\":\"40202122233038606100\",\"moniker\":\"cznode\",\"other\":{\"tx_index\":\"on\",\"rpc_address\":\"tcp://0.0.0.0:26657\"}},\"SyncInfo\":{\"latest_block_hash\":\"4441B6D626166822979597F252B333C131C1DCB2F0467FF282EF0EAA3936B8CC\",\"latest_app_hash\":\"1ED64AE88E5CFD53651CD2B6B4E970292778071C8FBE23289393B4A06F57975F\",\"latest_block_height\":\"8284780\",\"latest_block_time\":\"2022-10-31T15:28:17.565762161Z\",\"earliest_block_hash\":\"E25CE5DD10565D6D63CDA65C8653A15F962A4D2960D5EC45D1DC0A4DE06F8EE3\",\"earliest_app_hash\":\"19526102DDBCE254BA71CC8E44185721D611635F638624C6F950EF31D3074E2B\",\"earliest_block_height\":\"5851001\",\"earliest_block_time\":\"2022-05-12T17:51:58.430492536Z\",\"catching_up\":false},\"ValidatorInfo\":{\"Address\":\"354E7FA2BF8C5B9C0F1C80F1C222818EC992D377\",\"PubKey\":{\"type\":\"tendermint/PubKeyEd25519\",\"value\":\"1+dVHZD7kfqnDU6I+bKbCv4ZE1LPieyMH+mwsOowhqY=\"},\"VotingPower\":\"0\"}}\n```\n\n### Confirm Validator's Staking Status\n\n#### Template\n\n```\nakash query staking validator <akashvaloper-address>\n```\n\n#### Example\n\n```\nakash query staking validator akashvaloper16j3ge9lkpgtdkzntlja08gt6l63fql60xdupxq\n```\n\n#### Example Output\n\n* Status will display as \\`BOND\\_STATUS\\_UNBONDED\\` after initial build\n\n```\ncommission:\n  commission_rates:\n    max_change_rate: \"0.010000000000000000\"\n    max_rate: \"0.200000000000000000\"\n    rate: \"0.100000000000000000\"\n  update_time: \"2022-10-31T15:24:25.040091667Z\"\nconsensus_pubkey:\n  '@type': /cosmos.crypto.ed25519.PubKey\n  key: 1+dVHZD7kfqnDU6I+bKbCv4ZE1LPieyMH+mwsOowhqY=\ndelegator_shares: \"1000000.000000000000000000\"\ndescription:\n  details: \"\"\n  identity: \"\"\n  moniker: cznode\n  security_contact: \"\"\n  website: \"\"\njailed: false\nmin_self_delegation: \"1\"\noperator_address: akashvaloper1jy7ej9t6r8q5dyjrst88nt9rjgkdltgx97wfvd\nstatus: BOND_STATUS_UNBONDED\ntokens: \"1000000\"\nunbonding_height: \"0\"\nunbonding_time: \"1970-01-01T00:00:00Z\"\n```\n\n### Active Set Confirmation\n\nYour validator is active if the following command returns anything\n\n> _**NOTE**_ - this command will only display output of your validator is in the active set\n\n```bash\nakash query tendermint-validator-set | grep \"$(akash tendermint show-validator)\"\n```\n\nYou should now see your validator in one of the Akash Testnet explorers. You are looking for the `bech32` encoded `address` in the `~/.akash/config/priv_validator.json` file.\n\n## Halting Your Validator\n\nWhen attempting to perform routine maintenance or planning for an upcoming coordinated upgrade, it can be useful to have your validator systematically and gracefully halt. You can achieve this by either setting the `halt-height` to the height at which you want your node to shutdown or by passing the `--halt-height` flag to `akash`. The node will shutdown with a zero exit code at that given height after committing the block.\n\n## Common Problems\n\n### Problem #1: My validator has `voting_power: 0`\n\nYour validator has become jailed. Validators get jailed, i.e. get removed from the active validator set, if they do not vote on `500` of the last `10000` blocks, or if they double sign.\n\nIf you got jailed for downtime, you can get your voting power back to your validator. First, if `akash` is not running, start it up again. If you are running `systemd` this will be different:\n\n```bash\nakash start\n```\n\nWait for your full node to catch up to the latest block. Then, you can [unjail your validator](#unjail-validator)\n\nLastly, check your validator again to see if your voting power is back.\n\n```bash\nakash status\n```\n\nYou may notice that your voting power is less than it used to be. That's because you got slashed for downtime!\n\n### Problem #2: My `akash` crashes because of `too many open files`\n\nThe default number of files Linux can open (per-process) is `1024`. `akash` is known to open more than `1024` files. This causes the process to crash. A quick fix is to run `ulimit -n 4096` (increase the number of open files allowed) and then restart the process with `akash start`. If you are using `systemd` or another process manager to launch `akash` this may require some configuration at that level. See the [`systemd` configuration doc](https://github.com/akash-network/docs/tree/1c9232aaec2197efbf4532e8883a247566cf9e28/guides/node/systemd.md) for details on how to configure `systemd` to aleviate this issue.","description":"Akash Validator Nodes enables interactions with the network, validation of transactions, and participation in the consensus process.","slug":"docs/validators/running-a-validator"},{"title":"Exploration of Akash Queries Using the Akash API Source Code","body":"\n\nIn this section we will use an Akash deployments query as an example of:\n\n* General use of the Akash API repository within custom code\n* Explore Akash API Go code generated thru protoc\n* Using the Protobuf generated gateway for blockchain queries\n* Sending receipt of Protobuf messages with HTTP API requests to Akash RPC nodes\n\n#### Sections Within This Guide\n\n* [gRPC Gateway Use](#grpc-gateway-use)\n* [Example gRPC Gateway Use and Deep Dive Explanations](#example-grpc-gateway-use-and-deep-dive-explanations)\n\n## gRPC Gateway Use\n\nThe Protobuf generated gRPC gateway creates a HTTP handler for upstream gRPC queries.  The gRPC gateway provides an interface for clients incapable of using gRPC calls directly.  In this guide we use the gRPC source code for blockchain queries to remove friction and ease of development.\n\nThe gRPC gateway definitions used on our example query of Akash deployments are located [here](https://github.com/akash-network/akash-api/blob/main/go/node/deployment/v1beta3/query.pb.gw.go).\n\n## Example gRPC Gateway Use and Deep Dive Explanations\n\n### Subsections within Code Exploration\n\n- [gRPC Gateway Use](#grpc-gateway-use)\n- [Example gRPC Gateway Use and Deep Dive Explanations](#example-grpc-gateway-use-and-deep-dive-explanations)\n\t- [Subsections within Code Exploration](#subsections-within-code-exploration)\n\t- [Complete Code Example - Query Deployments with Owner Filter](#complete-code-example---query-deployments-with-owner-filter)\n\t- [Deep Code Exploration Using Query Deployments Example](#deep-code-exploration-using-query-deployments-example)\n\t\t- [Protoc Generated Go Code Overview](#protoc-generated-go-code-overview)\n\t\t- [Protoc Generated File Deepdive](#protoc-generated-file-deepdive)\n\n### Complete Code Example - Query Deployments with Owner Filter\n\n> _**NOTE**_ - prior to execution of this code ensure package dependencies mirror those found in the source code repo `go.mod` file [here](https://github.com/akash-network/akash-api/blob/main/go.mod)\n\n```\npackage main\n\nimport (\n\t\"encoding/hex\"\n\t\"fmt\"\n\t\"io/ioutil\"\n\t\"net/http\"\n\t\"strings\"\n\n\t\"github.com/golang/protobuf/proto\"\n\n\t\"github.com/akash-network/akash-api/go/node/deployment/v1beta3\"\n)\n\nfunc main() {\n\t// Replace string with preferred Akash RPC Node\n\t// At the time of this writing a Testnet RPC Node is utilized to utilize the latest version of the Akash API\n\turl := \"https://rpc.testnet-02.aksh.pw:443\"\n\tmethod := \"GET\"\n\n\t// Create the QueryDeploymentsRequest with necessary filters\n\t// Replace the Owner address with the account of interest and to query deplpyments associated with that account\n\trequest := &v1beta3.QueryDeploymentsRequest{\n\t\tFilters: v1beta3.DeploymentFilters{\n\t\t\t// Set any filters you need to query deployments\n\t\t\t// For example, you can set the owner of the deployment, status, etc.\n\t\t\tOwner: \"akash1w3k6qpr4uz44py4z68chfrl7ltpxwtkngnc6xk\",\n\t\t},\n\t\t// Pagination: &v1beta3.PageRequest{\n\t\t// \t// Set pagination options if needed\n\t\t// \t// For example, you can set the number of results per page, the page number, etc.\n\t\t// },\n\t}\n\n\t// Manually serialize the struct fields into the protobuf message\n\tdata, err := manualMarshal(request)\n\tif err != nil {\n\t\tfmt.Println(\"Error encoding protobuf:\", err)\n\t\treturn\n\t}\n\n\t// Convert the byte slice to a hexadecimal string\n\tprotoMessage := hex.EncodeToString(data)\n\n\t// Construct the payload string and use the protoMessage variable in the data key\n\tpayload := strings.NewReader(fmt.Sprintf(`{\"jsonrpc\":\"2.0\",\"id\":0,\"method\":\"abci_query\",\"params\":{\"data\":\"%s\",\"height\":\"0\",\"path\":\"/akash.deployment.v1beta3.Query/Deployments\",\"prove\":false}}`, protoMessage))\n\n\tclient := &http.Client{}\n\treq, err := http.NewRequest(method, url, payload)\n\n\tif err != nil {\n\t\tfmt.Println(err)\n\t\treturn\n\t}\n\treq.Header.Add(\"Content-Type\", \"application/json\")\n\n\tres, err := client.Do(req)\n\tif err != nil {\n\t\tfmt.Println(err)\n\t\treturn\n\t}\n\tdefer res.Body.Close()\n\n\tbody, err := ioutil.ReadAll(res.Body)\n\tif err != nil {\n\t\tfmt.Println(err)\n\t\treturn\n\t}\n\n\t// Prints protobuf encoded message that can be converted to human readable format/Go struct\n\tfmt.Println(string(body))\n\n}\n\n// Manually serialize the struct fields into the protobuf message\nfunc manualMarshal(msg proto.Message) ([]byte, error) {\n\tvar buf proto.Buffer\n\tif err := buf.Marshal(msg); err != nil {\n\t\treturn nil, err\n\t}\n\treturn buf.Bytes(), nil\n}\n\n```\n\n### Deep Code Exploration Using Query Deployments Example\n\n#### Protoc Generated Go Code Overview\n\n* Within the example Deployments blockchain query we make use of:\n  * [ ] Deployments protobuf definitions which are originally defined [here](https://github.com/akash-network/akash-api/tree/main/proto/node/akash/deployment/v1beta3) and specifically in the `query.proto` definition file\n  * [ ] Go code generated via protoc and that are located [here](https://github.com/akash-network/akash-api/tree/main/go/node/deployment/v1beta3) and specifically the `query.pb.gw.go` and `query.pb.go` files\n\n> _**NOTE**_ - the protoc generated Go files used in our custom code examples are the same type definitions used throughout Akash source code.  For example - the Akash CLI source code and specifically Deployment/Deployments queries are defined [here](https://github.com/akash-network/node/blob/main/x/deployment/client/cli/query.go).   Within the referenced file and CLI client definitions the types located in `github.com/akash-network/akash-api/go/node/deployment/v1beta3` are utilized.  These are the same definitions and types we we use in this custom client code example.\n\n#### Protoc Generated File Deepdive\n\n* The example Deployments Query code uses the protoc generated gRPC gateway\n* The gRPC gateway registers the DeploymentsQuery endpoint\n* For DeploymentsQuery the HTTP GET request requires the following attributes in the payload with associated values for our use case:\n  * [ ] `\"path\":\"/akash.deployment.v1beta3.Query/Deployments\"`\n  * [ ] `\"data\"` - protobuf encoded message&#x20;\n\n_**Custom/Relevant Code**_\n\n* Utilize the protoc generated Go type of `QueryDeploymentsRequest`\n* Embed protobuf encoded payload in HTTP request to gRPC gateway\n\n```\n\t// Create the QueryDeploymentsRequest with necessary filters\n\t// Replace the Owner address with the account of interest and to query deplpyments associated with that account\n\trequest := &v1beta3.QueryDeploymentsRequest{\n\t\tFilters: v1beta3.DeploymentFilters{\n\t\t\t// Set any filters you need to query deployments\n\t\t\t// For example, you can set the owner of the deployment, status, etc.\n\t\t\tOwner: \"akash1w3k6qpr4uz44py4z68chfrl7ltpxwtkngnc6xk\",\n\t\t},\n\t\t// Pagination: &v1beta3.PageRequest{\n\t\t// \t// Set pagination options if needed\n\t\t// \t// For example, you can set the number of results per page, the page number, etc.\n\t\t// },\n\t}\n\n\t// Manually serialize the struct fields into the protobuf message\n\tdata, err := manualMarshal(request)\n\tif err != nil {\n\t\tfmt.Println(\"Error encoding protobuf:\", err)\n\t\treturn\n\t}\n\n\t// Convert the byte slice to a hexadecimal string\n\tprotoMessage := hex.EncodeToString(data)\n\n\t// Construct the payload string and use the protoMessage variable in the data key\n\tpayload := strings.NewReader(fmt.Sprintf(`{\"jsonrpc\":\"2.0\",\"id\":0,\"method\":\"abci_query\",\"params\":{\"data\":\"%s\",\"height\":\"0\",\"path\":\"/akash.deployment.v1beta3.Query/Deployments\",\"prove\":false}}`, protoMessage))\n\n```\n\n_**QueryDeploymentsRequest Type**_\n\n* Source code for this type is located [here](https://github.com/akash-network/akash-api/blob/main/go/node/deployment/v1beta3/query.pb.go)\n\n```\n// QueryDeploymentsRequest is request type for the Query/Deployments RPC method\ntype QueryDeploymentsRequest struct {\n\tFilters    DeploymentFilters  `protobuf:\"bytes,1,opt,name=filters,proto3\" json:\"filters\"`\n\tPagination *query.PageRequest `protobuf:\"bytes,2,opt,name=pagination,proto3\" json:\"pagination,omitempty\"`\n}\n```","description":null,"slug":"eng-notes/akash-api/exploration-of-akash-queries-using-the-akash-api-source-code"},{"title":"Akash App (app.go Initiation and Blockchain Definitions)","body":"\n## Initiation of App via Main.go Function\n\n> [Source code reference location](https://github.com/akash-network/node/blob/master/cmd/akash/main.go)\n\nThe `main.go` file and associated main function fires the method call of `NewRootCmd`.  This method - as detailed in the subsequent section is located in `node/cmd/akash/cmd/root.go`\n\n```\nfunc main() {\n\trootCmd, _ := cmd.NewRootCmd()\n\n\tif err := cmd.Execute(rootCmd, \"AKASH\"); err != nil {\n\t\tswitch e := err.(type) {\n\t\tcase server.ErrorCode:\n\t\t\tos.Exit(e.Code)\n\t\tdefault:\n\t\t\tos.Exit(1)\n\t\t}\n\t}\n}\n```\n\n## Root Command Registration\n\n> [Source code reference location](https://github.com/akash-network/node/blob/52d5ee5caa2c6e5a5e59893d903d22fe450d6045/cmd/akash/cmd/root.go#L46)\n\nWhen the root command for the Akash CLI - which is the `akash` command prefix registration via Cobra - the `initRootCmd` function is called.\n\nAs detailed in the subsequent section, `initRootCmd` is located in `node/md/akash/cmd/root.go`.\n\n```\nfunc NewRootCmd() (*cobra.Command, params.EncodingConfig) {\n\tencodingConfig := app.MakeEncodingConfig()\n\n\trootCmd := &cobra.Command{\n\t\tUse:               \"akash\",\n\t\tShort:             \"Akash Blockchain Application\",\n\t\tLong:              \"Akash CLI Utility.\\n\\nAkash is a peer-to-peer marketplace for computing resources and \\na deployment platform for heavily distributed applications. \\nFind out more at https://akash.network\",\n\t\tSilenceUsage:      true,\n\t\tPersistentPreRunE: GetPersistentPreRunE(encodingConfig, []string{\"AKASH\"}),\n\t}\n\n\tinitRootCmd(rootCmd, encodingConfig)\n\n\treturn rootCmd, encodingConfig\n}\n```\n\n## Root Command Initiation\n\n> [Source code reference location](https://github.com/akash-network/node/blob/52d5ee5caa2c6e5a5e59893d903d22fe450d6045/cmd/akash/cmd/root.go#L103)\n\nThe `initRootCmd` function calls method `AddCommands` within the Cosmos SDK `server` package.  Per Cosmos documentation:\n\n> The server package is responsible for providing the mechanisms necessary to start an ABCI CometBFT application and provides the CLI framework (based on cobra) necessary to fully bootstrap an application. The package exposes two core functions: StartCmd and ExportCmd which creates commands to start the application and export state respectively.\n\n```\nfunc initRootCmd(rootCmd *cobra.Command, encodingConfig params.EncodingConfig) {\n\t....\n\tserver.AddCommands(rootCmd, app.DefaultHome, newApp, createAppAndExport, addModuleInitFlags)\n\t....\n}\n```\n\n## Cosmos SDK AddCommands Function Detail\n\n> [Source code reference location](https://github.com/cosmos/cosmos-sdk/blob/main/server/util.go)\n\nAmongst other command registrations via the Cosmos SDK server package, note that the `startCmd` is registered via function `StartCmd`.\n\n```\n// add server commands\nfunc AddCommands(rootCmd *cobra.Command, defaultNodeHome string, appCreator types.AppCreator, appExport types.AppExporter, addStartFlags types.ModuleInitFlags) {\n\t....\n\n\tstartCmd := StartCmd(appCreator, defaultNodeHome)\n\taddStartFlags(startCmd)\n\n\trootCmd.AddCommand(\n\t\tstartCmd,\n\t\tcometCmd,\n\t\tExportCmd(appExport, defaultNodeHome),\n\t\tversion.NewVersionCommand(),\n\t\tNewRollbackCmd(appCreator, defaultNodeHome),\n\t)\n}\n```\n\n> [Source code reference location](https://github.com/cosmos/cosmos-sdk/blob/main/server/start.go)\n\nThe `SmartCmd` function registers and allows Akash CLI use of command `akash start`.  This command provokes the initiation of an Akash RPC Node and Akash Validator instances.\n\n```\nfunc StartCmd(appCreator types.AppCreator, defaultNodeHome string) *cobra.Command {\n\tcmd := &cobra.Command{\n\t\tUse:   \"start\",\n\t\tShort: \"Run the full node\",\n\t\tLong: `Run the full node application with CometBFT in or out of process. By\ndefault, the application will run with CometBFT in process.\n```\n\n## New App Initiation\n\n> [Source code reference location](https://github.com/akash-network/node/blob/52d5ee5caa2c6e5a5e59893d903d22fe450d6045/cmd/akash/cmd/root.go#L103)\n\nWhen the Cosmos SDK `AddCommands` method was called with the Akash `initRootCmd` function - included anew below for ease of reference - the `newApp` function is passed in as an argument.\n\n```\nfunc initRootCmd(rootCmd *cobra.Command, encodingConfig params.EncodingConfig) {\n\t....\n\n\tserver.AddCommands(rootCmd, app.DefaultHome, newApp, createAppAndExport, addModuleInitFlags)\n\n\t....\n}\n```\n\nThe `newApp` function calls the `NewApp` method located in `node/app/app.go`.  The `NewApp` method initiates and defines the base parameters of the blockchain.\n\n```\nfunc newApp(logger log.Logger, db dbm.DB, traceStore io.Writer, appOpts servertypes.AppOptions) servertypes.Application {\n\t...\n\n\treturn app.NewApp(\n\t\tlogger, db, traceStore, true, cast.ToUint(appOpts.Get(server.FlagInvCheckPeriod)), skipUpgradeHeights,\n\t\tcast.ToString(appOpts.Get(flags.FlagHome)),\n\t\tappOpts,\n\t\tbaseapp.SetPruning(pruningOpts),\n\t\tbaseapp.SetMinGasPrices(cast.ToString(appOpts.Get(server.FlagMinGasPrices))),\n\t\tbaseapp.SetHaltHeight(cast.ToUint64(appOpts.Get(server.FlagHaltHeight))),\n\t\tbaseapp.SetHaltTime(cast.ToUint64(appOpts.Get(server.FlagHaltTime))),\n\t\tbaseapp.SetMinRetainBlocks(cast.ToUint64(appOpts.Get(server.FlagMinRetainBlocks))),\n\t\tbaseapp.SetInterBlockCache(cache),\n\t\tbaseapp.SetTrace(cast.ToBool(appOpts.Get(server.FlagTrace))),\n\t\tbaseapp.SetIndexEvents(cast.ToStringSlice(appOpts.Get(server.FlagIndexEvents))),\n\t\tbaseapp.SetSnapshotStore(snapshotStore),\n\t\tbaseapp.SetSnapshotInterval(cast.ToUint64(appOpts.Get(server.FlagStateSyncSnapshotInterval))),\n\t\tbaseapp.SetSnapshotKeepRecent(cast.ToUint32(appOpts.Get(server.FlagStateSyncSnapshotKeepRecent))),\n\t)\n}\n```\n\n## Blockchain Definitions Via NewApp\n\n> [Source code reference location](https://github.com/akash-network/node/blob/52d5ee5caa2c6e5a5e59893d903d22fe450d6045/app/app.go#L179)\n\nWhen called the `NewApp` function creates many definitions for the blockchain including:\n\n* Keepers for blockchain store definitions for all modules\n* Blockchain store key values\n\nThe `NewApp` function returns an instance of the `AkashApp` struct.\n\n```\nfunc NewApp(\n\tlogger log.Logger, db dbm.DB, tio io.Writer, loadLatest bool, invCheckPeriod uint, skipUpgradeHeights map[int64]bool,\n\thomePath string, appOpts servertypes.AppOptions, options ...func(*bam.BaseApp),\n) *AkashApp {\n...\n}\n```\n\nThe `AkashApp` struct is defined as:\n\n```\ntype AkashApp struct {\n\t*bam.BaseApp\n\tcdc               *codec.LegacyAmino\n\tappCodec          codec.Codec\n\tinterfaceRegistry codectypes.InterfaceRegistry\n\n\tinvCheckPeriod uint\n\n\tkeys    map[string]*sdk.KVStoreKey\n\ttkeys   map[string]*sdk.TransientStoreKey\n\tmemkeys map[string]*sdk.MemoryStoreKey\n\n\tkeeper struct {\n\t\tacct     authkeeper.AccountKeeper\n\t\tauthz    authzkeeper.Keeper\n\t\tbank     bankkeeper.Keeper\n\t\tcap      *capabilitykeeper.Keeper\n\t\tstaking  stakingkeeper.Keeper\n\t\tslashing slashingkeeper.Keeper\n\t\tmint     mintkeeper.Keeper\n\t\tdistr    distrkeeper.Keeper\n\t\tgov      govkeeper.Keeper\n\t\tcrisis   crisiskeeper.Keeper\n\t\tupgrade  upgradekeeper.Keeper\n\t\tparams   paramskeeper.Keeper\n\t\tibc      *ibckeeper.Keeper\n\t\tevidence evidencekeeper.Keeper\n\t\ttransfer ibctransferkeeper.Keeper\n\n\t\t// make scoped keepers public for test purposes\n\t\tscopedIBCKeeper      capabilitykeeper.ScopedKeeper\n\t\tscopedTransferKeeper capabilitykeeper.ScopedKeeper\n\n\t\t// akash keepers\n\t\tescrow     escrowkeeper.Keeper\n\t\tdeployment dkeeper.IKeeper\n\t\tmarket     mkeeper.IKeeper\n\t\tprovider   pkeeper.IKeeper\n\t\taudit      audit.Keeper\n\t\tcert       cert.Keeper\n\t\tinflation  inflation.Keeper\n\t}\n\n\tmm *module.Manager\n\n\t// simulation manager\n\tsm *module.SimulationManager\n\n\t// module configurator\n\tconfigurator module.Configurator\n}\n```","description":null,"slug":"eng-notes/akash-app/akash-app"},{"title":"Code Conventions","body":"\nThis document outlines a collection of guidelines, style suggestions, and tips for writing code used throughout the Akash project.\n\n## Go Standards and Conventions\n\n### Code Submission Requirements\n\nThe creators and maintainers of the Go programming language have created standards and policies within this [documentation](https://github.com/golang/go/wiki/CodeReviewComments).\n\nAkash contributors should become familiar with all standards in this idiomatic Go guide with particular attention focused on topics such as:\n\n* [Go Fmt ](https://github.com/golang/go/wiki/CodeReviewComments#gofmt)- all code submitted to the Akash code base should have `go fmt` performed prior to submission to ensure proper and conventional formatting\n* [Code Comments](https://github.com/golang/go/wiki/CodeReviewComments#comment-sentences) - all code submitted should be commented throughly and adhere to commenting standards provided in this section\n* [Error Handling](https://github.com/golang/go/wiki/CodeReviewComments#handle-errors) - handle and do not discard errors as described further in this section\n* [Variable Names](https://github.com/golang/go/wiki/CodeReviewComments#variable-names) - const and var naming conventions should follow Go standards as described in this section\n\n## ReactJS and Javascript Conventions\n\n### Code Submission Requirements\n\nWhile the React project does not maintain or suggest standards for components/variables naming and other considerations, we recommend the use of Airbnb style guides for consistency.\n\n* [React/JSX Style Guide](https://github.com/airbnb/javascript/tree/master/react)\n* [JavaScript Style Guide](https://github.com/airbnb/javascript)","description":null,"slug":"eng-notes/akash-code-contributors---policies-and-standards/code-conventions"},{"title":"Contributor Cheatsheet","body":"\nNOT YET COMPLETED","description":null,"slug":"eng-notes/akash-code-contributors---policies-and-standards/contributor-cheatsheet"},{"title":"Getting Started with Akash Contributions","body":"\n\nThis guides provides ideas on how to get started with Akash code base contributions.\n\n## Find Something to Work On\n\nThe following list provides some thoughts on areas prime for contribution:\n\n- [Find Something to Work On](#find-something-to-work-on)\n  - [Documentation Improvement](#documentation-improvement)\n  - [Review and Isolate a Good First Topic](#review-and-isolate-a-good-first-topic)\n    - [Akash Node And Provider Repositories](#akash-node-and-provider-repositories)\n    - [Akash Console Repositories](#akash-console-repositories)\n  - [Become Involved in Akash SIGs](#become-involved-in-akash-sigs)\n    - [SIG Structure](#sig-structure)\n\n### Documentation Improvement\n\nContributing to Akash documentation is a great way to get started.  In the process of documentation contribution the author will benefit from:\n\n* Gain experience in the the code submission/review process\n* Gain insight further into Akash frameworks\n* Solve simple tasks in first submission such as fixing broken links or missing links\n\nAll Akash documentation - both the Akash user guides and engineering docs - are open source and prime for contributions.\n\n### Review and Isolate a Good First Topic\n\nThere are multiple repositories within the Akash code base. Each repository has beginner-friendly issues that are a great place to get started on your contributor journey. &#x20;\n\nThe core Akash team labels issues as  `good first issue` for focuses that don’t need deep Akash knowledge to contribute to. The `good first issue` label also indicates that the Akssh core team have committed to providing extra assistance for new contributors in these focuses.   Additionally the tag of `ready-for-community-dev` earmarks issues ripe for contribution.\n\n#### Akash Node And Provider Repositories\n\nAll issues across these repositories are collapsed within the [Support repository and associated issues](https://github.com/akash-network/support/issues) within that repo.\n\n#### Akash Console Repositories\n\nAll issues created for Akash Console are listed and tracked within the associated repository.  Directly access Akash Console issues [here](https://github.com/akash-network/console/issues).\n\n### Become Involved in Akash SIGs\n\nThe Akash community is organized into SIGs (Special Interest Groups) to improve its workflow. Understanding how to interact with SIGs is an important part of contributing to Akash. Review the list of [SIGs and active Working Groups](https://github.com/akash-network/community#community-groups) for additional guidance on how to get started.\n\n#### SIG Structure\n\nAnyone is welcome to jump into a SIG and begin fixing issues, critique design proposals, and review code. SIGs have regular video meetings which everyone is welcome to attend. Each SIG has an associated Discord channel, meeting notes, and their own documentation that is useful to read and understand.\n\n\n\n\n\n\n","description":null,"slug":"eng-notes/akash-code-contributors---policies-and-standards/getting-started-with-akash-contributions"},{"title":"Akash gRPC Implementation Overview","body":"\n## gRPC Repository Structure\n\nPrior to delving into the gRPC implementation it is useful to review an overview of the repository structure.  In the sections that follow Deployment protobuf messages and services are explored as an example and allows familiarization with the directory structures.\n\nIn recent code updates a new `akash-api` repository was created to isolate protobuf/gRPC definitions.\n\nAkash gRPC API definitions are structured as follows:\n\n- [gRPC Repository Structure](#grpc-repository-structure)\n  - [Protobuf Message Definitions](#protobuf-message-definitions)\n  - [Protobuf Service Definitions](#protobuf-service-definitions)\n\n### Protobuf Message Definitions\n\n> [Source code reference location](https://github.com/akash-network/akash-api/blob/main/proto/node/akash/deployment/v1beta3/deployment.proto)\n\nExample Akash protobuf message definition can be explored in the `akash-api/proto/node/akash/deployment/v1beta3/service.proto` file.\n\n### Protobuf Service Definitions\n\n> [Source code reference location](https://github.com/akash-network/akash-api/blob/main/proto/node/akash/deployment/v1beta3/service.proto)\n\nExample Akash protobuf service definition can be explored in the `akash-api/proto/node/akash/deployment/v1beta3/service.proto` file.","description":null,"slug":"eng-notes/akash-custom-client/akash-grpc-implementation-overview"},{"title":"Deployments CLI","body":"\n## Cobra CLI Command Registration\n\n#### Cobra sets root command of `akash` via `node/cmd/akash/cmd/root.go`\n\n```\nfunc NewRootCmd() (*cobra.Command, params.EncodingConfig) {\n\tencodingConfig := app.MakeEncodingConfig()\n\n\trootCmd := &cobra.Command{\n\t\tUse:               \"akash\",\n\t\tShort:             \"Akash Blockchain Application\",\n\t\tLong:              \"Akash CLI Utility.\\n\\nAkash is a peer-to-peer marketplace for computing resources and \\na deployment platform for heavily distributed applications. \\nFind out more at https://akash.network\",\n\t\tSilenceUsage:      true,\n\t\tPersistentPreRunE: GetPersistentPreRunE(encodingConfig, []string{\"AKASH\"}),\n\t}\n\n\tinitRootCmd(rootCmd, encodingConfig)\n\n\treturn rootCmd, encodingConfig\n}\n```\n\n#### Akash CLI command is registered via Cobra for deployment creation (file location = x/deployment/client/cli/tx.go).\n\n```\nfunc cmdCreate(key string) *cobra.Command {\n\tcmd := &cobra.Command{\n\t\tUse:   \"create [sdl-file]\"\n```\n\n#### Flags - which in Cobra are command switches are added via the following function call. This call is made within the `cmdCreate` function discussed above.\n\nThe flags package used comes from Cosmos SDK at `github.com/cosmos/cosmos-sdk/client/flags` which is called as an import withn the tx.go file.\n\n```\nflags.AddTxFlagsToCmd(cmd)\n```\n\nThe Cosmos SDK function called registers the following flags for items such as:\n\n* Specifying the RPC node used in the command via `FlagNode`. In this example a `FlagNode` const exists which specified the flag should be `node` which allows command CLI command execution such as: `akash deployment create --node <node-address:port>`.\n\n```\nfunc AddTxFlagsToCmd(cmd *cobra.Command) {\n\tf := cmd.Flags()\n\tf.StringP(FlagOutput, \"o\", \"json\", \"Output format (text|json)\")\n\tf.String(FlagFrom, \"\", \"Name or address of private key with which to sign\")\n\tf.Uint64P(FlagAccountNumber, \"a\", 0, \"The account number of the signing account (offline mode only)\")\n\tf.Uint64P(FlagSequence, \"s\", 0, \"The sequence number of the signing account (offline mode only)\")\n\tf.String(FlagNote, \"\", \"Note to add a description to the transaction (previously --memo)\")\n\tf.String(FlagFees, \"\", \"Fees to pay along with transaction; eg: 10uatom\")\n\tf.String(FlagGasPrices, \"\", \"Gas prices in decimal format to determine the transaction fee (e.g. 0.1uatom)\")\n\tf.String(FlagNode, \"tcp://localhost:26657\", \"<host>:<port> to tendermint rpc interface for this chain\")\n\tf.Bool(FlagUseLedger, false, \"Use a connected Ledger device\")\n\tf.Float64(FlagGasAdjustment, DefaultGasAdjustment, \"adjustment factor to be multiplied against the estimate returned by the tx simulation; if the gas limit is set manually this flag is ignored \")\n\tf.StringP(FlagBroadcastMode, \"b\", BroadcastSync, \"Transaction broadcasting mode (sync|async)\")\n\tf.Bool(FlagDryRun, false, \"ignore the --gas flag and perform a simulation of a transaction, but don't broadcast it (when enabled, the local Keybase is not accessible)\")\n\tf.Bool(FlagGenerateOnly, false, \"Build an unsigned transaction and write it to STDOUT (when enabled, the local Keybase only accessed when providing a key name)\")\n\tf.Bool(FlagOffline, false, \"Offline mode (does not allow any online functionality)\")\n\tf.BoolP(FlagSkipConfirmation, \"y\", false, \"Skip tx broadcasting prompt confirmation\")\n\tf.String(FlagSignMode, \"\", \"Choose sign mode (direct|amino-json|direct-aux), this is an advanced feature\")\n\tf.Uint64(FlagTimeoutHeight, 0, \"Set a block timeout height to prevent the tx from being committed past a certain height\")\n\tf.String(FlagFeePayer, \"\", \"Fee payer pays fees for the transaction instead of deducting from the signer\")\n\tf.String(FlagFeeGranter, \"\", \"Fee granter grants fees for the transaction\")\n\tf.String(FlagTip, \"\", \"Tip is the amount that is going to be transferred to the fee payer on the target chain. This flag is only valid when used with --aux, and is ignored if the target chain didn't enable the TipDecorator\")\n\tf.Bool(FlagAux, false, \"Generate aux signer data instead of sending a tx\")\n\tf.String(FlagChainID, \"\", \"The network chain ID\")\n```","description":null,"slug":"eng-notes/akash-custom-client/deployments-cli"},{"title":"Deployments","body":"\n## Deployments Tendermint RPC Endpoint Overview\n\nClient communication for Akash deployment creation, updates, and deletions occurs via the Tendermint RPC implementation.\n\nFurther details on the Tendermint RPC implementation can be found [here](https://docs.cosmos.network/main/core/grpc\\_rest#tendermint-rpc).\n\nThe source code review in this section primarily focus on the module directory of `node/x/deployment`.\n\n## Deployments RPC Endpoint Route Registration\n\n> [Source code reference location](https://github.com/akash-network/node/blob/52d5ee5caa2c6e5a5e59893d903d22fe450d6045/x/deployment/module.go#L138)\n\n```\nfunc (am AppModule) Route() sdk.Route {\n\treturn sdk.NewRoute(types.RouterKey, handler.NewHandler(am.keeper, am.mkeeper, am.ekeeper, am.authzKeeper))\n}\n```\n\n## Deployments RPC Endpoint Handler\n\n> [Source code reference location](https://github.com/akash-network/node/blob/52d5ee5caa2c6e5a5e59893d903d22fe450d6045/x/deployment/handler/handler.go#L12)\n\nThe Deployments handler matches a case based on the types defined via protobuf and contained within the `node//x/deployment/types/v1beta2/deploymentmsg.pb.go` file.  An example of the protobuf file is covered in the subsequent section.\n\n```\nfunc NewHandler(keeper keeper.IKeeper, mkeeper MarketKeeper, ekeeper EscrowKeeper, authzKeeper AuthzKeeper) sdk.Handler {\n\tms := NewServer(keeper, mkeeper, ekeeper, authzKeeper)\n\n\treturn func(ctx sdk.Context, msg sdk.Msg) (*sdk.Result, error) {\n\t\tswitch msg := msg.(type) {\n\t\tcase *types.MsgCreateDeployment:\n\t\t\tres, err := ms.CreateDeployment(sdk.WrapSDKContext(ctx), msg)\n\t\t\treturn sdk.WrapServiceResult(ctx, res, err)\n\n\t\tcase *types.MsgDepositDeployment:\n\t\t\tres, err := ms.DepositDeployment(sdk.WrapSDKContext(ctx), msg)\n\t\t\treturn sdk.WrapServiceResult(ctx, res, err)\n\n\t\tcase *types.MsgUpdateDeployment:\n\t\t\tres, err := ms.UpdateDeployment(sdk.WrapSDKContext(ctx), msg)\n\t\t\treturn sdk.WrapServiceResult(ctx, res, err)\n\n\t\tcase *types.MsgCloseDeployment:\n\t\t\tres, err := ms.CloseDeployment(sdk.WrapSDKContext(ctx), msg)\n\t\t\treturn sdk.WrapServiceResult(ctx, res, err)\n\n\t\tcase *types.MsgCloseGroup:\n\t\t\tres, err := ms.CloseGroup(sdk.WrapSDKContext(ctx), msg)\n\t\t\treturn sdk.WrapServiceResult(ctx, res, err)\n\n\t\tcase *types.MsgPauseGroup:\n\t\t\tres, err := ms.PauseGroup(sdk.WrapSDKContext(ctx), msg)\n\t\t\treturn sdk.WrapServiceResult(ctx, res, err)\n\n\t\tcase *types.MsgStartGroup:\n\t\t\tres, err := ms.StartGroup(sdk.WrapSDKContext(ctx), msg)\n\t\t\treturn sdk.WrapServiceResult(ctx, res, err)\n\n\t\tdefault:\n\t\t\treturn nil, sdkerrors.ErrUnknownRequest\n\t\t}\n\t}\n}\n```\n\n## Example Msg Type - Create Deployment\n\n> [Source code reference location](https://github.com/akash-network/node/blob/52d5ee5caa2c6e5a5e59893d903d22fe450d6045/x/deployment/types/v1beta2/deploymentmsg.pb.go#L28)\n\n```\ntype MsgCreateDeployment struct {\n\tID      DeploymentID `protobuf:\"bytes,1,opt,name=id,proto3\" json:\"id\" yaml:\"id\"`\n\tGroups  []GroupSpec  `protobuf:\"bytes,2,rep,name=groups,proto3\" json:\"groups\" yaml:\"groups\"`\n\tVersion []byte       `protobuf:\"bytes,3,opt,name=version,proto3\" json:\"version\" yaml:\"version\"`\n\tDeposit types.Coin   `protobuf:\"bytes,4,opt,name=deposit,proto3\" json:\"deposit\" yaml:\"deposit\"`\n\t// Depositor pays for the deposit\n\tDepositor string `protobuf:\"bytes,5,opt,name=depositor,proto3\" json:\"depositor\" yaml:\"depositor\"`\n}\n```\n\n## Create Deployment Keeper Initiation\n\nIn a prior section the Handler matching of the message `MsgCreateDeployment` was introduced.  Including the `NewHandler` message correlation logic again below for clarity.\n\n```\nfunc NewHandler(keeper keeper.IKeeper, mkeeper MarketKeeper, ekeeper EscrowKeeper, authzKeeper AuthzKeeper) sdk.Handler {\n\tms := NewServer(keeper, mkeeper, ekeeper, authzKeeper)\n\n\treturn func(ctx sdk.Context, msg sdk.Msg) (*sdk.Result, error) {\n\t\tswitch msg := msg.(type) {\n\t\tcase *types.MsgCreateDeployment:\n\t\t\tres, err := ms.CreateDeployment(sdk.WrapSDKContext(ctx), msg)\n\t\t\treturn sdk.WrapServiceResult(ctx, res, err)\n```\n\nWhen the `MsgCreateDeployment` is received and correlated in the Handler, the `CreateDeployment` method is called.  This method is found in `node/x/deployment/handler/server.go`.\n\nSeveral validations are performed - such as `GetDeployment` to ensure that the deployment does not already exist, assurance that the minimum deposit is fulfilled, etc - and eventually the following methods are called for deployment, order, and escrow blockchain entries.\n\n* ms.deployment.Create\n* ms.market.CreateOrder\n* ms.escrow.AccountCreate\n\nThe methods called - and as reviewed in the subsequent section - call their respective Keepers for blockchain store population.\n\n```\nfunc (ms msgServer) CreateDeployment(goCtx context.Context, msg *types.MsgCreateDeployment) (*types.MsgCreateDeploymentResponse, error) {\n\tctx := sdk.UnwrapSDKContext(goCtx)\n\n\tif _, found := ms.deployment.GetDeployment(ctx, msg.ID); found {\n\t\treturn nil, types.ErrDeploymentExists\n\t}\n\n\tminDeposit := ms.deployment.GetParams(ctx).DeploymentMinDeposit\n\n\tif minDeposit.Denom != msg.Deposit.Denom {\n\t\treturn nil, errors.Wrapf(types.ErrInvalidDeposit, \"mininum:%v received:%v\", minDeposit, msg.Deposit)\n\t}\n\tif minDeposit.Amount.GT(msg.Deposit.Amount) {\n\t\treturn nil, errors.Wrapf(types.ErrInvalidDeposit, \"mininum:%v received:%v\", minDeposit, msg.Deposit)\n\t}\n\n\tdeployment := types.Deployment{\n\t\tDeploymentID: msg.ID,\n\t\tState:        types.DeploymentActive,\n\t\tVersion:      msg.Version,\n\t\tCreatedAt:    ctx.BlockHeight(),\n\t}\n\n\tif err := types.ValidateDeploymentGroups(msg.Groups); err != nil {\n\t\treturn nil, errors.Wrap(types.ErrInvalidGroups, err.Error())\n\t}\n\n\towner, err := sdk.AccAddressFromBech32(msg.ID.Owner)\n\tif err != nil {\n\t\treturn &types.MsgCreateDeploymentResponse{}, err\n\t}\n\n\tdepositor, err := sdk.AccAddressFromBech32(msg.Depositor)\n\tif err != nil {\n\t\treturn &types.MsgCreateDeploymentResponse{}, err\n\t}\n\n\tif err = ms.authorizeDeposit(ctx, owner, depositor, msg.Deposit); err != nil {\n\t\treturn nil, err\n\t}\n\n\tgroups := make([]types.Group, 0, len(msg.Groups))\n\n\tfor idx, spec := range msg.Groups {\n\t\tgroups = append(groups, types.Group{\n\t\t\tGroupID:   types.MakeGroupID(deployment.ID(), uint32(idx+1)),\n\t\t\tState:     types.GroupOpen,\n\t\t\tGroupSpec: spec,\n\t\t\tCreatedAt: ctx.BlockHeight(),\n\t\t})\n\t}\n\n\tif err := ms.deployment.Create(ctx, deployment, groups); err != nil {\n\t\treturn nil, errors.Wrap(types.ErrInternal, err.Error())\n\t}\n\n\t// create orders\n\tfor _, group := range groups {\n\t\tif _, err := ms.market.CreateOrder(ctx, group.ID(), group.GroupSpec); err != nil {\n\t\t\treturn &types.MsgCreateDeploymentResponse{}, err\n\t\t}\n\t}\n\n\tif err := ms.escrow.AccountCreate(ctx,\n\t\ttypes.EscrowAccountForDeployment(deployment.ID()),\n\t\towner,\n\t\tdepositor,\n\t\tmsg.Deposit,\n\t); err != nil {\n\t\treturn &types.MsgCreateDeploymentResponse{}, err\n\t}\n\n\treturn &types.MsgCreateDeploymentResponse{}, nil\n}\n```\n\n## Deployments Keeper Processing for Store/Chain Population\n\n[Source code reference location](https://github.com/akash-network/node/blob/52d5ee5caa2c6e5a5e59893d903d22fe450d6045/x/deployment/keeper/keeper.go#L123)\n\nThe `Create` method takes in Deployment details as an argument and writes the new Deployment to the blockchain.\n\n```\nfunc (k Keeper) Create(ctx sdk.Context, deployment types.Deployment, groups []types.Group) error {\n\tstore := ctx.KVStore(k.skey)\n\n\tkey := deploymentKey(deployment.ID())\n\n\tif store.Has(key) {\n\t\treturn types.ErrDeploymentExists\n\t}\n\n\tstore.Set(key, k.cdc.MustMarshal(&deployment))\n\n\tfor idx := range groups {\n\t\tgroup := groups[idx]\n\n\t\tif !group.ID().DeploymentID().Equals(deployment.ID()) {\n\t\t\treturn types.ErrInvalidGroupID\n\t\t}\n\t\tgkey := groupKey(group.ID())\n\t\tstore.Set(gkey, k.cdc.MustMarshal(&group))\n\t}\n\n\tctx.EventManager().EmitEvent(\n\t\ttypes.NewEventDeploymentCreated(deployment.ID(), deployment.Version).\n\t\t\tToSDKEvent(),\n\t)\n\n\ttelemetry.IncrCounter(1.0, \"akash.deployment_created\")\n\n\treturn nil\n}\n```\n\n## Deployments API Amino Definitions\n\n> [Source code reference location](https://github.com/akash-network/node/blob/9c376e978213fba72e1023b829d780f1f4ce64e5/x/deployment/types/v1beta2/codec.go)\n\nBased on the use of the Tendermint RPC implementation and the associated amino client encoding standards the Deployment messages are registered with the Amino Codec.\n\n```\nfunc init() {\n\tRegisterLegacyAminoCodec(amino)\n\tcryptocodec.RegisterCrypto(amino)\n\tamino.Seal()\n}\n\n// RegisterLegacyAminoCodec register concrete types on codec\nfunc RegisterLegacyAminoCodec(cdc *codec.LegacyAmino) {\n\tcdc.RegisterConcrete(&MsgCreateDeployment{}, ModuleName+\"/\"+MsgTypeCreateDeployment, nil)\n\tcdc.RegisterConcrete(&MsgUpdateDeployment{}, ModuleName+\"/\"+MsgTypeUpdateDeployment, nil)\n\tcdc.RegisterConcrete(&MsgDepositDeployment{}, ModuleName+\"/\"+MsgTypeDepositDeployment, nil)\n\tcdc.RegisterConcrete(&MsgCloseDeployment{}, ModuleName+\"/\"+MsgTypeCloseDeployment, nil)\n\tcdc.RegisterConcrete(&MsgCloseGroup{}, ModuleName+\"/\"+MsgTypeCloseGroup, nil)\n\tcdc.RegisterConcrete(&MsgPauseGroup{}, ModuleName+\"/\"+MsgTypePauseGroup, nil)\n\tcdc.RegisterConcrete(&MsgStartGroup{}, ModuleName+\"/\"+MsgTypeStartGroup, nil)\n}\n```\n\nThe Messages types are defined in Protobuf file `node/proto/akash/deployment/v1beta1/deployment.proto`.  The `MsgCreateDeployment` Protobuf file is referenced below as an example.\n\n```\n// MsgCreateDeployment defines an SDK message for creating deployment\nmessage MsgCreateDeployment {\n  option (gogoproto.equal) = false;\n\n  DeploymentID id = 1 [\n    (gogoproto.nullable)   = false,\n    (gogoproto.customname) = \"ID\",\n    (gogoproto.jsontag)    = \"id\",\n    (gogoproto.moretags)   = \"yaml:\\\"id\\\"\"\n  ];\n  repeated GroupSpec groups = 2\n      [(gogoproto.nullable) = false, (gogoproto.jsontag) = \"groups\", (gogoproto.moretags) = \"yaml:\\\"groups\\\"\"];\n  bytes version = 3 [(gogoproto.jsontag) = \"version\", (gogoproto.moretags) = \"yaml:\\\"version\\\"\"];\n\n  cosmos.base.v1beta1.Coin deposit = 4\n      [(gogoproto.nullable) = false, (gogoproto.jsontag) = \"deposit\", (gogoproto.moretags) = \"yaml:\\\"deposit\\\"\"];\n}\n```","description":null,"slug":"eng-notes/akash-custom-client/deployments"},{"title":"Lease","body":"\n## Lease Tendermint RPC Endpoint Overview\n\nClient communication for Akash lease creation, updates, and deletions occurs via the Tendermint RPC implementation.\n\nFurther details on the Tendermint RPC implementation can be found [here](https://docs.cosmos.network/main/core/grpc\\_rest#tendermint-rpc).\n\nThe source code review in this section primarily focus on the module directory of `node/x/market`.\n\n### Lease RPC Endpoint Route Registration\n\n> [Source code reference location](https://github.com/akash-network/node/blob/master/x/market/module.go)\n\nThe RPC Endpoint for Deployments - allowing inbound communication for CRUD operations - is found in `node/x/market/module.go`.\n\nWhen encountered this Route calls the `NewHandler` method for further message handling.\n\n```\nfunc (am AppModule) Route() sdk.Route {\n\treturn sdk.NewRoute(types.RouterKey, handler.NewHandler(am.keepers))\n}\n```\n\n## Lease RPC Endpoint Handler\n\n> [Source code reference location](https://github.com/akash-network/node/blob/master/x/market/handler/handler.go)\n\nThe Lease handler matches a case based on the types defined via protobuf and contained within the `node/x/market/types/v1beta2/lease.pb.go` file.  An example of the protobuf file is covered in the subsequent section.\n\n```\nfunc NewHandler(keepers Keepers) sdk.Handler {\n\tms := NewServer(keepers)\n\n\treturn func(ctx sdk.Context, msg sdk.Msg) (*sdk.Result, error) {\n\t\tswitch msg := msg.(type) {\n\t\tcase *types.MsgCreateBid:\n\t\t\tres, err := ms.CreateBid(sdk.WrapSDKContext(ctx), msg)\n\t\t\treturn sdk.WrapServiceResult(ctx, res, err)\n\n\t\tcase *types.MsgCloseBid:\n\t\t\tres, err := ms.CloseBid(sdk.WrapSDKContext(ctx), msg)\n\t\t\treturn sdk.WrapServiceResult(ctx, res, err)\n\n\t\tcase *types.MsgWithdrawLease:\n\t\t\tres, err := ms.WithdrawLease(sdk.WrapSDKContext(ctx), msg)\n\t\t\treturn sdk.WrapServiceResult(ctx, res, err)\n\n\t\tcase *types.MsgCreateLease:\n\t\t\tres, err := ms.CreateLease(sdk.WrapSDKContext(ctx), msg)\n\t\t\treturn sdk.WrapServiceResult(ctx, res, err)\n\n\t\tcase *types.MsgCloseLease:\n\t\t\tres, err := ms.CloseLease(sdk.WrapSDKContext(ctx), msg)\n\t\t\treturn sdk.WrapServiceResult(ctx, res, err)\n\n\t\tdefault:\n\t\t\treturn nil, sdkerrors.ErrUnknownRequest\n\t\t}\n\t}\n}\n```\n\n## Example Msg Type - Create Lease\n\n> [Source code reference location](https://github.com/akash-network/node/blob/52d5ee5caa2c6e5a5e59893d903d22fe450d6045/x/market/types/v1beta2/lease.pb.go#L301)\n\n```\ntype MsgCreateLease struct {\n\tBidID BidID `protobuf:\"bytes,1,opt,name=bid_id,json=bidId,proto3\" json:\"id\" yaml:\"id\"`\n}\n```\n\n## Create Lease Keeper Initiation\n\nIn a prior section the Handler matching of the message `MsgCreateLease` was introduced.  Including the `NewHandler` message correlation logic anew below for clarity.\n\n```\nfunc NewHandler(keepers Keepers) sdk.Handler {\n\tms := NewServer(keepers)\n\n\treturn func(ctx sdk.Context, msg sdk.Msg) (*sdk.Result, error) {\n\t\tswitch msg := msg.(type) {\n\t\t...\n\n\t\tcase *types.MsgCreateLease:\n\t\t\tres, err := ms.CreateLease(sdk.WrapSDKContext(ctx), msg)\n\t\t\treturn sdk.WrapServiceResult(ctx, res, err)\n\t\t...\n\t}\n}\n```\n\nWhen the `MsgCreateLease` is received and correlated in the Handler, the `CreateLease` method is called.  This method is found in `node/x/market/handler/server.go`.\n\nSeveral validations are performed - such as `GetBid` to ensure that the associated bid is found, `BidOpen` to ensure that the bid is open, etc - and eventually the following methods are called for lease creation and associated blockchain entries.\n\n* ms.keepers.Market.CreateLease\n* ms.keepers.Market.OnOrderMatched\n* ms.keepers.Market.OnBidMatched\n\nAdditionally all lost bids are closed via:\n\n* ms.keepers.Market.OnBidLost\n\nThe methods called - and as reviewed in the subsequent section - call their respective Keepers for blockchain store population.\n\n```\nfunc (ms msgServer) CreateLease(goCtx context.Context, msg *types.MsgCreateLease) (*types.MsgCreateLeaseResponse, error) {\n\tctx := sdk.UnwrapSDKContext(goCtx)\n\n\tbid, found := ms.keepers.Market.GetBid(ctx, msg.BidID)\n\tif !found {\n\t\treturn &types.MsgCreateLeaseResponse{}, types.ErrBidNotFound\n\t}\n\n\tif bid.State != types.BidOpen {\n\t\treturn &types.MsgCreateLeaseResponse{}, types.ErrBidNotOpen\n\t}\n\n\torder, found := ms.keepers.Market.GetOrder(ctx, msg.BidID.OrderID())\n\tif !found {\n\t\treturn &types.MsgCreateLeaseResponse{}, types.ErrOrderNotFound\n\t}\n\n\tif order.State != types.OrderOpen {\n\t\treturn &types.MsgCreateLeaseResponse{}, types.ErrOrderNotOpen\n\t}\n\n\tgroup, found := ms.keepers.Deployment.GetGroup(ctx, order.ID().GroupID())\n\tif !found {\n\t\treturn &types.MsgCreateLeaseResponse{}, types.ErrGroupNotFound\n\t}\n\n\tif group.State != dtypes.GroupOpen {\n\t\treturn &types.MsgCreateLeaseResponse{}, types.ErrGroupNotOpen\n\t}\n\n\towner, err := sdk.AccAddressFromBech32(msg.BidID.Provider)\n\tif err != nil {\n\t\treturn &types.MsgCreateLeaseResponse{}, err\n\t}\n\n\tif err := ms.keepers.Escrow.PaymentCreate(ctx,\n\t\tdtypes.EscrowAccountForDeployment(msg.BidID.DeploymentID()),\n\t\ttypes.EscrowPaymentForLease(msg.BidID.LeaseID()),\n\t\towner,\n\t\tbid.Price); err != nil {\n\t\treturn &types.MsgCreateLeaseResponse{}, err\n\t}\n\n\tms.keepers.Market.CreateLease(ctx, bid)\n\tms.keepers.Market.OnOrderMatched(ctx, order)\n\tms.keepers.Market.OnBidMatched(ctx, bid)\n\n\t// close losing bids\n\tvar lostbids []types.Bid\n\tms.keepers.Market.WithBidsForOrder(ctx, msg.BidID.OrderID(), func(bid types.Bid) bool {\n\t\tif bid.ID().Equals(msg.BidID) {\n\t\t\treturn false\n\t\t}\n\t\tif bid.State != types.BidOpen {\n\t\t\treturn false\n\t\t}\n\n\t\tlostbids = append(lostbids, bid)\n\t\treturn false\n\t})\n\n\tfor _, bid := range lostbids {\n\t\tms.keepers.Market.OnBidLost(ctx, bid)\n\t\tif err := ms.keepers.Escrow.AccountClose(ctx,\n\t\t\ttypes.EscrowAccountForBid(bid.ID())); err != nil {\n\t\t\treturn &types.MsgCreateLeaseResponse{}, err\n\t\t}\n\t}\n\n\treturn &types.MsgCreateLeaseResponse{}, nil\n}\n```\n\n## Create Lease Keeper Processing for Store/Chain Population\n\n[Source code reference location](https://github.com/akash-network/node/blob/52d5ee5caa2c6e5a5e59893d903d22fe450d6045/x/market/keeper/keeper.go#L144)\n\nThe `CreateLease` method within `node/x/market/keeper/keeper.go` takes in Bid details as an argument and writes the new Lease to the blockchain.\n\n```\nfunc (k Keeper) CreateLease(ctx sdk.Context, bid types.Bid) {\n\tstore := ctx.KVStore(k.skey)\n\n\tlease := types.Lease{\n\t\tLeaseID:   types.LeaseID(bid.ID()),\n\t\tState:     types.LeaseActive,\n\t\tPrice:     bid.Price,\n\t\tCreatedAt: ctx.BlockHeight(),\n\t}\n\n\t// create (active) lease in store\n\tkey := keys.LeaseKey(lease.ID())\n\tstore.Set(key, k.cdc.MustMarshal(&lease))\n\n\tctx.Logger().Info(\"created lease\", \"lease\", lease.ID())\n\tctx.EventManager().EmitEvent(\n\t\ttypes.NewEventLeaseCreated(lease.ID(), lease.Price).\n\t\t\tToSDKEvent(),\n\t)\n\n\tsecondaryKeys := keys.SecondaryKeysForLease(lease.ID())\n\tfor _, secondaryKey := range secondaryKeys {\n\t\tstore.Set(secondaryKey, key)\n\t}\n}\n```\n\n## Deployments API Amino Definitions\n\n> [Source code reference location](https://github.com/akash-network/node/blob/52d5ee5caa2c6e5a5e59893d903d22fe450d6045/x/market/types/v1beta2/codec.go)\n\nBased on the use of the Tendermint RPC implementation and the associated amino client encoding standards the Lease messages are registered with the Amino Codec.\n\n```\nfunc init() {\n\tRegisterLegacyAminoCodec(amino)\n\tcryptocodec.RegisterCrypto(amino)\n\tamino.Seal()\n}\n\n// RegisterCodec registers the necessary x/market interfaces and concrete types\n// on the provided Amino codec. These types are used for Amino JSON serialization.\nfunc RegisterLegacyAminoCodec(cdc *codec.LegacyAmino) {\n\tcdc.RegisterConcrete(&MsgCreateBid{}, ModuleName+\"/\"+MsgTypeCreateBid, nil)\n\tcdc.RegisterConcrete(&MsgCloseBid{}, ModuleName+\"/\"+MsgTypeCloseBid, nil)\n\tcdc.RegisterConcrete(&MsgCreateLease{}, ModuleName+\"/\"+MsgTypeCreateLease, nil)\n\tcdc.RegisterConcrete(&MsgWithdrawLease{}, ModuleName+\"/\"+MsgTypeWithdrawLease, nil)\n\tcdc.RegisterConcrete(&MsgCloseLease{}, ModuleName+\"/\"+MsgTypeCloseLease, nil)\n}\n```\n\nThe Messages types are defined in Protobuf file `node/proto/akash/market/v1beta2/lease.proto`.  The `MsgCreateLease` Protobuf file is referenced below as an example.\n\n```\n// MsgCreateLease is sent to create a lease\nmessage MsgCreateLease {\n  option (gogoproto.equal) = false;\n\n  BidID bid_id = 1 [\n    (gogoproto.customname) = \"BidID\",\n    (gogoproto.nullable)   = false,\n    (gogoproto.jsontag)    = \"id\",\n    (gogoproto.moretags)   = \"yaml:\\\"id\\\"\"\n  ];\n}\n\n// MsgCreateLeaseResponse is the response from creating a lease\nmessage MsgCreateLeaseResponse {}\n```","description":null,"slug":"eng-notes/akash-custom-client/lease"},{"title":"Complete Message Construction","body":"\n\n\nIn this section of our code in `main.go` we complete the remaining values need in a message type defined in the Akash source code of `MsgCreateDeployment`.\n\nThe `MsgCreateDeployment` struct is defined within:\n\n[github.com/akash-network/node/x/deployment/types/v1beta2/deploymentmsg.pb.go](https://github.com/akash-network/node/blob/52d5ee5caa2c6e5a5e59893d903d22fe450d6045/x/deployment/types/v1beta2/deploymentmsg.pb.go#L28)\n\n## Code Review\n\nMost of this code is self-explanatory. For example the `Version` (Akash code) and `ParseCoinNormalized` (CosmosSDK) define the hash version of the SDL and convert an AKT string deposit declaration for the deployment.\n\nSending the SDL read earier from file and stored as `sdlManifest` is sent to the Akash `DeploymentGroups` method to eventually store in the `GroupSpec` struct that capture individual profiles defined and required in the manifest.  A slice of such `GroupSpec` structs is stored in the `MsgCreateDeployment` struct's `Groups` field.\n\n```\n...truncated...\nversion, err := sdl.Version(sdlManifest)\nif err != nil {\n\tfmt.Println(\"Error from Version: \", err)\n}\n\ndeposit := \"5000000uakt\"\ndepositCoin, err := sdk.ParseCoinNormalized(deposit)\nif err != nil {\n\tfmt.Println(\"Error ParseCoinNormalized: \", err)\n}\n\ngroups, err := sdlManifest.DeploymentGroups()\nif err != nil {\n\tfmt.Println(\"Error: \", err)\n}\n\nmsg := &v1beta2.MsgCreateDeployment{\n\tID:      id,\n\tVersion: version,\n\tGroups:  make([]v1beta2.GroupSpec, 0, len(groups)),\n\tDeposit: depositCoin,\n\t// Depositor: depositorAcc,\n\tDepositor: id.Owner,\n}\n\nfor _, group := range groups {\n\tmsg.Groups = append(msg.Groups, *group)\n}\n\nif err := msg.ValidateBasic(); err != nil {\n\tfmt.Println(\"Error from ValidateBasic: \", err)\n}\n...truncated...\n```","description":null,"slug":"eng-notes/akash-custom-clients/akash-client---create-transactions/akash-message-creation/complete-message-construction"},{"title":"Deployment ID Message Population","body":"\n\n> [Source code reference location](https://github.com/chainzero/akash-client/blob/main/akashrpcclient\\_withtx/main.go)\n\nFor the purpose of eventually sending such info in the message - in this step we declare a `DeploymentID` struct and then populate the fields of:\n\n* _**Owner**_ - account of the deployer\n* _**DSeq**_ - the deployment ID which we will set to the current block height of the Akash blockchain\n\n## Code Review\n\nAs we continue message construction a variable with the name of `id` is created which is of type `DeploymentID`.  The type is defined in Deployment module and in the following Protobuf file:\n\n[github.com/akash-network/node/x/deployment/types/v1beta2/deployment.pb](https://github.com/akash-network/node/blob/52d5ee5caa2c6e5a5e59893d903d22fe450d6045/x/deployment/types/v1beta2/deployment.pb.go#L59)\n\nA couple of Cosmos SDK functions are called to perform the following objectives:\n\n* _**GetFromBech32 -**_ take the human readable bech32 address and convert it to a slice of bytes.  Within the called logic a sanity check of the account prefix - which is `akash` in this chain - is conducted.\n* _**AccAddress**_ - take the slice of bytes returned from previous function and convert it to string.  This does not occur via a simple string conversion but rather calls the `String` method defined in our source code > utils directory/package that parses/formats the slice of bytes appropriately and makes several validations of address format.\n\nThe returned data is stored as the `DeploymentID` struct `Owner` field.\n\nAdditionally we use a simple API request in the `utils` package and via the `BlockHeight` function to get the current block height of the chain and store it as the `DeploymentID` struct's `DSeq` field.  The returned value - which is an integer - is converted to string which the struct expects.\n\n```\n....truncated....\nid := v1beta2.DeploymentID{}\n\n// Replace the Akash address below with the address that should be used for deployment creation\nbech32Address := \"akash1w3k6qpr4uz44py4z68chfrl7ltpxwtkngnc6xk\"\n\nownerBytes, err := sdk.GetFromBech32(bech32Address, accountPrefix)\nif err != nil {\n\tfmt.Println(\"Error from GetFromBech32: \", err)\n}\n\naccOwnerBytes := sdk.AccAddress(ownerBytes)\n\nid.Owner = utils.String(accOwnerBytes)\n\nid.DSeq, err = strconv.ParseUint(utils.BlockHeight(), 10, 64)\nif err != nil {\n\tfmt.Println(\"Error: \", err)\n}\n....truncated....\n```","description":null,"slug":"eng-notes/akash-custom-clients/akash-client---create-transactions/akash-message-creation/deployment-id-message-population"},{"title":"Message Creation Exploration","body":"\n## Akash Source Code Use\n\n> [Source code reference location](https://github.com/akash-network/node/blob/master/x/deployment/client/cli/tx.go)\n\nFor the purpose of Deployment Creation message construction it is useful to reverse engineer Akash CLI source code within the Deployment module.&#x20;\n\nBased on Go client usage there are several adjustments that will be necessary and we will bring relevant code into our own local copy to make those adjustments.\n\nThis source code is location in the Deployments module at - `github.com/akash-network/node/x/deployment/client/cli/tx.go`.\n\nWe explore - and use - many of the techniques the Akash CLI source code for ease of exploration in other areas of interest by the developer in future use cases.  In other words - if we become familiar with message construction using the source code example for Deployment Creation transactions - we can easily use a similar review for the construction of a Lease Create transaction as an example.\n\n> _**NOTE**_ - for brevity and focus there are elements in Akash source code that we will not use in the construction of our custom client.  Example - in the source code a validation that a valid certificate for Akash provider interactions is verified.  In our focus solely on the Deployment Create transaction this validation is not necessary.  Certainly such validations would be wise in a production grade client but in exploration we want to be entirely focused on the specific matter of interest.\n\n## Akash Source Code Specific Location\n\nThe specific location of the Akash source code is within the `cmdCreate` function.  As the Akash CLI invokes this logic with the registered Cobra command such logic is called when the CLI command - `provider-services tx deployment create` is called.\n\n> `func cmdCreate(key string) *cobra.Command`","description":null,"slug":"eng-notes/akash-custom-clients/akash-client---create-transactions/akash-message-creation/message-creation-exploration"},{"title":"Onward to Transaction Create and Broadcast","body":"\n## Summary/Status\n\nWith the Akash `message` readied we can now process with the section where the Akash transaction will be created, signed, and broadcast.\n\nThe message - of type `MsgCreateDeployment` - will be sent to the blockchain within the transaction we will create.","description":null,"slug":"eng-notes/akash-custom-clients/akash-client---create-transactions/akash-message-creation/onward-to-transaction-create-and-broadcast"},{"title":"Overview","body":"\n\n> [Source code reference location](https://github.com/chainzero/akash-client/tree/main/akashrpcclient\\_withtx)\n\n\n\nWhen an Akash transaction is created and broadcast to the network it must contain a message that the RPC node which receives the communication recognizes and is capable of routing to the correct module.\n\nIn this section we explore an example of message creation for the Create Deployment transaction type. &#x20;\n\nFollowing the formation of the Create Deployment message, we will then proceed into the mechanics of broadcasting that message - stored within the transaction - to the network.\n","description":null,"slug":"eng-notes/akash-custom-clients/akash-client---create-transactions/akash-message-creation/overview"},{"title":"Read SDL from File","body":"\n\n> [Source code reference location](https://github.com/chainzero/akash-client/blob/main/akashrpcclient\\_withtx/main.go)\n\nThe Create Deployment message construction begins with the reading of SDL (Stack Definition Language) manifest from file.\n\n## Relevant Code\n\nBased on the purpose and utility of the client/integration the SDL may instead be prompted from the user but in this example our code expects the manifest to live within the current working directory and the `testsdl` subdirectory. &#x20;\n\nThe locale of the SDL could obviously be updated easily - I.e. use a different subdirectory - easily.\n\n```\n...TRUNCATED...\n// Start of Msg Create logic\n// Logic derived from code initiated via: https://github.com/akash-network/node/blob/52d5ee5caa2c6e5a5e59893d903d22fe450d6045/x/deployment/client/cli/tx.go#L83\n\nsdlLocation := (\"./testsdl/deploy.yml\")\naccountPrefix := \"akash\"\n\nsdlManifest, err := sdl.ReadFile(sdlLocation)\nif err != nil {\n\tfmt.Println(err)\n}\n...TRUNCATED...\n```\n\n## ReadFile Method - Additional Detail\n\nThe `ReadFile` method is called from Akash source code and from within the following path/file.\n\n> [github.com/akash-network/node/sdl/sdl.go](https://github.com/akash-network/node/blob/master/sdl/sdl.go)\n\n_**ReadFile Method**_\n\nAs seen in the Akash source code capture from this file the SDL is read via a simple os.ReadFile function call and then passed into the `Read` function as a slice of bytes.\n\n```\n// ReadFile read from given path and returns SDL instance\nfunc ReadFile(path string) (SDL, error) {\n\tbuf, err := os.ReadFile(path)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn Read(buf)\n}\n```\n\n_**Read Function**_\n\nThe `Read` function takes the SDL read from file and unmarshalls the data into a `sdl` struct.\n\nSeveral pieces of data parsing are initiated with the `Read` function -including passing the SDL struct to the `DeploymentGroups` method and subsequent validation of those deployment groups - and eventually the SDL struct is returned from to ReadFile which in turn returns the SDL struct to `main.go`.\n\nThe returned SDL struct is stored as variable `sdlManifest` in our `main.go` file and processing of the message proceeds.\n\n```\n// Read reads buffer data and returns SDL instance\nfunc Read(buf []byte) (SDL, error) {\n\tobj := &sdl{}\n\tif err := yaml.Unmarshal(buf, obj); err != nil {\n\t\treturn nil, err\n\t}\n\n\tif err := obj.validate(); err != nil {\n\t\treturn nil, err\n\t}\n\n\tdgroups, err := obj.DeploymentGroups()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tvgroups := make([]dtypes.GroupSpec, 0, len(dgroups))\n\tfor _, dgroup := range dgroups {\n\t\tvgroups = append(vgroups, *dgroup)\n\t}\n\n\tif err := dtypes.ValidateDeploymentGroups(vgroups); err != nil {\n\t\treturn nil, err\n\t}\n\n\tm, err := obj.Manifest()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tif err := validation.ValidateManifest(m); err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn obj, nil\n}\n```","description":null,"slug":"eng-notes/akash-custom-clients/akash-client---create-transactions/akash-message-creation/read-sdl-from-file"},{"title":"Overview","body":"\n## Code Repository\n\nThe full code examples used in this guide can be found [here](https://github.com/chainzero/akash-client/tree/main/akashrpcclient\\_withtx).\n\n## Deployment Transactions\n\nIn this documentation section we will create the following Akash blockchain transaction operations.  These examples provide a foundation in which other - non-deployment related transactions - may be created.\n\n* Within Go code create, sign, and broadcast a create deployment transaction\n\n## Primary Sections\n\nIn the process of reviewing the mechanics of a creating an Akash deployment transaction the following primary sections are found in `main.go`.&#x20;\n\n* [Akash Message Creation](/docs/eng-notes/akash-custom-clients/akash-client---create-transactions/akash-message-creation/overview/)\n* [Transaction Creation, Signing, and Broadcasting](/docs/eng-notes/akash-custom-clients/akash-client---create-transactions/transaction-creation-signing-and-broadcasting/broadcast-transaction/)","description":null,"slug":"eng-notes/akash-custom-clients/akash-client---create-transactions/overview"},{"title":"Broadcast Transaction","body":"\n```\n...TRUNCATED...\n// Broadcast a transaction from account with the message\n// to create a post store response in txResp\ntxResp, err := client.BroadcastTx(ctx, account, msg)\nif err != nil {\n\tlog.Fatal(err)\n}\n\n// Print response from broadcasting a transaction\nfmt.Print(\"Transaction broadcast result:\\n\\n\")\nfmt.Println(txResp)\n...TRUNCATED...\n```","description":null,"slug":"eng-notes/akash-custom-clients/akash-client---create-transactions/transaction-creation-signing-and-broadcasting/broadcast-transaction"},{"title":"Client New Function","body":"\n> [Source code reference location](https://github.com/chainzero/akash-client/blob/main/akashrpcclient\\_withtx/main.go)\n\nThe `New` function is called from `main.go` initiating the creation of a new client instance.  The `New` method in the local `client` package is called.\n\n```\n// Account `chainzero` was available in local OS keychain from test machine\naccountName := \"chainzero\"\n// accountAddress := \"akash1w3k6qpr4uz44py4z68chfrl7ltpxwtkngnc6xk\"\n\nctx := context.Background()\naddressPrefix := \"akash\"\n\n// Create a Cosmos client instance\nclient, err := client.New(ctx, client.WithAddressPrefix(addressPrefix))\nif err != nil {\n\tlog.Fatal(err)\n}\n```\n\n> [Source code reference location](https://github.com/chainzero/akash-client/blob/main/akashrpcclient\\_withtx/client/client.go)\n\nThe `New` function in the `client` returns  `Client` struct using hardcoded/default values some of which are defined as constants with in our example code.  Ability to dynamically override these default values would be needed in a production client but is not necessary in our client.\n\n<pre><code><strong>...TRUNCATED...\n</strong><strong>const (\n</strong>\t// GasAuto allows to calculate gas automatically when sending transaction.\n\tGasAuto = \"auto\"\n\n\tdefaultNodeAddress   = \"https://akash-rpc.polkachu.com:443\"\n\tdefaultGasAdjustment = 2.0\n\tdefaultGasPrice      = \"0.025uakt\"\n\t// defaultGasLimit      = 300000\n\n\tdefaultTXsPerPage = 30\n\n\tsearchHeight = \"tx.height\"\n\n\torderAsc = \"asc\"\n)\n\n...TRUNCATED...\n// New creates a new client with given options.\nfunc New(ctx context.Context, options ...Option) (Client, error) {\n\tc := Client{\n\t\tnodeAddress:    defaultNodeAddress,\n\t\tkeyringBackend: account.KeyringTest,\n\t\taddressPrefix:  \"akash\",\n\t\tout:            io.Discard,\n\t\tgas:            \"auto\",\n\t}\n...TRAUNCATED....\n</code></pre>\n\nExamining additional elements of the Client struct - located in the same file - are needed for downstream operations such as reading the account from the keyring.\n\n* Location of keyring\n* Address of the deployment account\n* Gas settings - expanded upon on the broadcast section of this doc\n\n```\ntype Client struct {\n\t// RPC is Tendermint RPC.\n\tRPC rpcclient.Client\n\n\t// TxFactory is a Cosmos SDK tx factory.\n\tTxFactory tx.Factory\n\n\t// context is a Cosmos SDK client context.\n\tcontext client.Context\n\n\t// AccountRegistry is the registry to access accounts.\n\tAccountRegistry account.Registry\n\n\taccountRetriever client.AccountRetriever\n\tbankQueryClient  banktypes.QueryClient\n\tgasometer        Gasometer\n\tsigner           Signer\n\n\taddressPrefix string\n\n\tnodeAddress string\n\tout         io.Writer\n\tchainID     string\n\n\thomePath           string\n\tkeyringServiceName string\n\tkeyringBackend     account.KeyringBackend\n\tkeyringDir         string\n\n\tgas           string\n\tgasPrices     string\n\tgasAdjustment float64\n\tfees          string\n\tgenerateOnly  bool\n}\n```","description":null,"slug":"eng-notes/akash-custom-clients/akash-client---create-transactions/transaction-creation-signing-and-broadcasting/client-new-function"},{"title":"Retrieve Account from Keyring","body":"\n## Keyring Lookup via Name or Address\n\n> [Source code reference location](https://github.com/chainzero/akash-client/blob/main/akashrpcclient\\_withtx/main.go)\n\nAs we begin the process of looking up the signing account in the keyring, decision can be made to initiate the lookup either by provided key name or account address.\n\nIn the example code a local key name of `chainzero` is used to conduct a lookup by name.\n\n```\n...TRUNCATED...\n///////TX OPERATIONS/////////\n\n// Account `chainzero` was available in local OS keychain from test machine\naccountName := \"chainzero\"\n// accountAddress := \"akash1w3k6qpr4uz44py4z68chfrl7ltpxwtkngnc6xk\"\n\nctx := context.Background()\naddressPrefix := \"akash\"\n\n// Create a Cosmos client instance\nclient, err := client.New(ctx, client.WithAddressPrefix(addressPrefix))\nif err != nil {\n\tlog.Fatal(err)\n}\n\n// Get account from the keyring\naccount, err := client.Account(accountName)\nif err != nil {\n\tlog.Fatal(err)\n}\n\n...TRUNCATED...\n```\n\n## Account Function Call\n\n> [Source code reference location](https://github.com/chainzero/akash-client/blob/main/akashrpcclient\\_withtx/client/client.go)\n\nThe `Account` method - called from `main.go` - and in the `client` local package provokes a keyring lookup based on declaration of keyring location.&#x20;\n\nAs the Account method may be passed either a key name or address:\n\n* First an attempt to lookup the passed in argument by name and via the `GetByName` method is attempted\n* Second - and only if `GetByName` returns an error based on the failure to lookup the account in the keyring by name - the `GetByAddress` method is attempted\n\nThe `GetByName` and `GetByAddress` methods are located in the local `account` package and are detailed in the subsequent section.\n\n```\n...TRUNCATED...\nfunc (c Client) Account(nameOrAddress string) (account.Account, error) {\n\tdefer c.lockBech32Prefix()()\n\n\tacc, err := c.AccountRegistry.GetByName(nameOrAddress)\n\tif err == nil {\n\t\treturn acc, nil\n\t}\n\treturn c.AccountRegistry.GetByAddress(nameOrAddress)\n}\n...TRUNCATED...\n```\n\n## Get Account By Name or Address\n\n> [Source code reference location](https://github.com/chainzero/akash-client/blob/main/akashrpcclient\\_withtx/account/account.go)\n\nThe `GetByName` and `GetByAddress` methods invoke upsrream calls to the Cosmos SDK `keyring` package located [here](https://github.com/cosmos/cosmos-sdk/blob/main/crypto/keyring/keyring.go).\n\n```\nfunc (r Registry) GetByName(name string) (Account, error) {\n\tinfo, err := r.Keyring.Key(name)\n\tif errors.Is(err, dkeyring.ErrKeyNotFound) || errors.Is(err, sdkerrors.ErrKeyNotFound) {\n\t\treturn Account{}, &AccountDoesNotExistError{name}\n\t}\n\tif err != nil {\n\t\treturn Account{}, err\n\t}\n\n\tacc := Account{\n\t\tName: name,\n\t\tInfo: info,\n\t}\n\n\treturn acc, nil\n}\n\n// GetByAddress returns an account by its address.\nfunc (r Registry) GetByAddress(address string) (Account, error) {\n\n\tsdkAddr, err := sdktypes.AccAddressFromBech32(address)\n\tif err != nil {\n\t\treturn Account{}, err\n\t}\n\tinfo, err := r.Keyring.KeyByAddress(sdkAddr)\n\tif errors.Is(err, dkeyring.ErrKeyNotFound) || errors.Is(err, sdkerrors.ErrKeyNotFound) {\n\t\treturn Account{}, &AccountDoesNotExistError{address}\n\t}\n\tif err != nil {\n\t\treturn Account{}, err\n\t}\n\treturn Account{\n\t\tName: address,\n\t\tInfo: info,\n\t}, nil\n}\n```","description":null,"slug":"eng-notes/akash-custom-clients/akash-client---create-transactions/transaction-creation-signing-and-broadcasting/retrieve-account-from-keyring"},{"title":"Akash Client - Foundational Elements","body":"\nCosmos has created the Ignite CLI to quickly and easily scaffold new Cosmos blockchains.  As part of this project the Ignite CLI is provided a [Go client example](https://docs.ignite.com/clients/go-client#creating-a-blockchain-client) to programmatically interact with create chains.\n\nIn our exploration of interacting with the Akash blockchain we will largely follow the framework created within the Cosmos Ignite project.  But we will not use the Ignite CLI code directly due to the following factors:\n\n* The Akash project and the Ignite CLI currently utilize different versions of the Cosmos SDK and thus we need to adjust the Ignite source code for several differences when interacting with Cosmos SDK versions.\n* Deconstructing the Ignite CLI Go client allows a through inspection and understanding of the mechanics necessary and involved in Akash blockchain interactions.\n\n## Objective\n\nThe code samples provided in this guide are not meant to provide a full Go client for Akash blockchain use.  We have full Go clients available for this purpose.  Rather the purpose of this guide is to break down the interactions and mechanics involved in querying and submitting transactions to the Akash blockchain to allow developer builds of future integrations.\n\nWe believe the review in this document will remove much of the friction and substantial time investments previously encountered when attempting to get started with Akash integrations.\n\n## Focus of Documentation\n\nAkash deployment creation transactions and deployment queries will serve as the focus of this document.\n\nUsing the deployment activities, one could easily review the Akash code base to create other transactions/queries of interest including lease creation, provider interractions, etc.\n\n## Base Client\n\nWithin this section we will concentrate on formation of the basic Go client capable of creating a session with the Akash blockchain.\n\nThe Client we construct in this section will then be utilized for Query and Transaction operations in later sections.","description":null,"slug":"eng-notes/akash-custom-clients/akash-client---foundational-elements"},{"title":"Client New Function","body":"\n> [Source code reference location](https://github.com/chainzero/akash-client/blob/main/akashrpcclient\\_queryonly/client/client.go)\n\nThe `New` function is called from `main.go` initiating the creation of a new client instance.\n\n```\n// New creates a new client with given options.\nfunc New(ctx context.Context, options ...Option) (Client, error) {\n\tc := Client{\n\t\tnodeAddress:    defaultNodeAddress,\n\t\tkeyringBackend: account.KeyringTest,\n\t\taddressPrefix:  \"akash\",\n\t\tout:            io.Discard,\n\t\tgas:            \"auto\",\n\t}\n...TRAUNCATED....\n```\n\nExamining the Client struct - located in the same file - reveals the settings/definitions the the `New` method will populate including:\n\n* RPC node address\n* Location of keyring\n* Address of the deployment account - expanded on in the coverage of transactions in this doc\n* Gas setting - expanded on in the coverage of transactions in this doc\n\nFor the purpose of our blockchain query client build a majority of these fields are not necessary with RPC node address being the main setting of importance.\n\n```\ntype Client struct {\n\t// RPC is Tendermint RPC.\n\tRPC rpcclient.Client\n\n\t// TxFactory is a Cosmos SDK tx factory.\n\tTxFactory tx.Factory\n\n\t// context is a Cosmos SDK client context.\n\tcontext client.Context\n\n\t// AccountRegistry is the registry to access accounts.\n\tAccountRegistry account.Registry\n\n\taccountRetriever client.AccountRetriever\n\tbankQueryClient  banktypes.QueryClient\n\tgasometer        Gasometer\n\tsigner           Signer\n\n\taddressPrefix string\n\n\tnodeAddress string\n\tout         io.Writer\n\tchainID     string\n\n\thomePath           string\n\tkeyringServiceName string\n\tkeyringBackend     account.KeyringBackend\n\tkeyringDir         string\n\n\tgas           string\n\tgasPrices     string\n\tgasAdjustment float64\n\tfees          string\n\tgenerateOnly  bool\n}\n```","description":null,"slug":"eng-notes/akash-custom-clients/akash-client---query-only/akash-client-creation/client-new-function"},{"title":"NewQueryClient Method","body":"\nWithin `main.go` the function `NewQueryClient` is called.\n\nBased on the import statements in `main.go` (shown below) and the call of `types.NewQueryClient(client.Context())` - it is evident that:\n\n* The NewQueryClient function exists in the Akash node repository\n* The `client` instance that was created in the prior block is passed into the function\n\n```\ntypes \"github.com/akash-network/node/x/deployment/types/v1beta2\"\n```\n\nFrom within the deployment module code - path above - `NewQueryClient` exists in `query.pb.go`.\n\n> [Source code reference location](https://github.com/akash-network/node/blob/master/x/deployment/types/v1beta2/query.pb.go)\n\n```\nfunc NewQueryClient(cc grpc1.ClientConn) QueryClient {\n\treturn &queryClient{cc}\n}\n```\n\nThe struct returned by `NewQueryClient` creates a client connection using `github.com/gogo/protobuf/grpc` .\n\n```\ntype queryClient struct {\n\tcc grpc1.ClientConn\n}\n```","description":null,"slug":"eng-notes/akash-custom-clients/akash-client---query-only/akash-client-creation/newqueryclient-method"},{"title":"Overview","body":"\n\n\nThe code reviewed in this section primarily lives within the following repository locations:\n\n* The `main.go` file directly accessible [here](https://github.com/chainzero/akash-client/blob/main/akashrpcclient\\_queryonly/main.go)\n* The `client.go` file directly accessible [here](https://github.com/chainzero/akash-client/blob/main/akashrpcclient\\_queryonly/client/client.go)\n\n> The client directory and package holds the primary logic for connecting and interacting with the Akash blockchain.  The files in this directory will be explored greatly within this guide.\n\n## Akash Client Initiation Within main.go\n\nThe build of a blockchain query begins via the following declarations and initiations:\n\n* Creation of a new Go context via `context.Background()`\n* Call of `New` method in the local `client` package.  Additional details of the `New` function can be found [here](/docs/eng-notes/akash-custom-clients/akash-client---query-only/akash-client-creation/client-new-function/).\n* instantiate a new query client which is defined in a Protobuf file in Akash source code and within the Deployment module.  Additional details on the `NewQueryClient` function can be found [here](/docs/eng-notes/akash-custom-clients/akash-client---query-only/akash-client-creation/newqueryclient-method/).\n\n```\n\nfunc main() {\n\tctx := context.Background()\n\taddressPrefix := \"akash\"\n\n\t// Create a Cosmos client instance\n\tclient, err := client.New(ctx, client.WithAddressPrefix(addressPrefix))\n\tif err != nil {\n\t\tlog.Fatal(err)\n\t}\n\n\t// Instantiate a query client\n\tqueryClient := types.NewQueryClient(client.Context())\n...<TRUNCATED>....\n```\n","description":null,"slug":"eng-notes/akash-custom-clients/akash-client---query-only/akash-client-creation/overview"},{"title":"Overview","body":"\n\nTwo example queries are included for illustration.  Using these examples queries as a template we can conduct similar read operations on any Akash blockchain as element.\n\n* [Query All Deployments on the Blockchain](/docs/eng-notes/akash-custom-clients/akash-client---query-only/example-rpc-queries/query-all-deployments-on-the-blockchain/)\n* [Query a Specific Deployment ID (DSEQ and Owner ID Specified)](/docs/eng-notes/akash-custom-clients/akash-client---query-only/example-rpc-queries/query-a-specific-deployment-id/)\n* [Using Examples to Build Your Own Query](/docs/eng-notes/akash-custom-clients/akash-client---query-only/example-rpc-queries/using-examples-to-build-your-own-query/)","description":null,"slug":"eng-notes/akash-custom-clients/akash-client---query-only/example-rpc-queries/overview"},{"title":"Query a Specific Deployment ID (DSEQ and Owner ID Specified)","body":"\n## Deployment Query Overview\n\n> [Source code reference location](https://github.com/chainzero/akash-client/blob/main/akashrpcclient\\_queryonly/main.go)\n\nFor the purpose of querying a specific deployment a `DeploymentID` struct - defined in Akash source code - is populated with the desired `Owner` and `Dseq` fields.\n\nThe `QueryDeploymentRequest` struct's `ID` field is then defined with the `deploymentid` struct reference.\n\n```\n....TRUNCATED....\n// Query the blockchain using the client's `Deployment` method for a return of a specific deployment\ndeploymentid := types.DeploymentID{\n\tOwner: \"akash1f53fp8kk470f7k26yr5gztd9npzpczqv4ufud7\",\n\tDSeq:  10219997,\n}\n\nquerydeploymentrequest := types.QueryDeploymentRequest{\n\tID: deploymentid,\n}\n\n// Query the blockchain using the client's `Deployment` method\nqueryResp, err := queryClient.Deployment(ctx, &querydeploymentrequest)\nif err != nil {\n\tlog.Fatal(err)\n}\n....TRUNCATED....\n```\n\n## Deployment Method\n\n> [Source code reference location](https://github.com/akash-network/node/blob/master/x/deployment/types/v1beta2/query.pb.go)\n\nThe Deployment method used in this example is located in the deployment module Protobuf file.\n\nIn the section [Using Examples to Build Your Own Query](/docs/eng-notes/akash-custom-clients/akash-client---query-only/example-rpc-queries/using-examples-to-build-your-own-query/) we will explore the Protobuf files and types further.\n\n```\nfunc (c *queryClient) Deployment(ctx context.Context, in *QueryDeploymentRequest, opts ...grpc.CallOption) (*QueryDeploymentResponse, error) {\n\tout := new(QueryDeploymentResponse)\n\terr := c.cc.Invoke(ctx, \"/akash.deployment.v1beta2.Query/Deployment\", in, out, opts...)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn out, nil\n}\n```","description":null,"slug":"eng-notes/akash-custom-clients/akash-client---query-only/example-rpc-queries/query-a-specific-deployment-id"},{"title":"Query All Deployments on the Blockchain","body":"\n## Deployments Query Overview\n\n> [Source code reference location](https://github.com/chainzero/akash-client/blob/main/akashrpcclient\\_queryonly/main.go)\n\nWithin the `main.go` file a very simplistic query example is examined first.\n\nDetails on the Deployments method:\n\n* The Deployments method requires no input and returns all deployments on the blockchain.\n* By default the output, returned data is limited to the most recent 100 records/deployments and is paginated.\n\n> _**NOTE**_ - in the sample code provided in the GitHub repository this `Deployments` query is commented out and the `Deployment` query is active/highlighted.  Uncomment this block to experiment with the Deployments query.\n\n```\n....TRUNCATED....\n// Query the blockchain using the client's `Deployments` method for a return of all deployments\nqueryResp, err := queryClient.Deployments(ctx, &types.QueryDeploymentsRequest{})\nif err != nil {\n\tlog.Fatal(err)\n}\n....TRUNCATED....\n```\n\n## Deployments Method&#x20;\n\n> [Source code reference location](https://github.com/akash-network/node/blob/master/x/deployment/types/v1beta2/query.pb.go)\n\nThe Deployments method used in this example is located in the deployment module Protobuf file.\n\nIn the section [Using Examples to Build Your Own Query](/docs/eng-notes/akash-custom-clients/akash-client---query-only/example-rpc-queries/using-examples-to-build-your-own-query/) we will explore the Protobuf files and types further.\n\n```\nfunc (c *queryClient) Deployments(ctx context.Context, in *QueryDeploymentsRequest, opts ...grpc.CallOption) (*QueryDeploymentsResponse, error) {\n\tout := new(QueryDeploymentsResponse)\n\terr := c.cc.Invoke(ctx, \"/akash.deployment.v1beta2.Query/Deployments\", in, out, opts...)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn out, nil\n}\n```","description":null,"slug":"eng-notes/akash-custom-clients/akash-client---query-only/example-rpc-queries/query-all-deployments-on-the-blockchain"},{"title":"Using Examples to Build Your Own Query","body":"\nThe `Deployment` and `Deployments` provide ready to use query examples.  In this section we ensure it is clear how relevant types may be found to build any other query of interest.\n\n## Lease Info Example\n\nA query of Akash lease info will be used to explore the location of relevant Go types to build related queries.  The exercise used in this example could be used for the construction of any blockchain query of interest.\n\n\\<ADDITIONAL DETAILS OF THIS SECTION NOT YET COMPLETED>","description":null,"slug":"eng-notes/akash-custom-clients/akash-client---query-only/example-rpc-queries/using-examples-to-build-your-own-query"},{"title":"Overview","body":"\n## Code Repository\n\nThe full code examples used in this guide can be found [here](https://github.com/chainzero/akash-client/tree/main/akashrpcclient\\_queryonly).\n\n## Deployment Queries\n\nIn this documentation section we will create the following Akash blockchain query operations.  These examples provide a foundation in which other - non-deployment related queries - may be created.\n\n* Query the blockchain for all Akash deployments owned by a specific owner address\n* Query the blockchain for a specific Akash deployment based on the DSEQ (depolyment ID) provided","description":null,"slug":"eng-notes/akash-custom-clients/akash-client---query-only/overview"},{"title":"Code","body":"\n\nFor this example, repositories will be located in `~/go/src/github.com/akash-network`. Create directory if it does not already exist via:\n\n```\nmkdir -p ~/go/src/github.com/akash-network\n```\n\n## Clone Akash Node and Provider Repositories\n\n> _**NOTE**_ - all commands in the remainder of this guide  assume a current directory of `~/go/src/github.com/akash-network`unless stated otherwise.\n\n```shell\ncd ~/go/src/github.com/akash-network \ngit clone https://github.com/akash-network/node.git\ngit clone https://github.com/akash-network/provider.git\n```\n\n## Allow Direnv Management of Provider Directory\n\n```\ncd provider\ndirenv allow\n```","description":null,"slug":"eng-notes/akash-development-environment/code"},{"title":"Development Environment General Behavior","body":"\nAll examples are located within [\\_run](https://github.com/akash-network/provider/tree/main/\\_run) directory. Commands are implemented as `make` targets.\n\nThere are three ways we use to set up the Kubernetes cluster.\n\n* kind\n* minukube\n* ssh\n\nBoth `kind` and `minikube` are e2e, i.e. the configuration is capable of spinning up cluster and the local host, whereas `ssh` expects cluster to be configured before use.","description":null,"slug":"eng-notes/akash-development-environment/development-environment-general-behavior"},{"title":"Install Tools","body":"\nRun following script to install all system-wide tools. Currently supported host platforms.\n\n* MacOS\n* Debian based OS PRs with another hosts are welcome\n* Windows is not supported\n\n```shell\ncd ~/go/src/github.com/akash-network\n./provider/script/install_dev_dependencies.sh\n```","description":null,"slug":"eng-notes/akash-development-environment/install-tools"},{"title":"Overview and Requirments","body":"\n\nThis page covers setting up development environment for both [node](https://github.com/akash-network/node) and [provider](https://github.com/akash-network/provider) repositories. The provider repo elected as placeholder for all the scripts as it depends on the `node` repo.   Should you already know what this guide is all about - feel free to explore examples.\n\n## Requirements\n\n### Golang\n\nGo must be installed on the machine used to initiate the code used in this guide. Both projects - Akash Node and Provider - are keeping up-to-date with major version on development branches. Both repositories are using the latest version of the Go, however only minor that has to always match.\n\n### **Docker Engine**\n\nEnsure that Docker Desktop/Engine has been installed on machine that the development environment will be launched from.\n\n### Direnv Use\n\n#### Install Direnv if Necessary\n\nDirenv is used for the install process.  Ensure that you have Direnv install these [instructions](https://direnv.net/).\n\n#### Configure Environment for Direnv Use\n\n* Edit the ZSH shell profile with visual editor.\n\n```\nvi .zshrc\n```\n\n* Add the following line to the profile.\n\n```\neval \"$(direnv hook zsh)\"\n```","description":null,"slug":"eng-notes/akash-development-environment/overview-and-requirments"},{"title":"Parameters","body":"\nParameters for use within the Runbooks detailed later in this guide.\n\n| Name                | Default value                                                                                     | Effective on target(s)                                                                                                                                            |\n| ------------------- | ------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| SKIP\\_BUILD         | false                                                                                             |                                                                                                                                                                   |\n| DSEQ                | 1                                                                                                 | <ul><li>deployment-* </li><li>lease-* </li><li>bid-* </li><li>send-manifest</li></ul>                                                                             |\n| OSEQ                | 1                                                                                                 | <ul><li>deployment-* </li><li>lease-* </li><li>bid-* </li><li>send-manifest</li></ul>                                                                             |\n| GSEQ                | 1                                                                                                 | <ul><li>deployment-* </li><li>lease-* </li><li>bid-* </li><li>send-manifest</li></ul>                                                                             |\n| KUSTOMIZE\\_INSTALLS | <p>Depends on runbook.<br>Refer to each runbook's <code>Makefile</code> to see default value.</p> | <ul><li>kustomize-init</li><li>kustomize-templates</li><li>kustomize-set-images</li><li>kustomize-configure-services </li><li>kustomize-deploy-services</li></ul> |\n","description":null,"slug":"eng-notes/akash-development-environment/parameters"},{"title":"Runbook","body":"\nThere are four configuration variants, each presented as directory within[ \\_run](https://github.com/akash-network/provider/tree/main/\\_run).\n\n* `kube` - uses `kind` to set up local cluster. It is widely used by e2e testing of the provider. Provider and the node run as host services. All operators run as kubernetes deployments.\n* `single` - uses `kind` to set up local cluster. Main difference is both node and provider (and all operators) are running within k8s cluster as deployments. (at some point we will merge `single` with `kube` and call it `kind`)\n* `minikube` - not in use for now\n* `ssh` - expects cluster to be up and running. mainly used to test sophisticated features like `GPU` or `IP leases`\n\nThe only difference between environments above is how they set up. Once running, all commands are the same.\n\nRunning through the entire runbook requires multiples terminals. Each command is marked **t1**-**t3** to indicate a suggested terminal number.\n\nIf at any point something goes wrong and cluster needs to be run from the beginning:\n\n```shell\ncd provider/_run/<kube|single|ssh>\nmake kube-cluster-delete\nmake clean\nmake init\n```","description":null,"slug":"eng-notes/akash-development-environment/runbook"},{"title":"Use Runbook","body":"\n\n\n> _**NOTE**_ - this runbook requires three simultaneous terminals\n\nFor the purpose of documentation clarity we will refer to these terminal sessions as:\n\n* terminal1\n* terminal2\n* terminal3\n\n### STEP 1 - Open Runbook\n\n> _**NOTE**_ - run the commands in this step on terminal1, terminal2, and terminal3&#x20;\n\nRun this step on all three terminal sessions to ensure we are in the correct directory for later steps.\n\n```\ncd ~/go/src/github.com/akash-network/provider/_run/kube\n```\n\n### STEP 2 - Create and Provision Local Kind Kubernetes Cluster\n\n> _**NOTE**_ - run this command in this step on terminal1 only\n\n> _**NOTE**_ - this step may take several minutes to complete\n\n```\nmake kube-cluster-setup\n```\n\n#### Possible Timed Out Waiting for the Condition Error\n\nIf the following error is encountered when running `make kube-cluster-setup`:\n\n```\nWaiting for deployment \"ingress-nginx-controller\" rollout to finish: 0 out of 1 new replicas have been updated...\nWaiting for deployment \"ingress-nginx-controller\" rollout to finish: 0 of 1 updated replicas are available...\nerror: timed out waiting for the condition\nmake: *** [../common-kube.mk:120: kube-setup-ingress-default] Error 1\n```\n\nThis is an indication that the Kubernetes ingress-controller did not initialize within the default timeout period.  In such cases, re-execute `make kube-cluster-setup` with a custom timeout period such as the example below.  This step is NOT necessary if `make kube-cluster-setup` completed on first run with no errors encountered.\n\n```\ncd provider/_run/<kube|single|ssh>\nmake kube-cluster-delete\nmake clean\nmake init\nKUBE_ROLLOUT_TIMEOUT=500 make kube-cluster-setup\n```\n\n### STEP 3 - Start Akash Node\n\n> _**NOTE**_ - run this command in this step on terminal2 only\n\n```\nmake node-run\n```\n\n### STEP 4 - Create an Akash Provider\n\n> _**NOTE**_ - run this command in this step on terminal1 only\n\n```\nmake provider-create\n```\n\n#### Note on Keys\n\nEach configuration creates four keys: The keys are assigned to the targets and under normal circumstances there is no need to alter it. However, it can be done with setting KEY\\_NAME:\n\n```\n# create provider from **provider** key\nmake provider-create\n\n# create provider from custom key\nKEY_NAME=other make provider-create\n```\n\n### STEP 5 - Start the Akash Provider\n\n> _**NOTE**_ - run this command in this step on terminal3 only\n\n```\nmake provider-run\n```\n\n### STEP 6 - Create and Verify Test Deployment\n\n> _**NOTE**_ - run the commands in this step on terminal1 only\n\n#### Create the Deployment\n\n* Take note of the deplpyment ID (DSEQ) generated for use in subsequent steps\n\n```\nmake deployment-create\n```\n\n#### Query Deployments\n\n```\nmake query-deployments\n```\n\n#### Query Orders\n\n* Steps ensure that an order is created for the deployment after a short period of time\n\n```\nmake query-orders\n```\n\n#### Query Bids\n\n* Step ensures the Provider services daemon bids on the test deployment\n\n```\nmake query-bids\n```\n\n### STEP 7 - Test Lease Creation for the Test Deployment\n\n> _**NOTE**_ - run the commands in this step on terminal1 only\n\n#### Create Lease\n\n```\nmake lease-create\n```\n\n#### Query Lease\n\n```\nmake query-leases\n```\n\n#### Ensure Provider Received Lease Create Message\n\n* Should see \"pending\" inventory in the provider status and for the test deployment\n\n```\nmake provider-status\n```\n\n### STEP 8 - Send Manifest\n\n> _**NOTE**_ - run the commands in this step on terminal1 only\n\n#### Send the Manifest to the Provider\n\n```\nmake send-manifest\n```\n\n#### Check Status of  Deployment\n\n```\nmake provider-lease-status\n```\n\n#### Ping the Deplpyment to Ensure Liveness\n\n```\n make provider-lease-ping\n```\n\n### STEP 9 - Verify Service Status\n\n> _**NOTE**_ - run the commands in this step on terminal1 only\n\n#### Query Lease Status\n\n```\nmake provider-lease-status\n```\n\n#### Fetch Pod Logs\n\n* Note that this will fetch the logs for all pods in the Kubernetes cluster.  Filter/search for the test deployment's ID (DSEQ) for related activities.\n\n```\nmake provider-lease-logs\n```","description":null,"slug":"eng-notes/akash-development-environment/use-runbook"},{"title":"providerRepoCoverage","body":"\n\n## Akash Provider Repo SDL Testing Coverage\n\n## Content\n\n* CURRENT TESTS\n  * [Persistent Storage](#persistent-storage)\n  * [IP Leases](#ip-leases)\n  * [GPU](#gpu)\n  * [Services](#services)\n  * [Profiles](#profiles)\n  * [Placement](#placement)\n  * [Resources](#resources)\n  * [Escrow/Payments](#escrowpayments)\n  * [General](#general)\n* SUGGESTED ADDITIONS\n\n## _**Current Tests**_\n\n## Persistent Storage\n\n### Test Coverage\n\n| Success/Failure Test | Test Specs                                                                                            | Current E2E Test Coverage |\n| -------------------- | ----------------------------------------------------------------------------------------------------- | ------------------------- |\n|                      |                                                                                                       |                           |\n| _**Success**_        |                                                                                                       |                           |\n|                      | [SIMPLE PERSISTENT STORAGE DEPLOYMENT TEST WITH BETA2 STORAGE TYPE](broken-reference)                 | \\[] Coverage              |\n|                      | [SIMPLE PERSISTENT STORAGE DEPLOYMENT TEST WITH NO STORAGE TYPE DEFINED](broken-reference)            | \\[X] Coverage             |\n| _**Failure**_        |                                                                                                       |                           |\n|                      | [SIMPLE PERSISTENT STORAGE DEPLOYMENT TEST WITH NO MOUNT SPECIFIED](broken-reference)                 | \\[] Coverage              |\n|                      | [SIMPLE PERSISTENT STORAGE DEPLOYMENT TEST FAIL ON NO ABSOLUTE MOUNT PATH](broken-reference)          | \\[] Coverage              |\n|                      | [SIMPLE PERSISTENT STORAGE DEPLOYMENT TEST FAIL ON INVALID NAME](broken-reference)                    | \\[] Coverage              |\n|                      | [SIMPLE PERSISTENT STORAGE DEPLOYMENT TEST FAIL ON ATTEMPT TO USE MOUNT PATH TWICE](broken-reference) | \\[] Coverage              |\n|                      | [SIMPLE PERSISTENT STORAGE DEPLOYMENT TEST FAIL ON NO SERVICE CONFIG](broken-reference)               | \\[] Coverage              |\n\n#### Expected Success\n\n**SIMPLE PERSISTENT STORAGE DEPLOYMENT TEST WITH BETA2 STORAGE TYPE**\n\n* _**Description**_ - verify simple persistent storage deployment with the storage type of BETA2.\n* [Current SDL](https://github.com/akash-network/provider/blob/f13aca40ac42f96b80ec5e863cdfa20093e23b44/testdata/deployment/deployment-v2-storage-beta2.yaml) - `deployment-v2-storage-beta2.yaml`\n* _**Expected Outcome**_ - success deployment/order creation with associated provider bid receipt for persistent storage type of BETA2.\n\n**SIMPLE PERSISTENT STORAGE DEPLOYMENT TEST WITH NO STORAGE TYPE DEFINED**\n\n* _**Description**_ - verify simple persistent storage deployment with the storage type omitted/not populated.\n* [Current SDL](https://github.com/akash-network/provider/blob/main/testdata/deployment/deployment-v2-storage-default.yaml) - `deployment-v2-storage-default.yaml`\n* _**Expected Outcome**_ - success deployment/order creation. Note - no providers on the network (at the time of this writing) bid on default storage type.\n* _**Current Coverage**_ - [end to end test coverage](https://github.com/akash-network/provider/blob/main/integration/persistentstorage\\_test.go)\n\n**PERSISTENT STORAGE DEPLOYMENT UPDATE TEST A**\n\n* _**Description**_ - verify deployment update capability when persistent storage is included in workload.\n* [Current SDL](https://github.com/akash-network/provider/blob/main/testdata/deployment/deployment-v2-storage-updateA.yaml) - `deployment-v2-storage-updateA.yaml`\n* _**Expected Outcome**_ - successful update of image and/or env variables when persistent storage is included in deplopyment.\n\n**PERSISTENT STORAGE DEPLOYMENT UPDATE TEST B**\n\n* _**Description**_ - verify deployment update capability when persistent storage is included in workload.\n* [Current SDL](https://github.com/akash-network/provider/blob/main/testdata/deployment/deployment-v2-storage-updateB.yaml) - `deployment-v2-storage-updateB.yaml`\n* _**Expected Outcome**_ - successful update of image and/or env variables when persistent storage is included in deplopyment.\n\n#### Expected Failure\n\n**SIMPLE PERSISTENT STORAGE DEPLOYMENT TEST WITH NO MOUNT SPECIFIED**\n\n* _**Description**_ - Failure test when no mount point is provided for persistent storage use\n* [Current SDL](https://github.com/akash-network/provider/blob/f13aca40ac42f96b80ec5e863cdfa20093e23b44/testdata/sdl/storageClass1.yaml) - `storageClass1.yaml`\n* _**Expected Outcome**_ - The SDL should fail validation on deployment creation attempt as there is no mount point specified for persistent storage in the `services > params > storage > configs` stanza.\n\n**SIMPLE PERSISTENT STORAGE DEPLOYMENT TEST FAIL ON NO ABSOLUTE MOUNT PATH**\n\n* _**Description**_ - Failure test when no absolute directory path is supplied in mount point.\n* [Current SDL](https://github.com/akash-network/provider/blob/f13aca40ac42f96b80ec5e863cdfa20093e23b44/testdata/sdl/storageClass2.yaml) - `storageClass2.yaml`\n* _**Expected Outcome**_ - The SDL should fail validation on deployment creation attempt as the persistent storage mount point specified is a relative path of `etc/nginx`. If the path were the absolute path of `/etc/nginx` the validation would succeed.\n\n**SIMPLE PERSISTENT STORAGE DEPLOYMENT TEST FAIL ON INVALID NAME**\n\n* _**Description**_ - Failure test when the persistent storage name does not align with name provided in services stanza.\n* [Current SDL](https://github.com/akash-network/provider/blob/f13aca40ac42f96b80ec5e863cdfa20093e23b44/testdata/sdl/storageClass3.yaml) - `storageClass3.yaml`\n* _**Expected Outcome**_ - The SDL should fail validation on deployment creation attempt as the persistent storage name specified in the `profiles` stanza - which is `configs` - does not align with the name used in the `services` stanza which is `data`.\n\n**SIMPLE PERSISTENT STORAGE DEPLOYMENT TEST FAIL ON ATTEMPT TO USE MOUNT PATH TWICE**\n\n* _**Description**_ - Failure test when a single mount path is used on more than one persistent storage volume.\n* [Current SDL](https://github.com/akash-network/provider/blob/f13aca40ac42f96b80ec5e863cdfa20093e23b44/testdata/sdl/storageClass4.yaml) - `storageClass4.yaml`\n* _**Expected Outcome**_ - The SDL should fail validation on deployment creation attempt as the persistent storage mount point of `/etc/nginx` is used on multiple persistent storage volumes within the `services` stanza.\n\n**SIMPLE PERSISTENT STORAGE DEPLOYMENT TEST FAIL ON NO SERVICE CONFIG**\n\n* _**Description**_ - Failure test when no config for persistent storage is present in the `services > params > storage` stanza.\n* [Current SDL](https://github.com/akash-network/provider/blob/f13aca40ac42f96b80ec5e863cdfa20093e23b44/testdata/sdl/storageClass5.yaml) - `storageClass5.yaml`\n* _**Expected Outcome**_ - The SDL should fail validation on deployment creation attempt as the `params > storage` section of the `services` stanza does not contain expected specifications (volume name and mount point).\n\n## IP Leases\n\n### Test Coverage\n\n| Success/Failure Test | Test Specs                                                                                            | Current E2E Test Coverage |\n| -------------------- | ----------------------------------------------------------------------------------------------------- | ------------------------- |\n|                      |                                                                                                       |                           |\n| _**Success**_        |                                                                                                       |                           |\n|                      | [SIMPLE IP LEASES CREATION AND ASSIGNMENT](#simple-ip-leases-creation-and-assignment)                                          | \\[X] Coverage             |\n|                      | [MULTIPLE AND UNIQUE IP LEASES CREATION AND ASSIGNMENT - MULTIPLE PLACEMENT GROUPS](#multiple-and-unique-ip-leases-creation-and-assignment---multiple-placement-groups) | \\[] Coverage              |\n|                      | [MULTIPLE AND UNIQUE IP LEASES CREATION AND ASSIGNMENT - SINGLE PLACEMENT GROUP](#multiple-and-unique-ip-leases-creation-and-assignment---single-placement-group)    | \\[] Coverage              |\n|                      | [MULTIPLE AND UNIQUE IP LEASES CREATION AND ASSIGNMENT - MULTIPLE PLACEMENT GROUPS](#multiple-and-unique-ip-leases-creation-and-assignment---multiple-placement-groups-1) | \\[] Coverage              |\n|                      | [SINGLE IP LEASE CREATION AND WITH MULTIPLE SERVICES ASSIGNMENT](#single-ip-lease-creation-and-with-multiple-services-assignment)                    | \\[] Coverage              |\n\n#### Expected Success\n\n##### **SIMPLE IP LEASES CREATION AND ASSIGNMENT**\n\n* _**Description**_ - validation of IP Leases creation and assignment to service .\n* [Current SDL](https://github.com/akash-network/provider/blob/f13aca40ac42f96b80ec5e863cdfa20093e23b44/testdata/deployment/deployment-v2-ip-endpoint.yamll) - `deployment-v2-ip-endpoint.yaml`\n* _**Expected Outcome**_ - deployment creation succeeds with the creation of an IP endpoint named `meow` and successful assignment of the `meow` IP endpoint to the `web` service. Bid received from provider supporting IP Leases.\n* _**Current Coverage**_ - [end to end test coverage](https://github.com/akash-network/provider/blob/main/integration/ipaddress\\_test.go)\n\n##### **MULTIPLE AND UNIQUE IP LEASES CREATION AND ASSIGNMENT - MULTIPLE PLACEMENT GROUPS**\n\n* _**Description**_ - two IP Leases declaration and assignment in unique deployment groups with multiple (two) placement groups.\n* [Current SDL](https://github.com/akash-network/provider/blob/f13aca40ac42f96b80ec5e863cdfa20093e23b44/testdata/deployment/deployment-v2-multi-groups-ip-endpoint.yaml) - `deployment-v2-multi-groups-ip-endpoint.yaml`\n* _**Expected Outcome**_\n\n##### **MULTIPLE AND UNIQUE IP LEASES CREATION AND ASSIGNMENT - SINGLE PLACEMENT GROUP**\n\n* _**Description**_ - two IP Leases declaration and assignment in unique deployment groups with a single placement groups.\n* [Current SDL](https://github.com/akash-network/provider/blob/f13aca40ac42f96b80ec5e863cdfa20093e23b44/testdata/deployment/deployment-v2-multi-ip-endpoint.yamll) - `deployment-v2-multi-ip-endpoint.yaml`\n* _**Expected Outcome**_ - successful creation of two IP Leases with activation in associated deployment group.\n\n##### **MULTIPLE AND UNIQUE IP LEASES CREATION AND ASSIGNMENT - MULTIPLE PLACEMENT GROUPS**\n\n* _**Description**_ - two IP Leases declaration and assignment in unique deployment groups with multiple placement groups.\n* [Current SDL](https://github.com/akash-network/provider/blob/f13aca40ac42f96b80ec5e863cdfa20093e23b44/testdata/deployment/deployment-v2-multi-groups-ip-endpoint.yaml) - `deployment-v2-multi-groups-ip-endpoint.yaml`\n* _**Expected Outcome**_ - successful creation of two IP Leases with activation in associated deployment group.\n\n##### **SINGLE IP LEASE CREATION AND WITH MULTIPLE SERVICES ASSIGNMENT**\n\n* _**Description**_ - ensure that a single IP Lease may be shared by multiple services.\n* [Current SDL](https://github.com/akash-network/provider/blob/f13aca40ac42f96b80ec5e863cdfa20093e23b44/testdata/deployment/deployment-v2-shared-ip-endpoint.yaml) - `deployment-v2-shared-ip-endpoint.yaml`\n* _**Expected Outcome**_ - successful creation of a single IP Lease with verification of assignment to two services using unique ports (TCP 80 and 81).\n\n#### Expected Failure\n\n### GPU\n\n#### Expected Success\n\n#### Expected Failure\n\n### Services\n\n#### Expected Success\n\n**Intra Service Communication Test**\n\n* _**Description**_ - validation of communication of intra service communication using web front-end to Redis Server using Redis service name.\n* [Current SDL](https://github.com/akash-network/provider/blob/f13aca40ac42f96b80ec5e863cdfa20093e23b44/testdata/deployment/deployment-v2-c2c.yaml) - `deployment/deployment-v2-c2c.yaml`\n* _**Expected Outcome**_ - web front-end should have successful communication with Redis pod via Redis service name.\n* _**Current Coverage**_ - [end to end test coverage](https://github.com/akash-network/provider/blob/main/integration/container2container\\_test.go)\n\n**Node Port Assignment Test**\n\n* _**Description**_ - ensure a Kubernetes node port is assigned to service when a non-HTTP port is specified.\n* [Current SDL](https://github.com/akash-network/provider/blob/f13aca40ac42f96b80ec5e863cdfa20093e23b44/testdata/deployment/deployment-v2-nodeport.yaml) - `deployment-v2-nodeport.yaml`\n* _**Expected Outcome**_ - assignment of node port in the range of 30000-32767 when a non-HTTP port is specified in SDL port field.\n* _**Current Coverage**_ - [end to end test coverage](https://github.com/akash-network/provider/blob/main/integration/node\\_port\\_test.go)\n\n**URL Assignment on HTTP Service**\n\n* _**Description**_ - ensure assignment of URL on HTTP/HTTPS port usage within service.\n* [Current SDL](https://github.com/akash-network/provider/blob/f13aca40ac42f96b80ec5e863cdfa20093e23b44/testdata/deployment/deployment-v2-nohost.yaml) - `deployment-v2-nohost.yaml`\n* _**Expected Outcome**_ - assignment of valid HTTP URL for web services. Current SDL creates a single HTTP service and thus should expect a single URL assignment. URL should be in format of `<uniqueid>.provider.<provider-domain-name>`\n\n**PRIVATE SERVICE VALIDATION**\n\n* _**Description**_ - ensure a service that does not have `global: true` specification in the `expose\\to` stanza is only reachable to specified services of the same SDL.\n* [Current SDL](https://github.com/akash-network/provider/blob/f13aca40ac42f96b80ec5e863cdfa20093e23b44/testdata/sdl/private\\_service.yaml) - `private_service.yaml`\n* _**Expected Outcome**_ - referenced SDL should create a service with no node port assignment that should only be reachable inside the Kubernetes cluster and only to services it is exposed to explicitly. In the SDL tested only the `bind` service should have access to the `pg` service.\n\n#### Expected Failure\n\n**MISALIGNMENT/INCONSISTENT SERVICE NAMES**\n\n* _**Description**_ - service name mismatch in the declaration within the `services` stanza and the use of the service in the `deployment` stanza\n* [Current SDL](https://github.com/akash-network/provider/blob/f13aca40ac42f96b80ec5e863cdfa20093e23b44/testdata/sdl/deployment-svc-mismatch.yaml) - `deployment-svc-mismatch.yaml`\n* _**Expected Outcome**_ - referenced SDL names the service `web` in the `services` stanza but references the service with name `webapp` in the \\`deployment stanza. The misalignment of service name between stanzas should result in a validation failure during deployment creation transaction send.\n\n### Profiles\n\n#### Expected Success\n\n#### Expected Failure\n\n### Placement\n\n#### Expected Success\n\n#### Expected Failure\n\n### Resources\n\n#### Expected Success\n\n#### Expected Failure\n\n### Escrow/Payments\n\n#### Expected Success\n\n**Custom Denomination Test**\n\n**TEST1**\n\n* Description - validation of payment using custom denom in placement section of the SDL.\n* [Current SDL](https://github.com/akash-network/provider/blob/f13aca40ac42f96b80ec5e863cdfa20093e23b44/testdata/deployment/deployment-v2-custom-currency.yaml) - `deployment-v2-custom-currency.yaml`\n* _**Expected Outcome**_ - deployment creation succeeds with custom demon specified and bids received from provider supporting denom.\n* _**Current Coverage**_ - [end to end test coverage](https://github.com/akash-network/provider/blob/main/integration/customcurrency\\_test.go)\n\n**Per Block Price Specification Test**\n\n**TEST1**\n\n* Description - validation of specified per block pricing specification.\n* [Current SDL](https://github.com/akash-network/provider/blob/f13aca40ac42f96b80ec5e863cdfa20093e23b44/testdata/deployment/deployment-v2-escrow.yaml) - `deployment-v2-escrow.yaml`\n* _**Expected Outcome**_ - bid received from provider with a per block price of less than or equal to specified amount.\n* _**Current Coverage**_ - [end to end test coverage](https://github.com/akash-network/provider/blob/main/integration/escrow\\_monitor\\_test.go)\n\n#### Expected Failure\n\n### General\n\n#### Expected Success\n\n**SIMPLE DEPLOYMENT - SINGLE SERVICE - TEST A**\n\n* _**Description**_ - simple deployment test with a single service. No IP Leases, persistent storage, or other services.\n* [Current SDL](https://github.com/akash-network/provider/blob/f13aca40ac42f96b80ec5e863cdfa20093e23b44/testdata/sdl/simple.yaml) - `simple.yaml`\n* _**Expected Outcome**_ - successful deployment/order creation with receipt of provider bid on simple, single service SDL\n\n**SIMPLE DEPLOYMENT - SINGLE SERVICE - TEST B**\n\n* _**Description**_ - simple deployment test with a single service. No IP Leases, persistent storage, or other services.\n* [Current SDL](https://github.com/akash-network/provider/blob/main/testdata/deployment/deployment-v2.yaml) - `deployment-v2.yaml`\n* _**Expected Outcome**_ - successful deployment/order creation with receipt of provider bid on simple, single service SDL\n* _**Current Coverage**_ - [end to end test coverage](https://github.com/akash-network/provider/blob/main/integration/app\\_test.go)\n\n**SIMPLE DEPLOYMENT - SINGLE SERVICE - TEST C**\n\n* _**Description**_ - simple deployment test with a single service. No IP Leases, persistent storage, or other services.\n* [Current SDL](https://github.com/akash-network/provider/blob/main/testdata/deployment/deployment.yaml) - `deployment.yaml`\n* _**Expected Outcome**_ - successful deployment/order creation with receipt of provider bid on simple, single service SDL\n\n**MIGRATION TESTING**\n\n* _**Description**_ - migration testing to new API version\n* [Current SDL](https://github.com/akash-network/provider/blob/main/testdata/deployment/deployment-v2-migrate.yaml) - `deployment-v2-migrate.yaml`\n* _**Expected Outcome**_ - successful deployment/order creation with receipt of provider bid.\n* _**Current Coverage**_ - [end to end test coverage](https://github.com/akash-network/provider/blob/main/integration/migrate\\_hostname\\_test.go)\n\n**NEW CONTAINER**\n\n* _**Description**_ - validation of container/pod creation on new Akash deployment\n* [Current SDL](https://github.com/akash-network/provider/blob/main/testdata/deployment/deployment-v2-newcontainer.yaml) - `deployment-v2-newcontainer.yaml`\n* _**Expected Outcome**_ - successful deployment/order creation with receipt of provider bid.\n\n**DEPLOYMENT UPDATE TEST A**\n\n* _**Description**_ - verify deployment update capability.\n* [Current SDL](https://github.com/akash-network/provider/blob/main/testdata/deployment/deployment-v2-updateA.yaml) - `deployment-v2-updateA.yaml`\n* _**Expected Outcome**_ - successful update of image and/or env variables of pre-existing deplopyment.\n* _**Current Coverage**_ - [end to end test coverage](https://github.com/akash-network/provider/blob/main/integration/deployment\\_update\\_test.go)\n\n**DEPLOYMENT UPDATE TEST B**\n\n* _**Description**_ - verify deployment update capability.\n* [Current SDL](https://github.com/akash-network/provider/blob/main/testdata/deployment/deployment-v2-updateB.yaml) - `deployment-v2-updateB.yaml`\n* _**Expected Outcome**_ - successful update of image and/or env variables of pre-existing deplopyment.\n\n#### Expected Failure\n\n**SIMPLE DEPLOYMENT - TWO SERVICES**\n\n* **Description** - simple deployment test with two services. Deployment should fail validation as two services are declared but only one used. No IP Leases, persistent storage, or other services.\n* [Current SDL](https://github.com/akash-network/provider/blob/f13aca40ac42f96b80ec5e863cdfa20093e23b44/testdata/sdl/simple2.yaml) - `simple2.yaml`\n* _**Expected Outcome**_ - simple deployment with two services declares should fail validation as only one of the created services is called in the `deployment` stanza.\n\n## Suggested Additions\n\n**DEPLOYMENT WITH MULTIPLE SERVICES USING DIFFERENT REPLICA COUNTS**\n\n* _**Description**_ - create deployment/order with multiple services using different replica counts and ensure bid receipt from provider.","description":null,"slug":"eng-notes/akash-end-to-end-testing-provider/providerrepocoverage"},{"title":"Akash Operator Overview","body":"\n## Suggested Pre-Reading\n\n#### Kubebuilder for Operator Builds\n\nFamiliarity with Kubernetes Operators allows a better understanding of Akash provider Custom Resource Definitions (CRD) and custom controllers.\n\nA very populate tool for scaffolding Kubernetes Operators is Kubebuilder.  While the Akash Provider Operators were not created using Kubebuilder, reviewing this tool and experimenting with simple examples provided in the guide below serve as an excellent exposure to the creation of CRDs and associated controllers.\n\n* [Kubebuilder Tutorials](https://book.kubebuilder.io/introduction.html)\n\n#### Code-Generator for Operator Builds\n\nThe Akash code base uses [code-generator ](https://github.com/kubernetes/code-generator) for CRD and controller scaffolding.  Code-Generator limits some of the bloated boiler-plate created via other tools like Kubebuilder.  Many resources exist for an introduction to code-generator and listed below is one such article to increase familiarity with the scaffolded directory structure.  With this knowledge in place we can begin digging into Akash Provider Operators.\n\n* [Extending Kubernetes - Create Controllers for Core and Custom Resources (using code-generator)](https://trstringer.com/extending-k8s-custom-controllers/)\n\n## Akash Provider Operators\n\n* [Hostname Operator](/docs/eng-notes/akash-provider-operators/akash-operator-overview/hostname-operator-for-ingress-controller/hostname-operator-for-ingress-controller/)\n* [IP Operator](/docs/eng-notes/akash-provider-operators/akash-operator-overview/ip-operator-for-ip-leases/)\n* [Inventory Operator](/docs/eng-notes/akash-provider-operators/akash-operator-overview/inventory-operator-for-persistent-storage/)\n\n","description":null,"slug":"eng-notes/akash-provider-operators/akash-operator-overview/akash-operator-overview"},{"title":"Hostname Controller Deep Dive","body":"\n### 1). Hostname Operator Listening Loop\n\nThe Akash Provider Hostname Operator command -  `hostname-operator`  invokes initial controller variable and logging settings.\n\n> [Source code reference location](https://github.com/akash-network/provider/blob/e7aa0b5b81957a130f1dc584f335c6f9e41db6b1/operator/hostnameoperator/hostname\\_operator.go)\n\nThis logic begins with the call of the `doHostnameOperator` function from Hostname Operator command.  Eventually in this function the `run` method is called with an operator struct passed in.  The `run` function - covered in detail shortly - will begin a listening loop for new ingress controller entries.\n\n```\nfunc doHostnameOperator(cmd *cobra.Command) error {\n    ....\n    \tgroup.Go(func() error {\n\t\treturn op.run(ctx)\n\t})\n    ....\n}\n```\n\nThe `operator` struct of which `op` of type `hostnameOperator` is passed into the `run` method as mentioned.\n\n```\ntype hostnameOperator struct {\n\thostnames map[string]managedHostname\n\n\tleasesIgnored operatorcommon.IgnoreList\n\n\tclient cluster.Client\n\n\tlog log.Logger\n\n\tcfg    operatorcommon.OperatorConfig\n\tserver operatorcommon.OperatorHTTP\n\n\tflagHostnamesData  operatorcommon.PrepareFlagFn\n\tflagIgnoreListData operatorcommon.PrepareFlagFn\n}\n```\n\n### 2). Hostname Operator Listening Loop\n\n&#x20;The `run` method invokes loop listening for new ingress controller entries.\n\nA perpetual for loop is created and the upstream `monitorUntilError` method is called.  The `monitorUntilError` - as covered in detail next - will listen on an event bus for new Kubernetes Ingress Controller entries.\n\n```\nfunc (op *hostnameOperator) run(parentCtx context.Context) error {\n\top.log.Debug(\"hostname operator start\")\n\n\tfor {\n\t\t...\n\t\terr := op.monitorUntilError(parentCtx)\n\t\t...\n\t}\n}\n```\n\n### 3). Collect and Store Current Ingress Controller Entries\n\nThe `monitorUntilError` method calls the `GetHostnameDeploymentConnections` - located in `provider/cluster/kube/client_ingress.go` - which makes a call to the Kubernetes API server for a list of Ingress Controller entries.\n\nThe Ingress Controller entries are stored in the `connections` variable which is ranged/looped through and added to the `hostnameOperator` struct map of hostnames.  Future deployments will have their hostname added to the complete, current map when new `providerhost` custom resources are created.\n\nThe map of hostnames will be used in downstream logic to determine if an ingress controller entry needs to be created for a `providerhost` custom resource or if the entry already exists and can then be updated or deemed no add/update necessary.\n\n```\nfunc (op *hostnameOperator) monitorUntilError(parentCtx context.Context) error {\n\t....\n\top.log.Info(\"starting observation\")\n\n\tconnections, err := op.client.GetHostnameDeploymentConnections(ctx)\n\t....\n\n\tfor _, conn := range connections {\n\t\tleaseID := conn.GetLeaseID()\n\t\thostname := conn.GetHostname()\n\t\tentry := managedHostname{\n\t\t\tlastEvent:           nil,\n\t\t\tpresentLease:        leaseID,\n\t\t\tpresentServiceName:  conn.GetServiceName(),\n\t\t\tpresentExternalPort: uint32(conn.GetExternalPort()),\n\t\t}\n\n\t\top.hostnames[hostname] = entry\n\t\top.log.Debug(\"identified existing hostname connection\",\n\t\t\t\"hostname\", hostname,\n\t\t\t\"lease\", entry.presentLease,\n\t\t\t\"service\", entry.presentServiceName,\n\t\t\t\"port\", entry.presentExternalPort)\n\t}\n}\n```\n\n### 4). Monitor Kubernetes for New Provider Host Custom Resources\n\nThe  `ObserveHostnameState` function - located within provider/cluster/kube/client\\_hostname\\_connections.go - monitors for new `providerhost` custom resource adds, updates, or deletes.\n\nThe `ObserveHostnameState` method returns new events on a channel which is then taken off the channel within a select block.&#x20;\n\nFinally the event - stored in the `ev` variable once it is pulled off the channel - is passed into the `applyEvent` method.\n\n```\n\t....\n\t\n\tevents, err := op.client.ObserveHostnameState(ctx)\n\tif err != nil {\n\t\tcancel()\n\t\treturn err\n\t}\n\nloop:\n\tfor {\n\t\tselect {\n\t\t....\n\n\t\tcase ev, ok := <-events:\n\t\t\tif !ok {\n\t\t\t\texitError = operatorcommon.ErrObservationStopped\n\t\t\t\tbreak loop\n\t\t\t}\n\t\t\terr = op.applyEvent(ctx, ev)\n\t\t\tif err != nil {\n\t\t\t\top.log.Error(\"failed applying event\", \"err\", err)\n\t\t\t\texitError = err\n\t\t\t\tbreak loop\n\t\t\t}\n\t\tcase <-pruneTicker.C:\n\t\t\top.prune()\n\t\tcase <-prepareTicker.C:\n\t\t\tif err := op.server.PrepareAll(); err != nil {\n\t\t\t\top.log.Error(\"preparing web data failed\", \"err\", err)\n\t\t\t}\n\n\t\t}\n\t}\n\n\tcancel()\n\top.log.Debug(\"hostname operator done\")\n\treturn exitError\n```\n\n### 5). Apply the Event/Hostname Addition\n\nThe `applyEvent` method - located in the same file `hostname_operator.go` file as the `run` function - matches the event type (I.e. `ProviderResourceAdd`).  The event type was set prior via the `ObserveHostnameState` method.\\\n\\\nFollowing the path of a new `providerhost` resource add as an example the matched event is then passed to the `applyAddOrUpdateEvent` method.\n\n```\nfunc (op *hostnameOperator) applyEvent(ctx context.Context, ev ctypes.HostnameResourceEvent) error {\n\top.log.Debug(\"apply event\", \"event-type\", ev.GetEventType(), \"hostname\", ev.GetHostname())\n\tswitch ev.GetEventType() {\n\t...\n\tcase ctypes.ProviderResourceAdd, ctypes.ProviderResourceUpdate:\n\t\tif op.isEventIgnored(ev) {\n\t\t\top.log.Info(\"ignoring event for\", \"lease\", ev.GetLeaseID().String())\n\t\t\treturn nil\n\t\t}\n\t\terr := op.applyAddOrUpdateEvent(ctx, ev)\n\t...\n\n}\n```\n\n### 6). Determine if New Ingres Controller Entry is Necessary\n\nA check is conducted to determine if the hostname already exists in the hostname map of the `hostnameOperator (op)` struct.  If such an entry is not found in the map it is deemed a new ingress controller entry is necessary.\n\nThe ingress controller entry for the event is then made via the `ConnectHostnameToDeployment` method.\n\n```\nfunc (op *hostnameOperator) applyAddOrUpdateEvent(ctx context.Context, ev ctypes.HostnameResourceEvent) error {\n\tselectedExpose, err := locateServiceFromManifest(ctx, op.client, ev.GetLeaseID(), ev.GetServiceName(), ev.GetExternalPort())\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tleaseID := ev.GetLeaseID()\n\n\top.log.Debug(\"connecting\",\n\t\t\"hostname\", ev.GetHostname(),\n\t\t\"lease\", leaseID,\n\t\t\"service\", ev.GetServiceName(),\n\t\t\"externalPort\", ev.GetExternalPort())\n\tentry, exists := op.hostnames[ev.GetHostname()]\n\t....\n\tif isSameLease {\n\t\tshouldConnect := false\n\n\t\tif !exists {\n\t\t\tshouldConnect = true\n\t\t\top.log.Debug(\"hostname target is new, applying\")\n\t\t\t// Check to see if port or service name is different\n\t\t}\n\t\t....\n\t\tif shouldConnect {\n\t\t\top.log.Debug(\"Updating ingress\")\n\t\t\t// Update or create the existing ingress\n\t\t\terr = op.client.ConnectHostnameToDeployment(ctx, directive)\n\t\t}\n\t\t....\n}\n```\n\n### 7). Apply New Ingress Controller Rule\n\n```\nfunc (c *client) ConnectHostnameToDeployment(ctx context.Context, directive ctypes.ConnectHostnameToDeploymentDirective) error {\n\tingressName := directive.Hostname\n\tns := builder.LidNS(directive.LeaseID)\n\trules := ingressRules(directive.Hostname, directive.ServiceName, directive.ServicePort)\n\n\tfoundEntry, err := c.kc.NetworkingV1().Ingresses(ns).Get(ctx, ingressName, metav1.GetOptions{})\n\tmetricsutils.IncCounterVecWithLabelValuesFiltered(kubeCallsCounter, \"ingresses-get\", err, kubeErrors.IsNotFound)\n\n\tlabels := make(map[string]string)\n\tlabels[builder.AkashManagedLabelName] = \"true\"\n\tbuilder.AppendLeaseLabels(directive.LeaseID, labels)\n\n\tingressClassName := akashIngressClassName\n\tobj := &netv1.Ingress{\n\t\tObjectMeta: metav1.ObjectMeta{\n\t\t\tName:        ingressName,\n\t\t\tLabels:      labels,\n\t\t\tAnnotations: kubeNginxIngressAnnotations(directive),\n\t\t},\n\t\tSpec: netv1.IngressSpec{\n\t\t\tIngressClassName: &ingressClassName,\n\t\t\tRules:            rules,\n\t\t},\n\t}\n\n\tswitch {\n\tcase err == nil:\n\t\tobj.ResourceVersion = foundEntry.ResourceVersion\n\t\t_, err = c.kc.NetworkingV1().Ingresses(ns).Update(ctx, obj, metav1.UpdateOptions{})\n\t\tmetricsutils.IncCounterVecWithLabelValues(kubeCallsCounter, \"networking-ingresses-update\", err)\n\tcase kubeErrors.IsNotFound(err):\n\t\t_, err = c.kc.NetworkingV1().Ingresses(ns).Create(ctx, obj, metav1.CreateOptions{})\n\t\tmetricsutils.IncCounterVecWithLabelValues(kubeCallsCounter, \"networking-ingresses-create\", err)\n\t}\n\n\treturn err\n}\n```","description":null,"slug":"eng-notes/akash-provider-operators/akash-operator-overview/hostname-operator-for-ingress-controller/hostname-controller-deep-dive"},{"title":"Hostname Operator for Ingress Controller","body":"\nNot yet completed\n\n## Pre-Requisites\n\nA basic understanding of Kubernetes Custom Operator building and code structure is necessary to fully understand the concepts covered in our review of the Akash Provider's Hostname Operator.  Suggested reviews of Kubernetes Custom Operators are provided in the [Akash Operator Overview](/docs/eng-notes/akash-provider-operators/akash-operator-overview/akash-operator-overview/) section.\n\n## Visualization\n\n> Use the visualization coupled with the Code Review section for correlated code deep dive\n\n\n\n![](../../../../../assets/akashHostnameOperator.png)\n\n## Code Review\n\n#### 1). Types Used for Customer Resource Definition\n\nThe Go structs that define the Kubernetes Custom Resource Definition for the Hostname Operator are located in the `pkg/apis/akash.network/v2beta1` directory.  The Hostname Operator CRD specifically exists in the `types.go` file.\n\n> [Source code reference location](https://github.com/akash-network/provider/blob/e7aa0b5b81957a130f1dc584f335c6f9e41db6b1/pkg/apis/akash.network/v2beta1/types.go)\n\nPer typical Kubernetes CRD definition a `ProviderHostSpec` defines the schema for the Hostname Operator.  And `ProviderHostStatus` defines the values delivered in the response to a custom resource CRUD event.\n\nWith this definition we will extend the Kubernetes API for use with the Hostname Operator.  When `code-generator` is run against the Go struct the necessary YAML file for applying the CRD to the Kubernetes cluster will be generated and as detailed in the next section.\n\n```\ntype ProviderHostStatus struct {\n\tState   string `json:\"state,omitempty\"`\n\tMessage string `json:\"message,omitempty\"`\n}\n\ntype ProviderHostSpec struct {\n\tOwner        string `json:\"owner\"`\n\tProvider     string `json:\"provider\"`\n\tHostname     string `json:\"hostname\"`\n\tDseq         uint64 `json:\"dseq\"`\n\tGseq         uint32 `json:\"gseq\"`\n\tOseq         uint32 `json:\"oseq\"`\n\tServiceName  string `json:\"service_name\"`\n\tExternalPort uint32 `json:\"external_port\"`\n}\n```\n\n#### 2). CRD YAML Files Created via Code-Generator\n\nThe YAML files to create the Custom Resource Definition (CRD) within Kubernetes is auto-generated by `code-generator` using the Go struct definition into this file:\n\n> pkg/apis/akash.network/crd.yaml\n\n> _**NOTE**_ - along with the generation of the YAML file for CRD application on the provider Kubernetes cluster, code-generator additionally used deep-copy to scaffold the files created in the `provider/pkg/client` directory.  The code-generator files scaffolded into the clientset, informers, and listers subdirectories should not be manually edited but instead are spawned via the definitions in the CRD definition file (I.e. the ProviderHostSpec struct).\n\n#### 3). Cobra Registration of \"hostname-operator\" Command\n\nWithin the `hostname_operator.go` file the definitions of the customer controller are defined. &#x20;\n\n> /provider/operator/hostnameoperator/hostname\\_operator.go\n\nThe `hostname_operator.go` file registers a Cobra command and when executed - via `provider-services hostname-operator` - the Hostname custom controller is initialized.\n\n```\nfunc Cmd() *cobra.Command {\n\tcmd := &cobra.Command{\n\t\tUse:          \"hostname-operator\",\n\t\tShort:        \"kubernetes operator interfacing with k8s nginx ingress\",\n\t\tSilenceUsage: true,\n\t\tRunE: func(cmd *cobra.Command, args []string) error {\n\t\t\treturn doHostnameOperator(cmd)\n\t\t},\n\t}\n```\n\nNote the `RunE` invoke the `doHostnameOperator` function which will be reviewed as we continue into discussion of custom controller logic.\n\n#### 4). Operator Command Flag Registration\n\nNote the Cobra command flag registration in the Cmd function:\n\n```\n\toperatorcommon.AddOperatorFlags(cmd, \"0.0.0.0:8085\")\n\toperatorcommon.AddIgnoreListFlags(cmd)\n\terr := providerflags.AddKubeConfigPathFlag(cmd)\n```\n\nThe referenced `operatorcommon` path pulls in command flags from `provider/oerator/operatorcommon/operator_flags.go`.  Amongst the flags enabled on the `provider-services hostname-operator` command is the --listen which allows the specification of a HTTP endpoint address/port of the operator.\n\nBy default and without explicitly calling the `--listen` flag, the hostname-operator will listen on all  local interfaces (0.0.0.0) and port 8085 based on the default established on the Cobra command flag registration arguments.\n\nWithin `operatorcommon` this specific flag - along with other command operator flags - are registered via these functions (Listen Address flag example):\n\n```\n\tcmd.Flags().String(providerflags.FlagListenAddress, defaultListenAddress, \"listen address for web server\")\n\tif err := viper.BindPFlag(providerflags.FlagListenAddress, cmd.Flags().Lookup(providerflags.FlagListenAddress)); err != nil {\n\t\tpanic(err)\n\t}\n```\n\n#### 5). Custom Controller Logic\n\nThe Hostname Operator custom controller logic is located in the Go file `provider/operator/hostnameoperator/hostname_operator.go`.  This is the same file that the `hostname-operator` Cobra command is registered within and which was covered in the previous section.\n\n> _**NOTE**_ - When a container image is generated and which implements the logic within this file, a Kubernetes deployment may be created to host the controller.\n\nOur code review will detail the mechanics involved in the controller's reconciliation of Desired and Actual states.\n\n> [Source code location](https://github.com/akash-network/provider/blob/main/operator/hostnameoperator/hostname\\_operator.go)\n\nAs the implementation details of the Hostname Operator custom controller is a rather dense topic, the following sections have been created.  The Visual High Level Representation is sufficient for a quick understanding of the mechanics.  While the Controller Deep Dive section goes into detail at a code level.\n\n### Hostname Controller  - Visual High Level Representation\n\n\n![](../../../../../assets/akashHostnameOperatorController.png)\n\n### Hostname Controller - Deep Dive\n","description":null,"slug":"eng-notes/akash-provider-operators/akash-operator-overview/hostname-operator-for-ingress-controller/hostname-operator-for-ingress-controller"},{"title":"Inventory Operator for Persistent Storage","body":"\nDocumentation pending","description":null,"slug":"eng-notes/akash-provider-operators/akash-operator-overview/inventory-operator-for-persistent-storage"},{"title":"IP Operator for IP Leases","body":"\n\nDocumentation pending","description":null,"slug":"eng-notes/akash-provider-operators/akash-operator-overview/ip-operator-for-ip-leases"},{"title":"Provider Service","body":"\n## Visualization\n\n> Use the visualization coupled with the Code Review section for correlated code deep dive\n\n\n\n![](../../../assets/akashProviderService.png)\n\n## Code Review\n\n#### 1). Provider Command Registered Via Cobra\n\nThe Akash Provider command is registered via Cobra allowing initiation of the provider service via `provider-services run` via the Akash CLI.\n\n> [Source code reference location](https://github.com/akash-network/provider/blob/95458f90c22c3be343efa7402ba4ac72100e251c/cmd/provider-services/cmd/run.go)\n\n```\n// RunCmd launches the Akash Provider service\nfunc RunCmd() *cobra.Command {\n\tcmd := &cobra.Command{\n\t\tUse:          \"run\",\n\t\tShort:        \"run akash provider\",\n\t}\n```\n\n#### 2). Command Flag Registration\n\nProvider services `run` command flags are registered via Cobra.\n\n> Example Cobra command flag registration for the declaration of the provider withdraw period is displayed in code capture.  A similar declaration is made for all related `run` command flags.\n\n```\n\tcmd.Flags().Duration(FlagWithdrawalPeriod, time.Hour*24, \"period at which withdrawals are made from the escrow accounts\")\n\tif err := viper.BindPFlag(FlagWithdrawalPeriod, cmd.Flags().Lookup(FlagWithdrawalPeriod)); err != nil {\n\t\treturn nil\n\t}\n```\n\n> Within the const declaration within this file `FlagWithdrawalPeriod` is defined.  Via this const value the command and flag resultant allows the CLI entry of `provider-services run --withdrawal-period <value>` during provider service initiation\n\n```\n\tFlagWithdrawalPeriod                 = \"withdrawal-period\"\n```\n\n#### 3). Invoke of the doRunCmd Function\n\nWhen the Provider Services `run` command is executed the `RunE` block defines downstream code execution on initiation.  Primarily note the call of the `doRunCmd` function.  This function exists in the same source code file.\n\n> The `doRunCmd` function when called invokes several items core to Akash Provider operation a.  As the primary focus of this section is the Akash Provider service and relational Akash Proviider Operators, we will not deep dive into these critical components but will expand on such functionality in other documentation sections.\\\n> \\\n> A listing of core pieces of operation invoked in the `doRunCmd` include:\n>\n> * Creation of Akash RPC node client for transaction monitoring and broadcasting via a call of the `client.NewClientWithBroadcaster` function\n> * Creation of Kubernetes client for K8s cluster CRUD operations via a call of the `createClusterClient` function\n> * Creation of Akash Provider Bid Strategy via a call of the `createBidPricingStrategy` function\n> * Creation of REST API gateway - utilized for request/response for simple provider GET endpoints such as `/status` and `/version` and POST endpoints such as manifest receipt on lease won event - via call of the `gwrest.NewServer` function located in `provider/gateway/rest` directory.\n\n```\n\t\tRunE: func(cmd *cobra.Command, args []string) error {\n\t\t\treturn common.RunForeverWithContext(cmd.Context(), func(ctx context.Context) error {\n\t\t\t\treturn doRunCmd(ctx, cmd, args)\n\t\t\t})\n\t\t},\n```\n\n#### 4). Interaction with Kubernetes Customer Controller - IP Operator\n\nWithin a dedicated document in this section the [Akash IP Operator](/docs/eng-notes/akash-provider-operators/akash-operator-overview/ip-operator-for-ip-leases/) - a Kubernetes custom controller implementation - code is covered in detail.  In this document the interaction with the IP Operator from the Akash Provider service is covered.\n\nCobra command flag declaration includes the following definition which allows the basic enablement of IP Leases when `provider-services run` is executed during Provider creation.\n\n```\n###Const declaration which dictates the command flag to be \"ip-operator\".\n###The flag accepts a boolean true/false to determine if IP Leases should be enabled.\n\tFlagEnableIPOperator                 = \"ip-operator\"\n\n\ncmd.Flags().Bool(FlagEnableIPOperator, false, \"enable usage of the IP operator to lease IP addresses\")\n\tif err := viper.BindPFlag(FlagEnableIPOperator, cmd.Flags().Lookup(FlagEnableIPOperator)); err != nil {\n\t\treturn nil\n\t}\n```\n\nIf the `enableIPOperator` is `true` an `ipOperatorClient` session is opened to allow Provider service to IP Operator communication.\n\n```\n\t// This value can be nil, the operator is not mandatory\n\tvar ipOperatorClient operatorclients.IPOperatorClient\n\tif enableIPOperator {\n\t\tendpoint, err := providerflags.GetServiceEndpointFlagValue(logger, serviceIPOperator)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tipOperatorClient, err = operatorclients.NewIPOperatorClient(logger, kubeConfig, endpoint)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n```\n\n#### 5). Interaction with Kubernetes Customer Controller - Hostname Operator\n\nWithin a dedicated document in this section the [Akash Hostname Operator](/docs/eng-notes/akash-provider-operators/akash-operator-overview/hostname-operator-for-ingress-controller/hostname-operator-for-ingress-controller/) - a Kubernetes custom controller implementation - code is covered in detail.  In this document the interaction with the Hostname Operator from the Akash Provider service is covered.\n\nCobra command flag declaration includes the following definition which allows the basic enablement of IP Leases when `provider-services run` is executed during Provider creation.\n\n```\n\t###The AddServiceEndpointFlag function called is located in:\n\t###provider-services/cmd/flags\n\t###Via the function argument of `serviceHostnameOperator` - which is hardcoded\n\t###to the value of `hostname-operator` the command flag of `hostname-operator-endpoint`\n\t###is registered via Cobra\n\t###The `hostname-operator-endpoint` flag allows specification of the hostname\n\t###operator address and port.  If not specified during `provider-services run`\n\t###auto discovery of the Hostname Operator address/port will occur.\n\t\n\tif err := providerflags.AddServiceEndpointFlag(cmd, serviceHostnameOperator); err != nil {\n\t\treturn nil\n\t}\n```\n\nA hostnameOperatorClient session is opened to allow Provider service to Hostname Operator communication.\n\n```\n\thostnameOperatorClient, err := operatorclients.NewHostnameOperatorClient(logger, kubeConfig, endpoint)\n\tif err != nil {\n\t\treturn err\n\t}\n```\n","description":null,"slug":"eng-notes/akash-provider-operators/provider-service"},{"title":"Bid Engine Overview","body":"\n\n* [Provider Service Calls/Initiates the BidEngine Service](/docs/eng-notes/akash-provider-service-and-associated-sub-services/bid-engine-service#1-provider-service-callsinitiates-the-bidengine-service)\n* [BidEngine Calls/Initiates an Event Bus to Monitor New Orders](/docs/eng-notes/akash-provider-service-and-associated-sub-services/bid-engine-service#2-bidengine-callsinitiates-an-event-bus-to-monitor-new-orders)\n* [BidEngine Loop is Created to React to New Order Receipt and Then Process Order](/docs/eng-notes/akash-provider-service-and-associated-sub-services/bid-engine-service#3-bidengine-loop-is-created-to-react-to-new-order-receipt-and-then-process-order)\n* [Order/Bid Process Manager Uses Perpetual Loop for Event Processing and to Complete Each Step in Bid Process](/docs/eng-notes/akash-provider-service-and-associated-sub-services/bid-engine-service#4-orderbid-process-manager-uses-perpetual-loop-for-event-processing-and-to-complete-each-step-in-bid-process)\n* [Bid Engine Order Detail Fetch](/docs/eng-notes/akash-provider-service-and-associated-sub-services/bid-engine-service#run-function)\n* [groupch Channel Processing](/docs/eng-notes/akash-provider-service-and-associated-sub-services/bid-engine-service#groupch-channel)\n* [shouldBidCh Channel Processing](/docs/eng-notes/akash-provider-service-and-associated-sub-services/bid-engine-service#shouldbidch-channel)\n* [clusterch Channel Prcoessing](/docs/eng-notes/akash-provider-service-and-associated-sub-services/bid-engine-service#clusterch-channel)\n* [pricech Channel Processing](/docs/eng-notes/akash-provider-service-and-associated-sub-services/bid-engine-service#pricech-channel)\n* [bidch Channel Processing](/docs/eng-notes/akash-provider-service-and-associated-sub-services/bid-engine-service#bidch-channel)","description":null,"slug":"eng-notes/akash-provider-service-and-associated-sub-services/bid-engine-overview"},{"title":"Bid Engine Service","body":"## Visualization\n\n\n![](../../../assets/akashProviderBidProcess.png)\n\n## Code Review\n\n### 1). Provider Service Calls/Initiates the BidEngine Service\n\n> [Source code reference location](https://github.com/akash-network/provider/blob/e7aa0b5b81957a130f1dc584f335c6f9e41db6b1/service.go#L101)\n\n\n\n```\n\tbidengine, err := bidengine.NewService(ctx, session, cluster, bus, waiter, bidengine.Config{\n\t\tPricingStrategy: cfg.BidPricingStrategy,\n\t\tDeposit:         cfg.BidDeposit,\n\t\tBidTimeout:      cfg.BidTimeout,\n\t\tAttributes:      cfg.Attributes,\n\t\tMaxGroupVolumes: cfg.MaxGroupVolumes,\n\t})\n```\n\n### 2). BidEngine Calls/Initiates an Event Bus to Monitor New Orders\n\nThe `NewService` function called from `provider/bidengine/service.go` checks for existing orders and subscribes to a RPC node event bus for new order processing.\n\nEventually the `run` method in this package is called with a service type passed in.\n\n> [Source code reference location](https://github.com/akash-network/provider/blob/main/bidengine/service.go)\n\n```\nfunc NewService(ctx context.Context, session session.Session, cluster cluster.Cluster, bus pubsub.Bus, waiter waiter.OperatorWaiter, cfg Config) (Service, error) {\n\tsession = session.ForModule(\"bidengine-service\")\n\n\texistingOrders, err := queryExistingOrders(ctx, session)\n\tif err != nil {\n\t\tsession.Log().Error(\"finding existing orders\", \"err\", err)\n\t\treturn nil, err\n\t}\n\n\tsub, err := bus.Subscribe()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\t\n\t...\n\ts := &service{\n\t\tsession:  session,\n\t\tcluster:  cluster,\n\t\tbus:      bus,\n\t\tsub:      sub,\n\t\tstatusch: make(chan chan<- *Status),\n\t\torders:   make(map[string]*order),\n\t\tdrainch:  make(chan *order),\n\t\tlc:       lifecycle.New(),\n\t\tcfg:      cfg,\n\t\tpass:     providerAttrService,\n\t\twaiter:   waiter,\n\t}\n\n\tgo s.lc.WatchContext(ctx)\n\tgo s.run(ctx, existingOrders)\n```\n\n### 3). BidEngine Loop is Created to React to New Order Receipt and Then Process Order\n\nWithin the `run` function of `provider/bidengine/service.go` an endless for loop monitors for events placed onto a channel.\n\nWhen an event of type `EventOrderCreated` is seen a call to the `newOrder` function - which exists in `provider/bidengine/order.go` - is initiated.  The `newOrder` function call creates a new manager for a specific order.\n\n```\nloop:\n\tfor {\n\t\tselect {\n\t\tcase <-s.lc.ShutdownRequest():\n\t\t\ts.lc.ShutdownInitiated(nil)\n\t\t\tbreak loop\n\n\t\tcase ev := <-s.sub.Events():\n\t\t\tswitch ev := ev.(type) { // nolint: gocritic\n\t\t\tcase mtypes.EventOrderCreated:\n\t\t\t\t// new order\n\t\t\t\tkey := mquery.OrderPath(ev.ID)\n\n\t\t\t\ts.session.Log().Info(\"order detected\", \"order\", key)\n\n\t\t\t\tif order := s.orders[key]; order != nil {\n\t\t\t\t\ts.session.Log().Debug(\"existing order\", \"order\", key)\n\t\t\t\t\tbreak\n\t\t\t\t}\n\n\t\t\t\t// create an order object for managing the bid process and order lifecycle\n\t\t\t\torder, err := newOrder(s, ev.ID, s.cfg, s.pass, false)\n\t\t\t\tif err != nil {\n\t\t\t\t\ts.session.Log().Error(\"handling order\", \"order\", key, \"err\", err)\n\t\t\t\t\tbreak\n\t\t\t\t}\n\n\t\t\t\tordersCounter.WithLabelValues(\"start\").Inc()\n\t\t\t\ts.orders[key] = order\n```\n\n### 4). Order/Bid Process Manager Uses Perpetual Loop for Event Processing and to Complete Each Step in Bid Process\n\nWhen the `newOrder` function within `order.go` is called in the previous step, an `order` struct is populated and then passed to the `run` method.\n\n> [Source code reference location](https://github.com/akash-network/provider/blob/e7aa0b5b81957a130f1dc584f335c6f9e41db6b1/bidengine/order.go#L477)\n\n\n\n```\n\torder := &order{\n\t\tcfg:                        cfg,\n\t\torderID:                    oid,\n\t\tsession:                    session,\n\t\tcluster:                    svc.cluster,\n\t\tbus:                        svc.bus,\n\t\tsub:                        sub,\n\t\tlog:                        log,\n\t\tlc:                         lifecycle.New(),\n\t\treservationFulfilledNotify: reservationFulfilledNotify, // Normally nil in production\n\t\tpass:                       pass,\n\t}\n\n\t...\n\n\t// Run main loop in separate thread.\n\tgo order.run(checkForExistingBid)\n```\n\n#### Run Function\n\nWithin the `run` function details of the order are fetched.\n\n```\n\t// Begin fetching group details immediately.\n\tgroupch = runner.Do(func() runner.Result {\n\t\tres, err := o.session.Client().Query().Group(ctx, &dtypes.QueryGroupRequest{ID: o.orderID.GroupID()})\n\t\treturn runner.NewResult(res.GetGroup(), err)\n\t})\n```\n\n#### groupch Channel\n\nStill within the `run` function, a perpetual for loop awaits order group details to be sent to a channel named `groupch`.  When order/group details are placed onto that channel, the `shouldBid` method is called.\n\nEventually the result of calling `shouldBid` will be placed onto the `shouldBidCh` provoking further upstream order processing.  But prior to review upstream steps we will detail the `shouldBid` function logic.\n\n```\n\t\tcase result := <-groupch:\n\t\t\t// Group details fetched.\n\n\t\t\tgroupch = nil\n\t\t\to.log.Info(\"group fetched\")\n\n\t\t\tif result.Error() != nil {\n\t\t\t\to.log.Error(\"fetching group\", \"err\", result.Error())\n\t\t\t\tbreak loop\n\t\t\t}\n\n\t\t\tres := result.Value().(dtypes.Group)\n\t\t\tgroup = &res\n\n\t\t\tshouldBidCh = runner.Do(func() runner.Result {\n\t\t\t\treturn runner.NewResult(o.shouldBid(group))\n\t\t\t})\n```\n\n#### shouldBidCh Channel\n\nWhen a result from the prior step is placed onto the `shouldBinCh` channel, the `shouldBid` function - also located within `provider/bidengine/order.go` - processes several validations to determine if the provider should bid on the order.\n\n```\n\t\tcase result := <-shouldBidCh:\n\t\t\tshouldBidCh = nil\n```\n\nThe validations include:\n\n* _**MatchAttributes**_ - return `unable to fulfill` if provider does not possess necessary attributes\n* _**MatchResourcesRequirements**_ - return `unable to fulfill` if provider does not possess required, available resources\n* _**SignedBy**_ - return `attribute signature requirements not met` if provider does not possess required audited attributes&#x20;\n\n```\n\tif !group.GroupSpec.MatchAttributes(o.session.Provider().Attributes) {\n\t\to.log.Debug(\"unable to fulfill: incompatible provider attributes\")\n\t\treturn false, nil\n\t}\n\n\t...\n\n\t// does provider have required capabilities?\n\tif !group.GroupSpec.MatchResourcesRequirements(attr) {\n\t\to.log.Debug(\"unable to fulfill: incompatible attributes for resources requirements\", \"wanted\", group.GroupSpec, \"have\", attr)\n\t\treturn false, nil\n\t}\n\t...\n\tsignatureRequirements := group.GroupSpec.Requirements.SignedBy\n\tif signatureRequirements.Size() != 0 {\n\t\t// Check that the signature requirements are met for each attribute\n\t\tvar provAttr []atypes.Provider\n\t\townAttrs := atypes.Provider{\n\t\t\tOwner:      o.session.Provider().Owner,\n\t\t\tAuditor:    \"\",\n\t\t\tAttributes: o.session.Provider().Attributes,\n\t\t...\n\t\tok := group.GroupSpec.MatchRequirements(provAttr)\n\t\tif !ok {\n\t\t\to.log.Debug(\"attribute signature requirements not met\")\n\t\t\treturn false, nil\n\t\t}\n\t\t}\n\t...\n\t\n\n```\n\nShould either `MatchAttributes`, `MatchResourcesRequirements`, or `SignedBy` evaluations fail to satisfy requirements, a boolean false is returned.  If the result evaluates to `false` - meaning one of the validations does not satisfy requirements, `shouldBid` is set to `false`, the loop is exited, and a log message of `decline to bid` on the order is populated.\n\n```\n\t\t\tshouldBid := result.Value().(bool)\n\t\t\tif !shouldBid {\n\t\t\t\tshouldBidCounter.WithLabelValues(\"decline\").Inc()\n\t\t\t\to.log.Debug(\"declined to bid\")\n\t\t\t\tbreak loop\n\t\t\t}\n```\n\nThe next step will begin the Kubernetes cluster reservation of requested resources.\n\nWhile the bid process proceeds the reservation of resources in the Provider's Kubernetes cluster occurs via a call to the `cluster.Reserve` method.  If the bid is not won the reservation will be cancelled.\n\nIf the provider is capable of satisfying all of the requirements of the order the result is placed onto the clusterch channel which provokes the next step of order processing.\n\n```\n\t\t\tclusterch = runner.Do(metricsutils.ObserveRunner(func() runner.Result {\n\t\t\t\tv := runner.NewResult(o.cluster.Reserve(o.orderID, group))\n\t\t\t\treturn v\n\t\t\t}, reservationDuration))\n```\n\nThe `Reserve` function called - the result of which is placed onto the `clusterch` channel - is called from `provider.service.go`.\n\n```\nfunc (s *service) Reserve(order mtypes.OrderID, resources atypes.ResourceGroup) (ctypes.Reservation, error) {\n\treturn s.inventory.reserve(order, resources)\n}\n```\n\n#### clusterch Channel\n\nWhen a result from the prior step is placed onto the `clusterch` channel,  an analysis is made to ensure no errors were encountered during the Kubernetes cluster reservation.  If not error is found a log entry of `Reservation fulfilled` is populated.\n\n```\n\t\tcase result := <-clusterch:\n\t\t\tclusterch = nil\n\n\t\t\tif result.Error() != nil {\n\t\t\t\treservationCounter.WithLabelValues(metricsutils.OpenLabel, metricsutils.FailLabel)\n\t\t\t\to.log.Error(\"reserving resources\", \"err\", result.Error())\n\t\t\t\tbreak loop\n\t\t\t}\n\n\t\t\treservationCounter.WithLabelValues(metricsutils.OpenLabel, metricsutils.SuccessLabel)\n\n\t\t\to.log.Info(\"Reservation fulfilled\")\n```\n\nIf the Kubernetes cluster reservation for the order is successful,  the result of calling the `CalculatePrice` method (using the order specs as input) is placed onto the `pricech` channel which provokes the next step of order processing.\n\nCalling `CalculatePrice` provokes the logic to determine price extended thru bid response.\n\n```\n\t\t\tpricech = runner.Do(metricsutils.ObserveRunner(func() runner.Result {\n\t\t\t\t// Calculate price & bid\n\t\t\t\treturn runner.NewResult(o.cfg.PricingStrategy.CalculatePrice(ctx, group.GroupID.Owner, &group.GroupSpec))\n\t\t\t}, pricingDuration))\n```\n\nThe `CalculatePrice` function is located in `/bidengine/pricing.go` and will determine the price used in bid response to the order.  The price will be dictated by the order specs - I.e. CPU/memory/storage/replicas, etc - and the Provider's pricing script which defines per specification price.\n\n> [Source code reference location](https://github.com/akash-network/provider/blob/e7aa0b5b81957a130f1dc584f335c6f9e41db6b1/bidengine/pricing.go#L127)\n\n```\nfunc (fp scalePricing) CalculatePrice(_ context.Context, _ string, gspec *dtypes.GroupSpec) (sdk.DecCoin, error) {\n\t// Use unlimited precision math here.\n\t// Otherwise a correctly crafted order could create a cost of '1' given\n\t// a possible configuration\n\tcpuTotal := decimal.NewFromInt(0)\n\tmemoryTotal := decimal.NewFromInt(0)\n\tstorageTotal := make(Storage)\n\n\tfor k := range fp.storageScale {\n\t\tstorageTotal[k] = decimal.NewFromInt(0)\n\t}\n\n\tendpointTotal := decimal.NewFromInt(0)\n\tipTotal := decimal.NewFromInt(0).Add(fp.ipScale)\n\tipTotal = ipTotal.Mul(decimal.NewFromInt(int64(util.GetEndpointQuantityOfResourceGroup(gspec, atypes.Endpoint_LEASED_IP))))\n\t...\n```\n\n#### pricech Channel\n\nWhen a result from the prior step is placed onto the `pricech` channel, an analysis is made to ensure that the bid price is not larger than the max price defined in deployment manifest.\n\nIf the order gets past the maxPrice check the logs are populated with the `submitting fulfillment` with specified price message.\n\n```\ncase result := <-pricech:\n\t\t\tpricech = nil\n\t\t\tif result.Error() != nil {\n\t\t\t\to.log.Error(\"error calculating price\", \"err\", result.Error())\n\t\t\t\tbreak loop\n\t\t\t}\n\n\t\t\tprice := result.Value().(sdk.DecCoin)\n\t\t\tmaxPrice := group.GroupSpec.Price()\n\n\t\t\tif maxPrice.IsLT(price) {\n\t\t\t\to.log.Info(\"Price too high, not bidding\", \"price\", price.String(), \"max-price\", maxPrice.String())\n\t\t\t\tbreak loop\n\t\t\t}\n\n\t\t\to.log.Debug(\"submitting fulfillment\", \"price\", price)\n\n```\n\nIf the bid proceeds we eventually broadcast the bid to the blockchain and write the results of this transaction to the `bidch` channel which provokes additional upstream logic covered in the next section.\n\n```\n\t\t\tbidch = runner.Do(func() runner.Result {\n\t\t\t\treturn runner.NewResult(nil, o.session.Client().Tx().Broadcast(ctx, msg))\n\t\t\t})\n```\n\n#### bidch Channel\n\nWhen a result from the prior step is placed onto the `bidch` channel, an error check is made to ensure the bid has not failed for any reason.  And post this final bid validator a message is written to the provider logs of `bid complete`.\n\nThe Bid Engine Service logic for single bid processing is now complete.  The Bid Engine perpetual loop will continue to monitor for new orders found on the blockchain and repeat reviewed order processing on each receipt.\n\n```\n\t\tcase result := <-bidch:\n\t\t\tbidch = nil\n\t\t\tif result.Error() != nil {\n\t\t\t\tbidCounter.WithLabelValues(metricsutils.OpenLabel, metricsutils.FailLabel).Inc()\n\t\t\t\to.log.Error(\"bid failed\", \"err\", result.Error())\n\t\t\t\tbreak loop\n\t\t\t}\n\n\t\t\to.log.Info(\"bid complete\")\n\t\t\tbidCounter.WithLabelValues(metricsutils.OpenLabel, metricsutils.SuccessLabel).Inc()\n\n\t\t\t// Fulfillment placed.\n\t\t\tbidPlaced = true\n\n\t\t\tbidTimeout = o.getBidTimeout(\n```","description":null,"slug":"eng-notes/akash-provider-service-and-associated-sub-services/bid-engine-service"},{"title":"Cluster Service","body":"\n\nWhen the Provider primary service initiates the Cluster Service - detailed in this section - current deployments, hostnames, and inventory are gathered.&#x20;\n\nPerputually the Cluster Service then listens for new orders to place into inventory on bids the Provider acts on and invokes deployments for won bids.\n\n\\-link to cluster service initiation\n\n\\-link to cluster NewService section\n\n\\-etc\n\n## 1).  Cluster Service Initiation\n\n### Provider Service Calls/Initiates the Cluster Service\n\n> [Source code reference location](https://github.com/akash-network/provider/blob/e7aa0b5b81957a130f1dc584f335c6f9e41db6b1/service.go#L101)\n\n```\n\tcluster, err := cluster.NewService(ctx, session, bus, cclient, ipOperatorClient, waiter, clusterConfig)\n\tif err != nil {\n\t\tcancel()\n\t\t<-bc.lc.Done()\n\t\treturn nil, err\n\t}\n```\n\nThe parameters for `cluster.NewService` include the Provider's Kubernetes cluster settings.\n\n```\n\tclusterConfig := cluster.NewDefaultConfig()\n\tclusterConfig.InventoryResourcePollPeriod = cfg.InventoryResourcePollPeriod\n\tclusterConfig.InventoryResourceDebugFrequency = cfg.InventoryResourceDebugFrequency\n\tclusterConfig.InventoryExternalPortQuantity = cfg.ClusterExternalPortQuantity\n\tclusterConfig.CPUCommitLevel = cfg.CPUCommitLevel\n\tclusterConfig.MemoryCommitLevel = cfg.MemoryCommitLevel\n\tclusterConfig.StorageCommitLevel = cfg.StorageCommitLevel\n\tclusterConfig.BlockedHostnames = cfg.BlockedHostnames\n\tclusterConfig.DeploymentIngressStaticHosts = cfg.DeploymentIngressStaticHosts\n\tclusterConfig.DeploymentIngressDomain = cfg.DeploymentIngressDomain\n\tclusterConfig.ClusterSettings = cfg.ClusterSettings\n```\n\nThese settings are defined in the flags used when the `provider-services run` command is issued.\n\nExample flag made available within the `provider/cmd/provider-services/cmd/run.go` file for Ingress Domain declaration.\n\n```\nconst (\n\t...\n\tFlagDeploymentIngressDomain          = \"deployment-ingress-domain\"\n\t....\n)\n```\n\n## 2). Cluster NewService Function\n\n> [Source code reference location](https://github.com/akash-network/provider/blob/e7aa0b5b81957a130f1dc584f335c6f9e41db6b1/cluster/service.go)\n\nThe `NewService` function within `provider/cluster/service.go` invokes:\n\n* Subscription to RPC Node pubsub bus via the `bus.Subscribe` method call\n* The call of the `findDeployments` function to discover current deployments in the Kubernetes cluster.  This function is defined in the same file - `service.go` - as the cluster NewService function exists in.\n* The call of the `newInventoryService` function which will track new/existing orders and create an inventory reservation when the provider bids on a deployment.\n\n```\nfunc NewService(ctx context.Context, session session.Session, bus pubsub.Bus, client Client, ipOperatorClient operatorclients.IPOperatorClient, waiter waiter.OperatorWaiter, cfg Config) (Service, error) {\n\t...\n\n\tsub, err := bus.Subscribe()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tdeployments, err := findDeployments(ctx, log, client, session)\n\tif err != nil {\n\t\tsub.Close()\n\t\treturn nil, err\n\t}\n\n\tinventory, err := newInventoryService(cfg, log, lc.ShuttingDown(), sub, client, ipOperatorClient, waiter, deployments)\n\tif err != nil {\n\t\tsub.Close()\n\t\treturn nil, err\n\t}\n```\n\n## 3). Cluster Service Listening Bus\n\nThe `NewService` function eventually populates a `service` struct and passes the variable to the `run` method which invokes a perpetual listening bus for new deployments.  The `deployments` argument is additionally passed into the `run` method as an argument.\n\n```\n\t\n\ts := &service{\n\t\tsession:                        session,\n\t\tclient:                         client,\n\t\thostnames:                      hostnames,\n\t\tbus:                            bus,\n\t\tsub:                            sub,\n\t\tinventory:                      inventory,\n\t\tstatusch:                       make(chan chan<- *ctypes.Status),\n\t\tmanagers:                       make(map[mtypes.LeaseID]*deploymentManager),\n\t\tmanagerch:                      make(chan *deploymentManager),\n\t\tcheckDeploymentExistsRequestCh: make(chan checkDeploymentExistsRequest),\n\n\t\tlog:    log,\n\t\tlc:     lc,\n\t\tconfig: cfg,\n\t\twaiter: waiter,\n\t}\n\n\tgo s.lc.WatchContext(ctx)\n\tgo s.run(ctx, deployments)\n```\n\n## 4). Creating Deployment Managers for Existing Deployments\n\nWithin in the `run` method a for loop creates a deployment manager for pre-existing deployments on the provider.\n\nFurther explanation of the deployment manager can be found in a later section of this documentation section.\n\n```\nfunc (s *service) run(ctx context.Context, deployments []ctypes.Deployment) {\n\t...\n\n\tfor _, deployment := range deployments {\n\t\tkey := deployment.LeaseID()\n\t\tmgroup := deployment.ManifestGroup()\n\t\ts.managers[key] = newDeploymentManager(s, deployment.LeaseID(), &mgroup, false)\n\t\ts.updateDeploymentManagerGauge()\n\t}\n```\n\n## 5). Cluster Service Perpetual Listening Loop\n\nA perpetual for loop is spawned for the Cluster Service which - amongst other cases - monitor for an event type of `ManifestReceived`.\n\nFollowing a series of validations - for example ensuring that the deployment pre-exists in which would indicate a deployment update event and assurance that the deployment exist in inventory - when passed thru eventually reaches a call of the `newDeploymentManager` function.\n\n```\nloop:\n\tfor {\n\t\tselect {\n\t\t....\n\t\tcase ev := <-s.sub.Events():\n\t\t\tswitch ev := ev.(type) {\n\t\t\tcase event.ManifestReceived:\n\t\t\t\ts.log.Info(\"manifest received\", \"lease\", ev.LeaseID)\n\n\t\t\t\tmgroup := ev.ManifestGroup()\n\t\t\t\tif mgroup == nil {\n\t\t\t\t\ts.log.Error(\"indeterminate manifest group\", \"lease\", ev.LeaseID, \"group-name\", ev.Group.GroupSpec.Name)\n\t\t\t\t\tbreak\n\t\t\t\t}\n\n\t\t\t\tif _, err := s.inventory.lookup(ev.LeaseID.OrderID(), mgroup); err != nil {\n\t\t\t\t\ts.log.Error(\"error looking up manifest\", \"err\", err, \"lease\", ev.LeaseID, \"group-name\", mgroup.Name)\n\t\t\t\t\tbreak\n\t\t\t\t}\n\n\t\t\t\tkey := ev.LeaseID\n\t\t\t\tif manager := s.managers[key]; manager != nil {\n\t\t\t\t\tif err := manager.update(mgroup); err != nil {\n\t\t\t\t\t\ts.log.Error(\"updating deployment\", \"err\", err, \"lease\", ev.LeaseID, \"group-name\", mgroup.Name)\n\t\t\t\t\t}\n\t\t\t\t\tbreak\n\t\t\t\t}\n\n\t\t\t\ts.managers[key] = newDeploymentManager(s, ev.LeaseID, mgroup, true)\n```\n\n## 6). Deployment Managers\n\nThe call of the `newDeploymentManager` function - located in `provider/cluster/manager.go` - provokes a deployment specific lifecycle manager.\n\n> [Source code reference location](https://github.com/akash-network/provider/blob/e7aa0b5b81957a130f1dc584f335c6f9e41db6b1/cluster/manager.go#L81)\n\n```\nfunc newDeploymentManager(s *service, lease mtypes.LeaseID, mgroup *manifest.Group, isNewLease bool) *deploymentManager {\n\t....\n\n\tdm := &deploymentManager{\n\t\tbus:                 s.bus,\n\t\tclient:              s.client,\n\t\tsession:             s.session,\n\t\tstate:               dsDeployActive,\n\t\tlease:               lease,\n\t\tmgroup:              mgroup,\n\t\twg:                  sync.WaitGroup{},\n\t\tupdatech:            make(chan *manifest.Group),\n\t\tteardownch:          make(chan struct{}),\n\t\tlog:                 logger,\n\t\tlc:                  lifecycle.New(),\n\t\thostnameService:     s.HostnameService(),\n\t\tconfig:              s.config,\n\t\tserviceShuttingDown: s.lc.ShuttingDown(),\n\t\tcurrentHostnames:    make(map[string]struct{}),\n\t}\n\n\t...\n\n\tgo func() {\n\t\t<-dm.lc.Done()\n\t\tdm.log.Debug(\"sending manager into channel\")\n\t\ts.managerch <- dm\n\t}()\n\n\terr := s.bus.Publish(event.LeaseAddFundsMonitor{LeaseID: lease, IsNewLease: isNewLease})\n\tif err != nil {\n\t\ts.log.Error(\"unable to publish LeaseAddFundsMonitor event\", \"error\", err, \"lease\", lease)\n\t}\n\n\treturn dm\n}\n```\n\n## 7). Additional Inventory Service Notes\n\nAs described in the previous section the invoke of the NewService function spawns a call of the `newInventoryService` function.\n\nThe `newInventoryService` function is defined in `provider/cluster/inventory.go`.\n\nWhen the Provider's bid engine determines that it should bid on a new deployment the `Reserve` method is called.  Downstream logic places this reservation into inventory.\n\nIn summation this Bid Engine logic is the mechanism in which the Provider reserves Kubernetes resources and places the reservation into inventory while the bid is pending.\n\n> [Source code reference location](https://github.com/akash-network/provider/blob/95458f90c22c3be343efa7402ba4ac72100e251c/bidengine/order.go)\n\n```\n\t\tcase result := <-shouldBidCh:\n\t\t\t....\n\t\t\tclusterch = runner.Do(metricsutils.ObserveRunner(func() runner.Result {\n\t\t\t\tv := runner.NewResult(o.cluster.Reserve(o.orderID, group))\n\t\t\t\t\treturn v\n\t\t\t}, reservationDuration))\n```\n","description":null,"slug":"eng-notes/akash-provider-service-and-associated-sub-services/cluster-service"},{"title":"Manifest Service Overview","body":"\n\n* [Provider Service Calls/Initiates the Manifest Service](/docs/eng-notes/akash-provider-service-and-associated-sub-services/manifest-service#1-provider-service-callsinitiates-the-manifest-service)\n* [Manifest Initiates an Event Bus to Monitor Lease Won Events](/docs/eng-notes/akash-provider-service-and-associated-sub-services/manifest-service#2-manifest-callsinitiates-an-event-bus-to-monitor-lease-won-events)\n* [Monitor Service Loop is Created to React to New Lease Won Events](/docs/eng-notes/akash-provider-service-and-associated-sub-services/manifest-service#3-monitor-service-loop-is-created-to-react-to-new-lease-won-events)\n* [Manifest Manager Logic](/docs/eng-notes/akash-provider-service-and-associated-sub-services/manifest-service#4-manifest-manager-logic)\n* [Receipt of Manifest from Tenant Send to Provider](/docs/eng-notes/akash-provider-service-and-associated-sub-services/manifest-service#5-receipt-of-manifest-from-tenant-send-to-provider)","description":null,"slug":"eng-notes/akash-provider-service-and-associated-sub-services/manifest-service-overview"},{"title":"Manifest Service","body":"\n\n<!-- ## Visualization -->\n\n## Code Review\n\n### 1). Provider Service Calls/Initiates the Manifest Service\n\n\n\n> [Source code reference location](https://github.com/akash-network/provider/blob/e7aa0b5b81957a130f1dc584f335c6f9e41db6b1/service.go#L101)\n\n```\n\tmanifest, err := manifest.NewService(ctx, session, bus, cluster.HostnameService(), manifestConfig)\n\tif err != nil {\n\t\tsession.Log().Error(\"creating manifest handler\", \"err\", err)\n\t\tcancel()\n\t\t<-cluster.Done()\n\t\t<-bidengine.Done()\n\t\t<-bc.lc.Done()\n\t\treturn nil, err\n\t}\n```\n\n### 2). Manifest Calls/Initiates an Event Bus to Monitor Lease Won Events\n\nThe `NewService` function called from `provider/manifest/service.go` subscribes to a RPC node event bus for new lease won processing.\n\nEventually the `run` method in this package is called with a service type passed in.\n\n> [Source code reference location](https://github.com/akash-network/provider/blob/e7aa0b5b81957a130f1dc584f335c6f9e41db6b1/manifest/service.go)\n\n```\nfunc NewService(ctx context.Context, session session.Session, bus pubsub.Bus, hostnameService clustertypes.HostnameServiceClient, cfg ServiceConfig) (Service, error) {\n\tsession = session.ForModule(\"provider-manifest\")\n\n\tsub, err := bus.Subscribe()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\ts := &service{\n\t\tsession:         session,\n\t\tbus:             bus,\n\t\tsub:             sub,\n\t\tstatusch:        make(chan chan<- *Status),\n\t\tmreqch:          make(chan manifestRequest),\n\t\tactiveCheckCh:   make(chan isActiveCheck),\n\t\tmanagers:        make(map[string]*manager),\n\t\tmanagerch:       make(chan *manager),\n\t\tlc:              lifecycle.New(),\n\t\thostnameService: hostnameService,\n\t\tconfig:          cfg,\n\n\t\twatchdogch: make(chan dtypes.DeploymentID),\n\t\twatchdogs:  make(map[dtypes.DeploymentID]*watchdog),\n\t}\n\n\tgo s.lc.WatchContext(ctx)\n\tgo s.run()\n\n\treturn s, nil\n}\n```\n\n### 3). Monitor Service Loop is Created to React to New Lease Won Events\n\nWithin the run function of `provider/manifest/service.go` an endless for loop monitors for events placed onto a channel.  When a event is received for the RPC node event bus of type LeaseWon the `handleLease` method is called.\n\n```\n\tfor {\n\t\tselect {\n\n\t\tcase err := <-s.lc.ShutdownRequest():\n\t\t\ts.lc.ShutdownInitiated(err)\n\t\t\tbreak loop\n\n\t\tcase ev := <-s.sub.Events():\n\t\t\tswitch ev := ev.(type) {\n\n\t\t\tcase event.LeaseWon:\n\t\t\t\tif ev.LeaseID.GetProvider() != s.session.Provider().Address().String() {\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\ts.session.Log().Info(\"lease won\", \"lease\", ev.LeaseID)\n\t\t\t\ts.handleLease(ev, true)\n```\n\nThe `handleLease` method determines if a manager is active for the deployment via the `ensureManager` method.  The manifest manager logic exists in `provider/manifest/manager.go` and handles the validation/application of the manifest when received from the tenant send manifest operation.\n\n```\nfunc (s *service) handleLease(ev event.LeaseWon, isNew bool) {\n\t// Only run this if configured to do so\n\tif isNew && s.config.ManifestTimeout > time.Duration(0) {\n\t\t// Create watchdog if it does not exist AND a manifest has not been received yet\n\t\tif watchdog := s.watchdogs[ev.LeaseID.DeploymentID()]; watchdog == nil {\n\t\t\twatchdog = newWatchdog(s.session, s.lc.ShuttingDown(), s.watchdogch, ev.LeaseID, s.config.ManifestTimeout)\n\t\t\ts.watchdogs[ev.LeaseID.DeploymentID()] = watchdog\n\t\t}\n\t}\n\n\tmanager := s.ensureManager(ev.LeaseID.DeploymentID())\n\t....\n}\n```\n\nNew Manifest Manager instance is initiated by calling the `newManager` function in `provider/manifest/manager.go` with the service type and deployment ID (DSEQ) passed in as arguments.\n\n```\nfunc (s *service) ensureManager(did dtypes.DeploymentID) (manager *manager) {\n\tmanager = s.managers[dquery.DeploymentPath(did)]\n\tif manager == nil {\n\t\tmanager = newManager(s, did)\n\t\ts.managers[dquery.DeploymentPath(did)] = manager\n\t}\n\treturn manager\n}\n```\n\n### 4). Manifest Manager Logic\n\n> [Source code reference location](https://github.com/akash-network/provider/blob/e7aa0b5b81957a130f1dc584f335c6f9e41db6b1/manifest/manager.go)\n\nThe  `newManager` function calls the `run` method - passing in the `manager` struct that includes the  `leasech` channel - which invokes a perpetual for loop to await events on various channels.\n\nThe manifest manager instance is returned to the `handleLease` method in `service.go`.\n\n```\nfunc newManager(h *service, daddr dtypes.DeploymentID) *manager {\n\tsession := h.session.ForModule(\"manifest-manager\")\n\n\t...\n\n\tgo m.lc.WatchChannel(h.lc.ShuttingDown())\n\tgo m.run(h.managerch)\n\n\treturn m\n}\n```\n\nThe `handleLease` method in `service.go` continues and calls another `handleLease` method in `manager.go` passing in lease events received on the bus.\n\n```\nfunc (s *service) handleLease(ev event.LeaseWon, isNew bool) {\n\t....\n\n\tmanager := s.ensureManager(ev.LeaseID.DeploymentID())\n\n\tmanager.handleLease(ev)\n}\n```\n\nThe handleLease method in `manager.go` puts the event onto the `leasech` channel.\n\n```\nfunc (m *manager) handleLease(ev event.LeaseWon) {\n\tselect {\n\tcase m.leasech <- ev:\n\tcase <-m.lc.ShuttingDown():\n\t\tm.log.Error(\"not running: handle manifest\", \"lease\", ev.LeaseID)\n\t}\n}\n```\n\nWhen the manifest manager `run` method receives an event on the `leasech` channel the `maybeFetchData` method is called and results is placed onto the `runch` channel.\n\n```\nfunc (m *manager) run(donech chan<- *manager) {\n\t..\n\nloop:\n\tfor {\n\t\t....\n\t\tselect {\n\t\t....\n\t\tcase ev := <-m.leasech:\n\t\t\tm.log.Info(\"new lease\", \"lease\", ev.LeaseID)\n\t\t\tm.clearFetched()\n\t\t\tm.maybeScheduleStop()\n\t\t\trunch = m.maybeFetchData(ctx, runch)\n```\n\nThe `maybeFetchData` method attempts to fetch deployment and lease data with associated downstream logic.\n\n```\nfunc (m *manager) maybeFetchData(ctx context.Context, runch <-chan runner.Result) <-chan runner.Result {\n\tif runch != nil {\n\t\treturn runch\n\t}\n\n\tif !m.fetched || time.Since(m.fetchedAt) > m.config.CachedResultMaxAge {\n\t\tm.clearFetched()\n\t\treturn m.fetchData(ctx)\n\t}\n\treturn runch\n}\n```\n\n### 5). Receipt of Manifest from Tenant Send to Provider\n\nA method of name `Submit` is included in `provider/manifest/service.go` which accepts incoming manifest sends from the deployer/tenant to the provider.  The function is initiated via an incoming HTTP post detailed subsequently.\n\n```\nfunc (s *service) Submit(ctx context.Context, did dtypes.DeploymentID, mani manifest.Manifest) error {\n\t....\n\tselect {\n\tcase <-ctx.Done():\n\t\treturn ctx.Err()\n\tcase s.mreqch <- req:\n\tcase <-s.lc.ShuttingDown():\n\t\treturn ErrNotRunning\n\tcase <-s.lc.Done():\n\t\treturn ErrNotRunning\n\t}\n\n\t...\n}\n```\n\nThe `Submit` method is called when a HTTP post - which contains the Akash manifest in the body - is received on an endpoint and handler written/registered in `provider/gateway/rest/router.go`.\n\n_**HTTP Endpoint**_\n\n```\n\tdrouter.HandleFunc(\"/manifest\",\n\t\tcreateManifestHandler(log, pclient.Manifest())).\n\t\tMethods(http.MethodPut)\n\n\tlrouter := router.PathPrefix(leasePathPrefix).Subrouter()\n\tlrouter.Use(\n\t\trequireOwner(),\n\t\trequireLeaseID(),\n\t)\n```\n\n_**Request Handler**_\n\nNote the call of the `Submit` method which is the provider/manifest/service.go function shown prior.  The Deployment ID and manifest are sent to `Submit` as received in the HTTP post from the tenant's send manifest action following lease creation with a provider.\n\n```\nfunc createManifestHandler(log log.Logger, mclient pmanifest.Client) http.HandlerFunc {\n\t....\n\t\tif err := mclient.Submit(subctx, requestDeploymentID(req), mani); err != nil {\n\t\t\tif errors.Is(err, manifestValidation.ErrInvalidManifest) {\n\t\t\t\thttp.Error(w, err.Error(), http.StatusUnprocessableEntity)\n\t\t\t\treturn\n\t....\n\t}\n}\n```","description":null,"slug":"eng-notes/akash-provider-service-and-associated-sub-services/manifest-service"},{"title":"Overview","body":"## Visualization\n\n\n![](../../../assets/akashProviderBidProcess.png)\n\n## Code Review\n\n### 1). Provider Service Calls/Initiates the BidEngine Service\n\n> [Source code reference location](https://github.com/akash-network/provider/blob/e7aa0b5b81957a130f1dc584f335c6f9e41db6b1/service.go#L101)\n\n```\n\tbidengine, err := bidengine.NewService(ctx, session, cluster, bus, waiter, bidengine.Config{\n\t\tPricingStrategy: cfg.BidPricingStrategy,\n\t\tDeposit:         cfg.BidDeposit,\n\t\tBidTimeout:      cfg.BidTimeout,\n\t\tAttributes:      cfg.Attributes,\n\t\tMaxGroupVolumes: cfg.MaxGroupVolumes,\n\t})\n```\n\n### 2). BidEngine Calls/Initiates an Event Bus to Monitor New Orders\n\nThe `NewService` function called from `provider/bidengine/service.go` checks for existing orders and subscribes to a RPC node event bus for new order processing.\n\nEventually the `run` method in this package is called with a service type passed in.\n\n> [Source code reference location](https://github.com/akash-network/provider/blob/main/bidengine/service.go)\n\n```\nfunc NewService(ctx context.Context, session session.Session, cluster cluster.Cluster, bus pubsub.Bus, waiter waiter.OperatorWaiter, cfg Config) (Service, error) {\n\tsession = session.ForModule(\"bidengine-service\")\n\n\texistingOrders, err := queryExistingOrders(ctx, session)\n\tif err != nil {\n\t\tsession.Log().Error(\"finding existing orders\", \"err\", err)\n\t\treturn nil, err\n\t}\n\n\tsub, err := bus.Subscribe()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\t\n\t...\n\ts := &service{\n\t\tsession:  session,\n\t\tcluster:  cluster,\n\t\tbus:      bus,\n\t\tsub:      sub,\n\t\tstatusch: make(chan chan<- *Status),\n\t\torders:   make(map[string]*order),\n\t\tdrainch:  make(chan *order),\n\t\tlc:       lifecycle.New(),\n\t\tcfg:      cfg,\n\t\tpass:     providerAttrService,\n\t\twaiter:   waiter,\n\t}\n\n\tgo s.lc.WatchContext(ctx)\n\tgo s.run(ctx, existingOrders)\n```\n\n### 3). BidEngine Loop is Created to React to New Order Receipt and Then Process Order\n\nWithin the `run` function of `provider/bidengine/service.go` an endless for loop monitors for events placed onto a channel.\n\nWhen an event of type `EventOrderCreated` is seen a call to the `newOrder` function - which exists in `provider/bidengine/order.go` - is initiated.  The `newOrder` function call creates a new manager for a specific order.\n\n```\nloop:\n\tfor {\n\t\tselect {\n\t\tcase <-s.lc.ShutdownRequest():\n\t\t\ts.lc.ShutdownInitiated(nil)\n\t\t\tbreak loop\n\n\t\tcase ev := <-s.sub.Events():\n\t\t\tswitch ev := ev.(type) { // nolint: gocritic\n\t\t\tcase mtypes.EventOrderCreated:\n\t\t\t\t// new order\n\t\t\t\tkey := mquery.OrderPath(ev.ID)\n\n\t\t\t\ts.session.Log().Info(\"order detected\", \"order\", key)\n\n\t\t\t\tif order := s.orders[key]; order != nil {\n\t\t\t\t\ts.session.Log().Debug(\"existing order\", \"order\", key)\n\t\t\t\t\tbreak\n\t\t\t\t}\n\n\t\t\t\t// create an order object for managing the bid process and order lifecycle\n\t\t\t\torder, err := newOrder(s, ev.ID, s.cfg, s.pass, false)\n\t\t\t\tif err != nil {\n\t\t\t\t\ts.session.Log().Error(\"handling order\", \"order\", key, \"err\", err)\n\t\t\t\t\tbreak\n\t\t\t\t}\n\n\t\t\t\tordersCounter.WithLabelValues(\"start\").Inc()\n\t\t\t\ts.orders[key] = order\n```\n\n### 4). Order/Bid Process Manager Uses Perpetual Loop for Event Processing and to Complete Each Step in Bid Process\n\nWhen the `newOrder` function within `order.go` is called in the previous step, an `order` struct is populated and then passed to the `run` method.\n\n> [Source code reference location](https://github.com/akash-network/provider/blob/e7aa0b5b81957a130f1dc584f335c6f9e41db6b1/bidengine/order.go#L477)\n\n\n\n```\n\torder := &order{\n\t\tcfg:                        cfg,\n\t\torderID:                    oid,\n\t\tsession:                    session,\n\t\tcluster:                    svc.cluster,\n\t\tbus:                        svc.bus,\n\t\tsub:                        sub,\n\t\tlog:                        log,\n\t\tlc:                         lifecycle.New(),\n\t\treservationFulfilledNotify: reservationFulfilledNotify, // Normally nil in production\n\t\tpass:                       pass,\n\t}\n\n\t...\n\n\t// Run main loop in separate thread.\n\tgo order.run(checkForExistingBid)\n```\n\n#### Run Function\n\nWithin the `run` function details of the order are fetched.\n\n```\n\t// Begin fetching group details immediately.\n\tgroupch = runner.Do(func() runner.Result {\n\t\tres, err := o.session.Client().Query().Group(ctx, &dtypes.QueryGroupRequest{ID: o.orderID.GroupID()})\n\t\treturn runner.NewResult(res.GetGroup(), err)\n\t})\n```\n\n#### groupch Channel\n\nStill within the `run` function, a perpetual for loop awaits order group details to be sent to a channel named `groupch`.  When order/group details are placed onto that channel, the `shouldBid` method is called.\n\nEventually the result of calling `shouldBid` will be placed onto the `shouldBidCh` provoking further upstream order processing.  But prior to review upstream steps we will detail the `shouldBid` function logic.\n\n```\n\t\tcase result := <-groupch:\n\t\t\t// Group details fetched.\n\n\t\t\tgroupch = nil\n\t\t\to.log.Info(\"group fetched\")\n\n\t\t\tif result.Error() != nil {\n\t\t\t\to.log.Error(\"fetching group\", \"err\", result.Error())\n\t\t\t\tbreak loop\n\t\t\t}\n\n\t\t\tres := result.Value().(dtypes.Group)\n\t\t\tgroup = &res\n\n\t\t\tshouldBidCh = runner.Do(func() runner.Result {\n\t\t\t\treturn runner.NewResult(o.shouldBid(group))\n\t\t\t})\n```\n\n#### shouldBidCh Channel\n\nWhen a result from the prior step is placed onto the `shouldBinCh` channel, the `shouldBid` function - also located within `provider/bidengine/order.go` - processes several validations to determine if the provider should bid on the order.\n\n```\n\t\tcase result := <-shouldBidCh:\n\t\t\tshouldBidCh = nil\n```\n\nThe validations include:\n\n* _**MatchAttributes**_ - return `unable to fulfill` if provider does not possess necessary attributes\n* _**MatchResourcesRequirements**_ - return `unable to fulfill` if provider does not possess required, available resources\n* _**SignedBy**_ - return `attribute signature requirements not met` if provider does not possess required audited attributes&#x20;\n\n```\n\tif !group.GroupSpec.MatchAttributes(o.session.Provider().Attributes) {\n\t\to.log.Debug(\"unable to fulfill: incompatible provider attributes\")\n\t\treturn false, nil\n\t}\n\n\t...\n\n\t// does provider have required capabilities?\n\tif !group.GroupSpec.MatchResourcesRequirements(attr) {\n\t\to.log.Debug(\"unable to fulfill: incompatible attributes for resources requirements\", \"wanted\", group.GroupSpec, \"have\", attr)\n\t\treturn false, nil\n\t}\n\t...\n\tsignatureRequirements := group.GroupSpec.Requirements.SignedBy\n\tif signatureRequirements.Size() != 0 {\n\t\t// Check that the signature requirements are met for each attribute\n\t\tvar provAttr []atypes.Provider\n\t\townAttrs := atypes.Provider{\n\t\t\tOwner:      o.session.Provider().Owner,\n\t\t\tAuditor:    \"\",\n\t\t\tAttributes: o.session.Provider().Attributes,\n\t\t...\n\t\tok := group.GroupSpec.MatchRequirements(provAttr)\n\t\tif !ok {\n\t\t\to.log.Debug(\"attribute signature requirements not met\")\n\t\t\treturn false, nil\n\t\t}\n\t\t}\n\t...\n\t\n\n```\n\nShould either `MatchAttributes`, `MatchResourcesRequirements`, or `SignedBy` evaluations fail to satisfy requirements, a boolean false is returned.  If the result evaluates to `false` - meaning one of the validations does not satisfy requirements, `shouldBid` is set to `false`, the loop is exited, and a log message of `decline to bid` on the order is populated.\n\n```\n\t\t\tshouldBid := result.Value().(bool)\n\t\t\tif !shouldBid {\n\t\t\t\tshouldBidCounter.WithLabelValues(\"decline\").Inc()\n\t\t\t\to.log.Debug(\"declined to bid\")\n\t\t\t\tbreak loop\n\t\t\t}\n```\n\nThe next step will begin the Kubernetes cluster reservation of requested resources.\n\nWhile the bid process proceeds the reservation of resources in the Provider's Kubernetes cluster occurs via a call to the `cluster.Reserve` method.  If the bid is not won the reservation will be cancelled.\n\nIf the provider is capable of satisfying all of the requirements of the order the result is placed onto the clusterch channel which provokes the next step of order processing.\n\n```\n\t\t\tclusterch = runner.Do(metricsutils.ObserveRunner(func() runner.Result {\n\t\t\t\tv := runner.NewResult(o.cluster.Reserve(o.orderID, group))\n\t\t\t\treturn v\n\t\t\t}, reservationDuration))\n```\n\nThe `Reserve` function called - the result of which is placed onto the `clusterch` channel - is called from `provider.service.go`.\n\n```\nfunc (s *service) Reserve(order mtypes.OrderID, resources atypes.ResourceGroup) (ctypes.Reservation, error) {\n\treturn s.inventory.reserve(order, resources)\n}\n```\n\n#### clusterch Channel\n\nWhen a result from the prior step is placed onto the `clusterch` channel,  an analysis is made to ensure no errors were encountered during the Kubernetes cluster reservation.  If not error is found a log entry of `Reservation fulfilled` is populated.\n\n```\n\t\tcase result := <-clusterch:\n\t\t\tclusterch = nil\n\n\t\t\tif result.Error() != nil {\n\t\t\t\treservationCounter.WithLabelValues(metricsutils.OpenLabel, metricsutils.FailLabel)\n\t\t\t\to.log.Error(\"reserving resources\", \"err\", result.Error())\n\t\t\t\tbreak loop\n\t\t\t}\n\n\t\t\treservationCounter.WithLabelValues(metricsutils.OpenLabel, metricsutils.SuccessLabel)\n\n\t\t\to.log.Info(\"Reservation fulfilled\")\n```\n\nIf the Kubernetes cluster reservation for the order is successful,  the result of calling the `CalculatePrice` method (using the order specs as input) is placed onto the `pricech` channel which provokes the next step of order processing.\n\nCalling `CalculatePrice` provokes the logic to determine price extended thru bid response.\n\n```\n\t\t\tpricech = runner.Do(metricsutils.ObserveRunner(func() runner.Result {\n\t\t\t\t// Calculate price & bid\n\t\t\t\treturn runner.NewResult(o.cfg.PricingStrategy.CalculatePrice(ctx, group.GroupID.Owner, &group.GroupSpec))\n\t\t\t}, pricingDuration))\n```\n\nThe `CalculatePrice` function is located in `/bidengine/pricing.go` and will determine the price used in bid response to the order.  The price will be dictated by the order specs - I.e. CPU/memory/storage/replicas, etc - and the Provider's pricing script which defines per specification price.\n\n> [Source code reference location](https://github.com/akash-network/provider/blob/e7aa0b5b81957a130f1dc584f335c6f9e41db6b1/bidengine/pricing.go#L127)\n\n```\nfunc (fp scalePricing) CalculatePrice(_ context.Context, _ string, gspec *dtypes.GroupSpec) (sdk.DecCoin, error) {\n\t// Use unlimited precision math here.\n\t// Otherwise a correctly crafted order could create a cost of '1' given\n\t// a possible configuration\n\tcpuTotal := decimal.NewFromInt(0)\n\tmemoryTotal := decimal.NewFromInt(0)\n\tstorageTotal := make(Storage)\n\n\tfor k := range fp.storageScale {\n\t\tstorageTotal[k] = decimal.NewFromInt(0)\n\t}\n\n\tendpointTotal := decimal.NewFromInt(0)\n\tipTotal := decimal.NewFromInt(0).Add(fp.ipScale)\n\tipTotal = ipTotal.Mul(decimal.NewFromInt(int64(util.GetEndpointQuantityOfResourceGroup(gspec, atypes.Endpoint_LEASED_IP))))\n\t...\n```\n\n#### pricech Channel\n\nWhen a result from the prior step is placed onto the `pricech` channel, an analysis is made to ensure that the bid price is not larger than the max price defined in deployment manifest.\n\nIf the order gets past the maxPrice check the logs are populated with the `submitting fulfillment` with specified price message.\n\n```\ncase result := <-pricech:\n\t\t\tpricech = nil\n\t\t\tif result.Error() != nil {\n\t\t\t\to.log.Error(\"error calculating price\", \"err\", result.Error())\n\t\t\t\tbreak loop\n\t\t\t}\n\n\t\t\tprice := result.Value().(sdk.DecCoin)\n\t\t\tmaxPrice := group.GroupSpec.Price()\n\n\t\t\tif maxPrice.IsLT(price) {\n\t\t\t\to.log.Info(\"Price too high, not bidding\", \"price\", price.String(), \"max-price\", maxPrice.String())\n\t\t\t\tbreak loop\n\t\t\t}\n\n\t\t\to.log.Debug(\"submitting fulfillment\", \"price\", price)\n\n```\n\nIf the bid proceeds we eventually broadcast the bid to the blockchain and write the results of this transaction to the `bidch` channel which provokes additional upstream logic covered in the next section.\n\n```\n\t\t\tbidch = runner.Do(func() runner.Result {\n\t\t\t\treturn runner.NewResult(nil, o.session.Client().Tx().Broadcast(ctx, msg))\n\t\t\t})\n```\n\n#### bidch Channel\n\nWhen a result from the prior step is placed onto the `bidch` channel, an error check is made to ensure the bid has not failed for any reason.  And post this final bid validator a message is written to the provider logs of `bid complete`.\n\nThe Bid Engine Service logic for single bid processing is now complete.  The Bid Engine perpetual loop will continue to monitor for new orders found on the blockchain and repeat reviewed order processing on each receipt.\n\n```\n\t\tcase result := <-bidch:\n\t\t\tbidch = nil\n\t\t\tif result.Error() != nil {\n\t\t\t\tbidCounter.WithLabelValues(metricsutils.OpenLabel, metricsutils.FailLabel).Inc()\n\t\t\t\to.log.Error(\"bid failed\", \"err\", result.Error())\n\t\t\t\tbreak loop\n\t\t\t}\n\n\t\t\to.log.Info(\"bid complete\")\n\t\t\tbidCounter.WithLabelValues(metricsutils.OpenLabel, metricsutils.SuccessLabel).Inc()\n\n\t\t\t// Fulfillment placed.\n\t\t\tbidPlaced = true\n\n\t\t\tbidTimeout = o.getBidTimeout(\n```\n","description":null,"slug":"eng-notes/akash-provider-service-and-associated-sub-services/overview"},{"title":"Provider Service Overview","body":"\n\n## Visualization\n\n> Use the visualization coupled with the Code Review section for correlated code deep dive\n\n\n![](../../../assets/akashProviderService.png)\n\n## Code Review\n\n\n\n#### 1). Provider Command Registered Via Cobra\n\nThe Akash Provider command is registered via Cobra allowing initiation of the provider service via `provider-services run` via the Akash CLI.\n\n> [Source code reference location](https://github.com/akash-network/provider/blob/95458f90c22c3be343efa7402ba4ac72100e251c/cmd/provider-services/cmd/run.go)\n\n```\n// RunCmd launches the Akash Provider service\nfunc RunCmd() *cobra.Command {\n\tcmd := &cobra.Command{\n\t\tUse:          \"run\",\n\t\tShort:        \"run akash provider\",\n\t}\n```\n\n#### 2). Command Flag Registration\n\nProvider services `run` command flags are registered via Cobra.\n\n> Example Cobra command flag registration for the declaration of the provider withdraw period is displayed in code capture.  A similar declaration is made for all related `run` command flags.\n\n```\n\tcmd.Flags().Duration(FlagWithdrawalPeriod, time.Hour*24, \"period at which withdrawals are made from the escrow accounts\")\n\tif err := viper.BindPFlag(FlagWithdrawalPeriod, cmd.Flags().Lookup(FlagWithdrawalPeriod)); err != nil {\n\t\treturn nil\n\t}\n```\n\n> Within the const declaration within this file `FlagWithdrawalPeriod` is defined.  Via this const value the command and flag resultant allows the CLI entry of `provider-services run --withdrawal-period <value>` during provider service initiation\n\n```\n\tFlagWithdrawalPeriod                 = \"withdrawal-period\"\n```\n\n#### 3). Invoke of the doRunCmd Function\n\nWhen the Provider Services `run` command is executed the `RunE` block defines downstream code execution on initiation.  Primarily note the call of the `doRunCmd` function.  This function exists in the same source code file.\n\n> The `doRunCmd` function when called invokes several items core to Akash Provider operation a.  As the primary focus of this section is the Akash Provider service and relational Akash Proviider Operators, we will not deep dive into these critical components but will expand on such functionality in other documentation sections.\\\n> \\\n> A listing of core pieces of operation invoked in the `doRunCmd` include:\n>\n> * Creation of Akash RPC node client for transaction monitoring and broadcasting via a call of the `client.NewClientWithBroadcaster` function\n> * Creation of Kubernetes client for K8s cluster CRUD operations via a call of the `createClusterClient` function\n> * Creation of Akash Provider Bid Strategy via a call of the `createBidPricingStrategy` function\n> * Creation of REST API gateway - utilized for request/response for simple provider GET endpoints such as `/status` and `/version` and POST endpoints such as manifest receipt on lease won event - via call of the `gwrest.NewServer` function located in `provider/gateway/rest` directory.\n\n```\n\t\tRunE: func(cmd *cobra.Command, args []string) error {\n\t\t\treturn common.RunForeverWithContext(cmd.Context(), func(ctx context.Context) error {\n\t\t\t\treturn doRunCmd(ctx, cmd, args)\n\t\t\t})\n\t\t},\n```\n\n#### 4). Interaction with Kubernetes Customer Controller - IP Operator\n\nWithin a dedicated document in this section the [Akash IP Operator](/docs/eng-notes/akash-provider-operators/akash-operator-overview/ip-operator-for-ip-leases/) - a Kubernetes custom controller implementation - code is covered in detail.  In this document the interaction with the IP Operator from the Akash Provider service is covered.\n\nCobra command flag declaration includes the following definition which allows the basic enablement of IP Leases when `provider-services run` is executed during Provider creation.\n\n```\n###Const declaration which dictates the command flag to be \"ip-operator\".\n###The flag accepts a boolean true/false to determine if IP Leases should be enabled.\n\tFlagEnableIPOperator                 = \"ip-operator\"\n\n\ncmd.Flags().Bool(FlagEnableIPOperator, false, \"enable usage of the IP operator to lease IP addresses\")\n\tif err := viper.BindPFlag(FlagEnableIPOperator, cmd.Flags().Lookup(FlagEnableIPOperator)); err != nil {\n\t\treturn nil\n\t}\n```\n\nIf the `enableIPOperator` is `true` an `ipOperatorClient` session is opened to allow Provider service to IP Operator communication.\n\n```\n\t// This value can be nil, the operator is not mandatory\n\tvar ipOperatorClient operatorclients.IPOperatorClient\n\tif enableIPOperator {\n\t\tendpoint, err := providerflags.GetServiceEndpointFlagValue(logger, serviceIPOperator)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tipOperatorClient, err = operatorclients.NewIPOperatorClient(logger, kubeConfig, endpoint)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n```\n\n#### 5). Interaction with Kubernetes Customer Controller - Hostname Operator\n\nWithin a dedicated document in this section the [Akash Hostname Operator](/docs/eng-notes/akash-provider-operators/akash-operator-overview/hostname-operator-for-ingress-controller/hostname-operator-for-ingress-controller/) - a Kubernetes custom controller implementation - code is covered in detail.  In this document the interaction with the Hostname Operator from the Akash Provider service is covered.\n\nCobra command flag declaration includes the following definition which allows the basic enablement of IP Leases when `provider-services run` is executed during Provider creation.\n\n```\n\t###The AddServiceEndpointFlag function called is located in:\n\t###provider-services/cmd/flags\n\t###Via the function argument of `serviceHostnameOperator` - which is hardcoded\n\t###to the value of `hostname-operator` the command flag of `hostname-operator-endpoint`\n\t###is registered via Cobra\n\t###The `hostname-operator-endpoint` flag allows specification of the hostname\n\t###operator address and port.  If not specified during `provider-services run`\n\t###auto discovery of the Hostname Operator address/port will occur.\n\t\n\tif err := providerflags.AddServiceEndpointFlag(cmd, serviceHostnameOperator); err != nil {\n\t\treturn nil\n\t}\n```\n\nA hostnameOperatorClient session is opened to allow Provider service to Hostname Operator communication.\n\n```\n\thostnameOperatorClient, err := operatorclients.NewHostnameOperatorClient(logger, kubeConfig, endpoint)\n\tif err != nil {\n\t\treturn err\n\t}\n```\n","description":null,"slug":"eng-notes/akash-provider-service-and-associated-sub-services/provider-service-overview"},{"title":"Import","body":"\n","description":null,"slug":"eng-notes/import"},{"title":"Akash Documentation","body":"\nimport Card from \"@/components/docs/homepage-cat-cards.astro\";\n\n\n\n\nimport wallets from './assets/essentials/wallets.svg'\nimport stack from './assets/essentials/stack.svg'\nimport questions from './assets/essentials/questions.svg'\n\n### Essentials\n\n\n      <div class=\"mt-5 grid grid-cols-1 2xl:grid-cols-3 gap-5 sm:grid-cols-2\">\n        <Card\n          title=\"Tokens & Wallets\"\n          description=\"Tap into the Akash Marketplace and deploy permissionlessly using one of the network’s open-source deployment tools.\"\n          icon={wallets}\n          link=\"/docs\"\n        />\n        <Card\n          title=\"Stack Definition Language (SDL)\"\n          description=\"Deploy a wide range of applications with one-click templates, including the leading AI models and web services.\"\n          icon={stack}\n          link=\"/docs\"\n        />\n        <Card\n          title=\"Common Questions\"\n          description=\"Monetize your cloud resources on the open-source Akash marketplace.\"\n          icon={questions}\n          link=\"/docs\"\n        />\n      </div>\n\n### Deployments\n    \n\n      <div class=\"mt-5 grid grid-cols-1 gap-5  2xl:grid-cols-3 sm:grid-cols-2\">\n        <Card\n          title=\"Cloudmos Deploy\"\n          description=\"Tap into the Akash Marketplace and deploy permissionlessly using one of the network’s open-source deployment tools.\"\n          icon={wallets}\n          link=\"/docs\"\n        />\n        <Card\n          title=\"CLI\"\n          description=\"Deploy a wide range of applications with one-click templates, including the leading AI models and web services.\"\n          icon={stack}\n          link=\"/docs\"\n        />\n        <Card\n          title=\"Stable Payment Deployments\"\n          description=\"Monetize your cloud resources on the open-source Akash marketplace.\"\n          icon={questions}\n          link=\"/docs\"\n        />\n      </div>\n\n\n### Providers\n\n  <div class=\"mt-5 grid grid-cols-1 gap-5 2xl:grid-cols-3 sm:grid-cols-2\">\n    <Card\n      title=\"Access high-performance GPUs\"\n      description=\"Tap into the Akash Marketplace and deploy permissionlessly using one of the network’s open-source deployment tools.\"\n      icon={wallets}\n      link=\"/docs\"\n    />\n    <Card\n      title=\"Deploy an application\"\n      description=\"Deploy a wide range of applications with one-click templates, including the leading AI models and web services.\"\n      icon={stack}\n      link=\"/docs\"\n    />\n    <Card\n      title=\"Praetor Provider Build App\"\n      description=\"Monetize your cloud resources on the open-source Akash marketplace.\"\n      icon={questions}\n      link=\"/docs\"\n    />\n  </div>","description":"Explore the world of Akash through our comprehensive documentation, offering tutorials and a rich repository of platform insights..","slug":"eng-notes"},{"title":"Akash Node Directory Glossary","body":"\n## Source Code\n\n> [Source code reference location](https://github.com/akash-network/node)\n\n## Source for Directory Overview\n\n[https://docs.ignite.com/guide/getting-started](https://docs.ignite.com/guide/getting-started)\n\n## App Directory\n\nThe `app/` directory contains the files that connect the different parts of the blockchain together. The most important file in this directory is app.go, which includes the type definition of the blockchain and functions for creating and initializing it. This file is responsible for wiring together the various components of the blockchain and defining how they will interact with each other.\n\n## CMD Directory\n\nThe `cmd/` directory contains the main package responsible for the command-line interface (CLI) of the compiled binary. This package defines the commands that can be run from the CLI and how they should be executed. It is an important part of the blockchain project as it provides a way for developers and users to interact with the blockchain and perform various tasks, such as querying the blockchain state or sending transactions.\n\n## Docs Directory\n\nThe `docs/` directory is used for storing project documentation. By default, this directory includes an OpenAPI specification file, which is a machine-readable format for defining the API of a software project. The OpenAPI specification can be used to automatically generate human-readable documentation for the project, as well as provide a way for other tools and services to interact with the API. The docs/ directory can be used to store any additional documentation that is relevant to the project.\n\n## Proto Directory\n\nThe `proto/` directory contains protocol buffer files, which are used to describe the data structure of the blockchain. Protocol buffers are a language- and platform-neutral mechanism for serializing structured data, and are often used in the development of distributed systems, such as blockchain networks. The protocol buffer files in the `proto/` directory define the data structures and messages that are used by the blockchain, and are used to generate code for various programming languages that can be used to interact with the blockchain. In the context of the Cosmos SDK, protocol buffer files are used to define the specific types of data that can be sent and received by the blockchain, as well as the specific RPC endpoints that can be used to access the blockchain's functionality.\n\n## Testutil Directory\n\nThe `testutil/` directory contains helper functions that are used for testing. These functions provide a convenient way to perform common tasks that are needed when writing tests for the blockchain, such as creating test accounts, generating transactions, and checking the state of the blockchain. By using the helper functions in the `testutil/` directory, developers can write tests more quickly and efficiently, and can ensure that their tests are comprehensive and effective.\n\n## Module Directory\n\nThe `x/` directory contains custom Cosmos SDK modules that have been added to the blockchain. Standard Cosmos SDK modules are pre-built components that provide common functionality for Cosmos SDK-based blockchains, such as support for staking and governance. Custom modules, on the other hand, are modules that have been developed specifically for the blockchain project and provide project-specific functionality.","description":null,"slug":"eng-notes/table-of-contents/akash-node-directory-glossary"},{"title":"Akash Node Repo Table of Contents","body":"\n<table><thead><tr><th width=\"119\">Directory</th><th width=\"214\">Brief Description</th><th width=\"238\">Prominent SubDirectories/Files</th><th>Available Docs</th></tr></thead><tbody><tr><td>app</td><td>Cosmos SDK module registration and store definitions</td><td>app.go</td><td><a href=\"#akash-app\">Akash App</a></td></tr><tr><td>cmd</td><td>Cobra registration of <code>akash</code> root/sub-commands and flags.</td><td><br></td><td></td></tr><tr><td>proto</td><td>Akash API definitions via protobuf</td><td></td><td><a href=\"#akash-api\">Akash API</a></td></tr><tr><td>sdkutil</td><td>Holds utilities such as  function BroadcastTX</td><td></td><td></td></tr><tr><td>types</td><td>Types derived from protobuf's Go client (protoc)</td><td>node/types/v1beta2/</td><td></td></tr><tr><td>x</td><td>Cosmos SDK Modules</td><td></td><td><a href=\"#akash-modules\">Akash Modules</a></td></tr></tbody></table>\n\n\n## Akash App\n\n## Akash API\n\n\n\n## Akash Modules\n\n* Akash API - Deployment Creation\n* Akash API - Lease Creation","description":null,"slug":"eng-notes/table-of-contents/akash-node-repo-table-of-contents"}]